var tipuesearch = {"pages":[{"title":"Adding a dependency to your Python project: a practical guide","text":"After managing Python projects for quite a few years now, I've learned several things one should think about when bringing a 3rd party dependency to a project. In this post, I'll cover some of the points that are worth considering when deciding to rely on code external to your organization. This post is concerned with a long-living code that is part of a larger piece of software and not a throw-away one-time use Python script to which these concerns do not apply. What are the consequences of bringing external dependencies? When your software starts depending on the 3rd party packages: it takes longer to build a project (downloading and resolving dependencies) you become dependent on an external project that is not guaranteed to be maintained or developed the risks of having incompatible dependencies is increased (particularly when you have multiple external dependencies each having an extensive number of dependencies) upgrading versions of other external dependencies may be more brittle due to potential dependencies conflicts Before bringing in an external dependency, it may be helpful to find out whether it is really needed. In a general case, I believe it's useful to be reluctant to adding any external dependencies unless the benefits of bringing them outweigh the associated cost. If the Python program you write can complete a task without using any 3rd party code, keep it that way. Any code that you haven't written yourself (or is not originated and maintained within your team or organization) that becomes part of your software adds additional risks and maintenance costs. Using a Python library may indeed save development time for the team, however, adding a new dependency should be justified; very often it may not be worth it. Say your program needs to read an input .csv file, apply some filter on the data rows, and produce a new .csv file with a subset of the original one. pandas library would make doing this very easy – this could likely be done in a single line of code. However, unless the program needs to read very large files and do it very often (so there are some performance constraints), you are better off using Python standard library csv module that makes working with .csv files fairly easy. In contrast, when writing a new machine learning library, it would be a pity not to take advantage of existing numerical computation libraries such as numpy and scipy because they are likely to be core foundation of the project. In this case, it is very unlikely that you would need to implement own data structures that would meet the functional and performance requirements of your project, and overall be a better solution than an existing battle tested library. It may also be the case that a 3rd party dependency that you already use in your project provides the desired functionality. For instance, when looking for linear algebra tools, scipy.linalg contains all the functions in numpy.linalg , so if you depend on numpy , you may already have everything you need. Explore the external dependency When you have identified a dependency to bring in, it may be worth spending some time learning more about it. Do the research and explore Snyk Advisor , the project's source code repository , code quality reports , and the PyPI project page to learn more about the project. Library maintenance status and release cadence High commit frequency would indicate active development, and projects that are actively developed are more favorable than stale ones. It is also helpful to see the project owners being receptive to contributions from users which implies that you'd likely be able to submit patches for the bugs that may impact your project. Having a comprehensive test harness with a decent code coverage is highly desired as well. Number of downloads from the PyPI This should give an idea whether the project is used by other organizations and individuals. Any known security issues and vulnerabilities If this is applicable, you may want to explore any reported security issues or use static analysis tools before deciding whether to take in a dependency or not. Developer community A project with a single contributor can be considered to be less reliable than a project with multiple contributors - what happens if the only maintainer leaves the project? Reported issues The number of issues is likely to be proportional to the project popularity, but it's possible to get an idea whether project authors are responsive and work on resolving issues. Python version compatibility If a library of interest uses features of Python 3.9, you may not be able to use it in Python 3.6 environment. There may be a backport of the future Python version functionality for an older version (such as dataclasses ), but it may still stop you from embedding the external library if it is not supported on the version of the target Python runtime environment. Dependencies A project with no (or fewer) external dependencies is easier to integrate than a project with extensive number of external dependencies. Each dependency brings alone its own (transitive) dependencies which increases the chances of dependencies conflicts at dependency resolution time. Licensing A 3rd party package license may or may now allow further distribution of the code or using it in a commercial product. Distribution formats Having only source distributions ( sdist ) may imply that you may need to build the wheel(s) ( bdist_wheel ) to make a binary distribution accessible for your own project build process. Unless you are able to build the wheels yourself and make them available via a binary repository manager such as a hosted PyPI repository or in some other way, libraries with wheels published on PyPI are more preferable. Ideally, there should be wheels available in PyPI for all of your current target architectures (e.g., MacOS wheels for development and Linux for production deployment). Source code programming languages Having a non-pure Python package with certain bits written in a compiled language such as C or Rust would make building binary distributions harder since you'll need to have the necessary toolchain set up and may imply building multiple wheels for each target platform and/or architecture. Adding the dependency If after evaluating a library you have decided to bring it into your Python project, you would need to declare that dependency. The way you do it would depend on your dependency management approach - this may be done via a number of ways such as by using a requirements file, a Poetry project file, or a Pants/Bazel project file. Ideally, constraints file is used to make sure that the same versions of all the transitive dependencies of a project are used, even when a new version of the 3rd party library will be released. The reality is that for any Python project of decent size and complexity it may be difficult to not use 3rd party code. The necessity to add a dependency should be discussed within the team. If there are multiple libraries that provide functionality of interest, write a document that would let you compare them based on the criteria I've mentioned above. Bringing in external dependencies is an important step for your project and should be done with care and planning. Happy depending!","tags":"python","url":"/adding-python-dependency.html","loc":"/adding-python-dependency.html"},{"title":"Introduction to higher order functions with Python","text":"Overview Python is not considered to be a functional language, however it does support the functional paradigm. The Python documentation provides a gentle introduction to functional programming with excellent narrative. As I continue to learn ML , I wanted to share a few interesting concepts around higher-order functions with a few examples written in Python. What is a higher-order function? In Python, functions are first-class which means that a developer can pass functions as arguments to other functions, a function can return a function, and a function can be assigned to a variable or stored in some data structure. A higher-order function, in contrast, is a function that either (or both) takes one or more functions as arguments returns a function as its result The map built-in function is an excellent example of a higher order function as you pass a function as an argument: >>> list ( map ( lambda x : x ** x , [ 1 , 2 , 3 ])) [ 1 , 4 , 27 ] A decorator is also a higher-order function because it takes a function as an argument and returns a function: def twice ( func ): def caller ( ** args ): func ( ** args ) func ( ** args ) return caller @twice def work (): print ( \"Doing work\" ) The work function will be executed twice because it has been decorated with the twice decorator: >>> work () Doing work Doing work Examples of higher-order functions Call a function multiple times re-using the result A function that given x , will call f(x) the n times. The result of each call will become input for the subsequent call. For instance, do_ntimes(lambda x: x+x, 3, 1) is 8 because 1 + 1 = 2 and now it's 2 + 2 = 4 and then finally 4 + 4 = 8 . This happens using a recursive call . def do_ntimes ( f , n , x ): \"\"\"Higher-order function that will do an operation f on x the n times. >>> do_ntimes(lambda x: x+x, 3, 1) 8 >>> do_ntimes(lambda x: x+x, 10, 1) 1024 \"\"\" if n == 0 : return x else : return f ( do_ntimes ( f , n - 1 , x )) Function composition with two functions Function composition is the process of combining two function calls into a single one. def compose_two ( f , g ): \"\"\"Function composition of two functions. >>> compose_two(f=lambda x: x + 10, g=lambda x: x - 5)(10) 15 >>> compose_two(f=lambda x: x - 25, g=lambda x: x * 10)(10) 75 \"\"\" fg = lambda x : f ( g ( x )) return fg It is possible to pass the lambda functions directly or by assigning them to variables first: >>> compose_two ( f = lambda x : x + 10 , g = lambda x : x - 5 )( 10 ) 15 >>> add10 = lambda x : x + 10 >>> minus5 = lambda x : x - 5 >>> compose_two ( f = add10 , g = minus5 )( 10 ) 15 Reducing to calculate the factorial In addition to a recursion based solution (or a plain loop), it's also possible to calculate factorial of a number using the functools.reduce : from functools import reduce from operator import mul def factorial_reduce ( n ): \"\"\"Calculate factorial. >>> factorial_reduce(5) 120 \"\"\" return reduce ( mul , range ( 1 , n + 1 ), 1 ) Reducing a chain of function calls The function composition example can be rewritten in a standalone call. This is done using the built-in functools.reduce() function call. The result of the first function execution becomes the input argument for the next function. In the example below, the operation is ( ( (0 + 10) * 3 ) / 2 ) . >>> reduce ( lambda res , f : f ( res ), [ lambda n : n + 10 , lambda n : n * 3 , lambda n : n / 2 ], 0 ) 15.0 Function composition with arbitrary number of functions Building up on the example above, the compose function can take arbitrary arguments wrapped up in a tuple. def compose ( * functions ): \"\"\"Function composition of arbitrary number of functions. >>> compose(*(lambda x: x + 2, lambda x: x + 5, lambda x: x - 6))(10) 11 >>> compose(*(lambda x: x + 2,))(10) 12 \"\"\" fg = lambda value : reduce ( lambda res , f : f ( res ), functions , value ) return fg Currying to check if elements are sorted When one is currying a function, one is converting a function that takes multiple arguments into a sequence of functions that each take a single argument. def are_3elems_sorted (): \"\"\"Use currying to check if three elements are sorted. >>> ((are_3elems_sorted()(1))(2))(3) True >>> ((are_3elems_sorted()(1))(4))(3) False \"\"\" return lambda x : lambda y : lambda z : z > y > x Happy functioning!","tags":"python","url":"/python-higher-order-functions-intro.html","loc":"/python-higher-order-functions-intro.html"},{"title":"Introducton to recursion with Python","text":"Overview Recursion can be one of the programming concepts that can be a bit of a challenge to understand. I believe this is because the recursive call to a function is not written in a procedural way so that one cannot see the actual sequence of operations that will be taken. Running a program containing a recursive function call with a debugger to be able to step through each line can be very useful. I've spent a few weeks learning ML and was fascinated by the recursion implementation in functional languages and ML in particular. It can be mind bending for anyone who has learned programming through non-functional programming languages that do not encourage use of recursion be it Python or Java. However, I had quite a few \"aha!\" moments when writing recursive functions in ML and I definitely understand recursion much better now. In this post, I wanted to share a few simple recursive functions that are written in Python. I believe practicing writing recursive functions is fun and is a great mind-expanding exercise. Declarative iterations or recursions? I believe the consensus about recursion is that it's appropriate to use recursion in situations when a recursive solution would be better expressed in a recursive call. That is, there isn't a problem that can't be solved without using recursion or if there is one, it must be so rare you'll probably never face it. However, it's very useful to know what recursion is to be able to understand the code that others have written and the caveats associated with using recursion in your programming language. Let's take a look at a few simple cases. Examples of recursive functions If you run any of the of the programs below (each featuring a recursive function call) in a debugger to be able to step through it, you'll see that for each recursive function call, a new frame is added on stack . Python imposes a limit on the number of the recursive calls ( see maximum recursion depth ) which guards against a stack overflow. This means that for most of the real life data, it won't be safe to use the examples below. This is why it can be useful to recognize a recursive call and evaluate whether it's safe to use or not. The coding guidelines for some industries may even explicitly ask to avoid using recursion and replace it with a loop or an iterative algorithm. You can also attempt to solve these tasks as an exercise before looking at my solutions. Each function has a doctest which you can use to check your understanding of the problem. The problems are sorted by difficulty in ascending order. Sum values in an array of integers Given an array of integers, find the sum of all its elements. This can be solved by simply using the built-in sum() which is what most Python developers would use: sum ([ 1 , 2 , 3 , 4 , 5 ]) However, this task can also be solved using a simple for loop and a result variable: total = 0 for number in [ 1 , 2 , 3 , 4 , 5 ]: total += number print ( total ) # 15 And of course, a recursive solution can be written as well: def sum_list ( values ): \"\"\"Sum values in an array of integers. >>> sum_list([]) 0 >>> sum_list([1]) 1 >>> sum_list([1,2,3,4]) 10 \"\"\" if len ( values ) == 0 : return 0 elif len ( values ) == 1 : return values [ 0 ] else : return values [ 0 ] + sum_list ( values [ 1 :]) The way I used to reason about this function call is that I decompose the result of each recursive call which can be helpful to understand what is going on. The way I see the line return values[0] + sum_list(values[1:]) for the initial array of [1,2,3,4] is 1 + sum_list([2,3,4]) => 1 + (2 + sum_list([3,4])) => 1 + 2 + (3 + sum_list([4])) => 1 + 2 + 3 + 4 => 10 Produce range of integers Given a positive integer, produce a list of integers from the given integer down to 0. def countdown ( num ): \"\"\"Get a list of integers from the given positive integer down to 0. >>> countdown(5) [5, 4, 3, 2, 1, 0] \"\"\" if num == 0 : return 0 elif num == 1 : return [ 1 , 0 ] else : return [ num ] + countdown ( num - 1 ) Merge two arrays of items Given two arrays of items, merge them into a single array keeping the order of items. def merge ( arr1 , arr2 ): \"\"\"Merge two arrays of items. >>> merge([1,2,3], [4,5,6]) [1, 2, 3, 4, 5, 6] >>> merge([1,2,3], []) [1, 2, 3] >>> merge([], [4,5,6]) [4, 5, 6] \"\"\" if not arr1 : return arr2 else : return [ arr1 [ 0 ]] + merge ( arr1 [ 1 :], arr2 ) Sum integers in a list of pairs Given an array of non-empty pairs (with each pair containing two integers), find the sum of all numbers in the array. def sum_list_of_pairs ( arr ): \"\"\"Sum integers in a list of pairs. >>> sum_list_of_pairs([]) 0 >>> sum_list_of_pairs([(1,2)]) 3 >>> sum_list_of_pairs([(10,20), (30,40), (50,60)]) 210 \"\"\" if not arr : return 0 else : return arr [ 0 ][ 0 ] + arr [ 0 ][ 1 ] + sum_list_of_pairs ( arr [ 1 :]) Get first elements of sub-arrays in array def firsts ( arr ): \"\"\"Get first elements of all collections in a given array. >>> firsts([]) [] >>> firsts([(1,2,3), (4,5,6)]) [1, 4] >>> firsts([(1,2), (3,4), (5,6)]) [1, 3, 5] \"\"\" if not arr : return [] else : return [ arr [ 0 ][ 0 ]] + firsts ( arr [ 1 :]) Get digits of a number in a given base (up to base of 10) def get_digits_in_base ( num , base ): \"\"\"Get digits of a number in a given base (up to base of 10). >>> get_digits_in_base(192837, 10) ['1', '9', '2', '8', '3', '7'] >>> get_digits_in_base(16, 2) ['1', '0', '0', '0', '0'] >>> get_digits(1000, 16) ['3', 'E', '8'] \"\"\" if num == 0 : return [] quotient , remainder = divmod ( num , base ) return get_digits_in_base ( quotient , base ) + [ str ( remainder )] Get digits of a number in a given base (up to base of 16) def get_digits ( num , base ): \"\"\"Get digits of a number in a given base (up to base of 16). >>> get_digits_in_base(192837, 10) ['1', '9', '2', '8', '3', '7'] >>> get_digits_in_base(16, 2) ['1', '0', '0', '0', '0'] >>> get_digits_in_base(256, 16) ['1', '0', '0'] \"\"\" lookup = '0123456789ABCDEF' if num < base : return [ lookup [ num ]] else : quotient , remainder = divmod ( num , base ) return get_digits ( quotient , base ) + [ lookup [ remainder ]] Tail recursion optimizations A special, perhaps more difficult concept to understand, is tail recursion. The recursive functions we have above have a pattern in the return statement. They return a value and the call to itself (such as return array[0] + func(array[1:]) )). However, if a function would only call itself recursively and wouldn't need to keep any intermediate data, it would be considered a tail recursive function. That is, since there is no intermediate data to maintain (which is why the stack frames are needed), we can essentially replace the current stack frame where the function is calling itself with the new stack frame since we don't need to get back to it – we would only be interested in the final recursive call that would have the final value we need. Factorial of a positive integer This function is not tail-recursive because we need to maintain the value of n that we will multiply with the result of factorial(n-1) . def factorial ( n ): \"\"\"Get factorial of n. >>> factorial(0) 1 >>> factorial(5) 120 \"\"\" if n == 0 : return 1 else : return n * factorial ( n - 1 ) The reason we would want to make our function tail-recursive is because some programming languages provide what is called a tail call optimization . This is how a functional programming language such as Scala supports hundreds of thousands recursive function calls, see here . Python does not have tail call optimization ( here's why ), but let's rewrite the factorial function in a tail-recursive fashion anyway to practice. The idea around tail-call optimization is to use some kind of accumulator, the acc parameter in the example below, that tracks the intermediate data and is passed across the subsequent calls thus eliminating the need to keep the previous stack frames. def factorial_tail ( n , acc ): \"\"\"Get factorial of n in a tail-recursion fashion. >>> factorial_tail(0, 1) 1 >>> factorial_tail(5, 1) 120 \"\"\" if n == 0 : return acc else : return factorial_tail ( n - 1 , acc * n ) Let's write a few more examples of functions with tail call. Sum values in an array (tail call) def sum_values_tail ( arr , acc ): \"\"\"Sum values in an array in a tail-recursion fashion. >>> sum_values_tail([1,2,3], 0) 6 >>> sum_values_tail([], 0) 0 \"\"\" if len ( arr ) == 0 : return acc else : return sum_values_tail ( arr [ 1 :], arr [ 0 ] + acc ) Reverse an array (tail call) def reverse_list_tail ( arr , final ): \"\"\"Reverse list in a tail-recursion fashion. >>> reverse_list_tail([], []) [] >>> reverse_list_tail([1], []) [1] >>> reverse_list_tail([1,2,3], []) [3, 2, 1] \"\"\" if len ( arr ) == 0 : return final else : return reverse_list_tail ( arr [ 1 :], [ arr [ 0 ]] + final ) Reduce (tail call) def reduce_tail ( func , acc , arr ): \"\"\"Reduce (fold) an array to a single value applying a function. >>> reduce_tail(lambda x,y: x * y, 1, [1,2,3,4]) 24 >>> reduce_tail(lambda x,y: x + y, 0, [1,2,3,4]) 10 \"\"\" if len ( arr ) == 0 : return acc else : return reduce_tail ( func , func ( acc , arr [ 0 ]), arr [ 1 :]) Mutual recursion Another more advanced concept is mutual recursion which is a type of recursion when two functions are defined with referral to each other. In this example, given an array of integers, we want to check if it follows a certain pattern, [1,2,1,2...1,2] in this particular case. Once the function responsible to confirm that the first item of the array is 1 is done, it calls another function that confirms that the first item of the array is 2 , and then it calls the first function that confirms that the first item of the array is 1 and the cycle repeats. def pattern_match_one_two ( arr ): \"\"\"Check if a sequence is a pattern 1,2 repeated using a mutual recursion. >>> pattern_match_one_two([1,2,1,2,1,2]) True >>> pattern_match_one_two([1,2,1,2,3,1,2]) False >>> pattern_match_one_two([2,1,2,1,2]) False \"\"\" if len ( arr ) == 0 : return True else : if arr [ 0 ] == 1 : return needs_two ( arr [ 1 :]) else : return False def needs_two ( arr ): if len ( arr ) == 0 : return True else : if arr [ 0 ] == 2 : return pattern_match_one_two ( arr [ 1 :]) else : return False Hope this short introduction helped you understand recursion better and you find the code examples illustrative and useful. Happy recursive coding!","tags":"python","url":"/python-recursion-intro.html","loc":"/python-recursion-intro.html"},{"title":"Python raising SyntaxError when having too many nested for loops","text":"Overview When writing programs in any programming language, it is common to see some syntax or runtime errors. For instance, in Python, it is easy to mess up the indentation in a file after merging files from different codebases. Likewise, one can make an off-by-one error when accessing an array which will be found at the runtime only. Some other types of errors, however, are very rare and it is likely that you will not see many of them in your lifetime as a Python programmer. For instance, if you never use recursion to process a large array, you may never be hit by the maximum recursion depth limitation that exists to guard against a stack overflow. In this post, I document my findings around an issue I have faced when attempting to run auto-generated Python code that contained many nested for loops. Use case I was working on a simple code generation library that given input numeric matrix would produce boilerplate Python program code that could be extended further. I thought it would be useful to experiment how the tool would behave on a matrix of many dimensions because that would involve creating quite a few nested loops. I've been planning to start using itertools.product instead of relying on nested for loops, but wanted to experiment before refactoring. The generated Python code looked like this: for i in range ( 1 ): print ( 0 ) for i in range ( 1 ): print ( 1 ) for i in range ( 1 ): print ( 2 ) # all the way to the 20th nested \"for\" loop for i in range ( 1 ): print ( 18 ) for i in range ( 1 ): print ( 19 ) for i in range ( 1 ): print ( 20 ) You can generate this Python code programmatically if you'd like to experiment: loop = \"\"\" {for_spaces} for i in range(1): {print_spaces} print( {loop_number} ) \\n \"\"\" code = \"\" for i in range ( 0 , 21 , 1 ): code += loop . format ( for_spaces = \" \" * 2 * i , print_spaces = \" \" * 2 * i + \" \" , loop_number = i , ) print ( code ) A very useful tool I've been using occasionally to verify that Python module contains syntactically valid code is compileall which can be used both from a command line and in Python programs. compileall tool will compile your source code into bytecode files ( .pyc ) and if there are any syntax errors, the compilation will fail reporting the problem. Bytecompiling the following Python code: print \"hello!\" produces $ python3 -m compileall code.py Compiling 'code.py' ... *** File \"code.py\" , line 1 print \"hello!\" &#94; SyntaxError: Missing parentheses in call to 'print' . Did you mean print ( \"hello!\" ) ? compileall has also been very useful when migrating legacy codebases from Python 2 to Python 3 when it was used for the first-level sanity check. Too many statically nested blocks Bytecompiling the module with 20+ nested for loops: $ python3 -m compileall too_many_nested_for_loops.py Compiling 'too_many_nested_for_loops.py' ... *** File \"too_many_nested_for_loops.py\" , line None SyntaxError: too many statically nested blocks It turns out that Python has a limit on how many nested blocks (so not just for loops) one is allowed to have. This seems to be a design decision that was made when the CPython interpreter was developed. CPython has a concept of a stack, namely blockstack , which is used to execute code blocks, and it's maximum size is 20. This is an internal implementation detail which I'd unlikely ever hit dealing with human written Python code, but I find it to be very exciting to be able to see a low level detail of CPython design. This Stackoverflow question provides a more thorough explanation of this limit. Happy coding!","tags":"python","url":"/python-too-many-nested-loops.html","loc":"/python-too-many-nested-loops.html"},{"title":"Some helpful Bash notes","text":"Overview There are quite a few resources online on Bash scripting which are extremely useful. I particularly recommend Awesome Bash and The Art of Command Line . There is little point writing yet another Bash tutorial, however, I'd like to share a few helpful notes which others who just start using a command line may find useful. Multiple arguments to the same command Many Unix commands accept multiple arguments, one after another: $ ls *.jar *.vsix tmp-142zodmSW1YLvRv.vsix tmp-417e1PTGYuSLOnV.vsix winstone10385665803316081333.jar tmp-192iFDUu55RqcPk.vsix tmp-417qbhJlHnXzkZl.vsix winstone4439219046698760640.jar tmp-31894bwaoyq9zL7em.vsix tmp-549k8JeuzrFyHMC.vsix winstone4470366079702491377.jar $ touch foo.bar foo.baz $ ls foo.* foo.bar foo.baz One line for loop It's common to see a for loop that spans over multiple lines in shell scripts: for jarfile in *.jar ; do file ${ jarfile } ; done However, working with the for loop spanning over multiple lines in terminal can be cumbersome. Fortunately, the for loop can be put into a single line: $ for jarfile in *.jar ; do file ${ jarfile } ; done winstone10385665803316081333.jar: Java archive data ( JAR ) winstone4439219046698760640.jar: Java archive data ( JAR ) winstone4470366079702491377.jar: Java archive data ( JAR ) Reading standard input Some programs are limited and may not accept files as input arguments. Another use case is when you have to pass sensitive data such as passwords as input to programs using a command line interface (so that it doesn't end up in the terminal history). For example, you may need to produce a semicolon separated list of files (and some program has already produced a list of files): $ cat files.txt winstone10385665803316081333.jar winstone4439219046698760640.jar winstone4470366079702491377.jar $ tr '\\n' ';' < files.txt winstone10385665803316081333.jar ; winstone4439219046698760640.jar ; winstone4470366079702491377.jar ; % Submit multiline input to a command When you need to supply a multiline input to a command, particularly if this needs to happen interactively, you can use a here document which can be used within a shell script file or at a prompt: $ tr '[:lower:]' '[:upper:]' << END first line and second line and third line END FIRST LINE AND SECOND LINE AND THIRD LINE Another useful feature is to be able to write multiline input to a file. This can be very handy when you have to create a multiline file (potentially with non trivial indentation) and the machine you are connected to does not have any text editors available. $ cat << EOF > dummy.txt The file contents to be written: line 1 and line 2 EOF $ cat dummy.txt The file contents to be written: line 1 and line 2 Single and double quotes Single quotes do not let filename and variable expansion to happen in the quoted text. Be very careful mixing the single and double quotes! $ export SITE_TOKEN = 'mytoken' $ echo \" $SITE_TOKEN \" mytoken $ echo '$SITE_TOKEN' $SITE_TOKEN Grouping commands and values using curly braces It can be very useful to be able to run multiple commands, one after another, and save the output to a file. This would let you avoid having multiple lines in your script (where each line would be appending to the file). For instance, to create a log of some system operation: $ { date ; whoami ; echo \"----\" ; ls /var } > log $( date '+%Y-%m-%d' ) .txt $ head -n 5 log2020-12-15.txt Tue Dec 15 22 :21:59 GMT 2020 username ---- backups cache crash Happy shelling!","tags":"bash","url":"/some-helpful-bash-notes.html","loc":"/some-helpful-bash-notes.html"},{"title":"Using python3-apt Debian package for system package management with Python","text":"For Debian-based systems such as Ubuntu, most package management happens via the apt system package. It provides a friendly command line interface, however, there aren't many robust ways to use it in some other way other than via a terminal. Fortunately, there is a Python package, python3-apt , which provides a Python 3 interface to the libapt-pkg library. With this package, you'll be able to list installed packages, check what packages are available for installation, install new packages and so much more. Installation python3-apt package should be installed with apt . The sources are currently available at GitLab: python-apt repo . There isn't a great amount of resources that will help you get started with python3-apt , but the official documentation is very comprehensive. To experiment with the basic usage of the python3-apt package, let's define a Docker image: FROM ubuntu:18.04 RUN apt-get -qq update RUN apt-get install -y --no-install-recommends \\ python3-apt \\ ca-certificates \\ gnupg and build it: $ docker build -t python-apt-docker . Installing a package Let's install a package, make sure that it's available, and then delete it. This can be useful when you need to install a package, but simply using subprocess to make a system command call won't suffice. Parsing apt output is very unreliable and is strongly discouraged. import subprocess import shutil import apt # check that tree is not installed print ( f \"tree executable location: { shutil . which ( 'tree' ) } \" ) # update the cache cache = apt . cache . Cache () cache . update () cache . open () # mark packages you'd like to install package = cache [ 'tree' ] package . candidate = package . versions . get ( '1.7.0-5' ) package . mark_install () cache . commit () # open the cache again and inspect installed package cache = apt . cache . Cache () pkg = cache . get ( 'tree' ) print ( f \"Package installed: { pkg . is_installed } ; version: { pkg . installed . source_version } \" ) print ( f \"Package location: { shutil . which ( 'tree' ) } \" ) # delete the package pkg . mark_delete () cache . commit () # open the cache again and check that the tree package is gone cache = apt . cache . Cache () print ( f \"tree executable location: { shutil . which ( 'tree' ) } \" ) Let's run this code in a Docker container: $ docker run -it --rm -v ${ PWD } /:/project python-apt-docker /bin/bash -c 'python3 /project/install_tree.py' tree executable location: None debconf: delaying package configuration, since apt-utils is not installed Selecting previously unselected package tree. ( Reading database ... 5200 files and directories currently installed. ) Preparing to unpack .../tree_1.7.0-5_amd64.deb ... Unpacking tree ( 1 .7.0-5 ) ... Setting up tree ( 1 .7.0-5 ) ... Package installed: True ; version: 1 .7.0-5 Package location: /usr/bin/tree ( Reading database ... 5207 files and directories currently installed. ) Removing tree ( 1 .7.0-5 ) ... tree executable location: None Checking what packages are installed Let's find out what Python related packages are available in a system. This can be handy for a script that makes sure that system dependencies have been installed. import re import apt_pkg apt_pkg . init_config () apt_pkg . init_system () pkgs = [ pkg for pkg in apt_pkg . Cache () . packages if re . compile ( r 'python' ) . match ( pkg . name )] for pkg in [ pkg for pkg in pkgs if pkg . current_state == apt_pkg . CURSTATE_INSTALLED ]: print ( pkg . name ) Let's run this code in a Docker container: $ docker run -it --rm -v ${ PWD } /:/project python-apt-docker /bin/bash -c 'python3 /project/list_pythons.py' Reading package lists... Done Building dependency tree Reading state information... Done python3.6 python-apt-common python3 python3.6-minimal python3-minimal python3-apt Adding additional Debian sources If the default sources available for system packages do not provide the packages you need (which is a common case for legacy packages or corporate environment Debian pools), it is possible to add additional apt sources using SourcesList on demand. Adding public sources import subprocess import aptsources.sourceslist as sourceslist # showing original sources sources = sourceslist . SourcesList () uris_before = set ([ source . uri for source in sources . list ]) print ( uris_before ) # adding a custom apt source source = ( \"deb [trusted=yes]\" , \"http://download.virtualbox.org/virtualbox/debian\" , \"bionic\" , [ \"contrib\" ]) sources . add ( * source ) sources . save () # showing extended sources uris_after = set ([ source . uri for source in sources . list ]) print ( uris_after ) # printing the contents of the sources.list file process = subprocess . Popen ([ \"tail\" , \"/etc/apt/sources.list\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE ) out , errors = process . communicate () print ( out . decode ()) print ( errors . decode ()) # install a package from the VirtualBox Debian pool ... Let's run this code in a Docker container: $ docker run -it --rm -v ${ PWD } /:/project python-apt-docker /bin/bash -c 'python3 /project/install_from_public.py' { '' , 'http://archive.ubuntu.com/ubuntu/' , 'http://archive.canonical.com/ubuntu' , 'http://security.ubuntu.com/ubuntu/' } { '' , 'http://archive.ubuntu.com/ubuntu/' , 'http://archive.canonical.com/ubuntu' , 'http://security.ubuntu.com/ubuntu/' , 'http://download.virtualbox.org/virtualbox/debian' } # deb http://archive.canonical.com/ubuntu bionic partner # deb-src http://archive.canonical.com/ubuntu bionic partner deb http://security.ubuntu.com/ubuntu/ bionic-security main restricted # deb-src http://security.ubuntu.com/ubuntu/ bionic-security main restricted deb http://security.ubuntu.com/ubuntu/ bionic-security universe # deb-src http://security.ubuntu.com/ubuntu/ bionic-security universe deb http://security.ubuntu.com/ubuntu/ bionic-security multiverse # deb-src http://security.ubuntu.com/ubuntu/ bionic-security multiverse deb [ trusted = yes ] http://download.virtualbox.org/virtualbox/debian bionic contrib Adding private sources To add private sources (that may require authentication), you would need to make some changes. In the Docker image, you may need to download the GPG key from the server that hosts Debian packages. The key can be added later using the apt-key add command. You would also need to add authentication file(s) to the /etc/apt/ directory so that apt would be able to use the authentication details when attempting to download the Debian packages from the private repository. apt_auth.conf files can be handled using a built-in netrc Python module so writing a custom file parser is not necessary. Once the sources.list has the private repository listed, you can install the packages from the repository that requires authentication in the same way as we have installed the tree package earlier. Happy packaging!","tags":"python","url":"/use-apt-from-python.html","loc":"/use-apt-from-python.html"},{"title":"Patching with unittest.mock for Python testing: cheat sheet","text":"Overview of patching Python built-in unittest framework provides the mock module which gets very handy when writing unit tests. It also provides the patch entity which can be used as a function decorator, class decorator or a context manager. The basic idea behind patching is that it lets you temporarily change the object that is being used when your test runs. For example, if you are testing a function that needs to read some data out of a database, you can patch it so that you don't need to communicate with any external services when unit tests run. Given this piece of code, we are interested in testing the get_customers() function. We have tests in place for functions ( validate_input and fetch ) that this function calls, so that we can ignore them. def get_customers ( city : str ): validate_input ( object_type = CITY , value = city ) customers = fetch ( query = f 'customers;city= { city } ' ) return customers To patch those functions, instead of calling them at the execution time, anonymous lambda functions could be used. For the validate_input() function, we are not interested in the return value; however, for the fetch function we are. from unittest.mock import patch from utils import get_customers @patch ( 'utils.validate_input' , lambda object_type , value : None ) def test_get_customers (): expected_customers = [ 'Customer1' , 'Customer2' ] with patch ( 'utils.fetch' , lambda query : expected_customers ): actual_customers = get_customers ( city = 'Paris' ) assert actual_customers == expected_customers The patched fetch function doesn't have to return any meaningful value as we will only be testing whether it returned what the patching function is supposed to return. However, I often find it to be easier to understand the context and business logic when some more real values are used. Note that the patch() was used both as a context manager and as a test function decorator. In the rest of the post, we will explore typical use cases for patching and mocking. Patching class initialization Use case: You want to test a class method, but initializing a class instance would require a lot of additional mocking to pass valid input parameters. You also want to avoid any real initialization operations, but would still want to have some of the class instance variables set. Code: class FileProcessor : def __init__ ( self , files : List [ Path ], process_config : ProcessorConfiguration ): self . files = files self . process_config = process_config def validate_files ( self ): # operate on the files and return some validation result ... Test: def test_validate_files (): with patch . object ( FileProcessor , '__init__' , lambda self : None ): fp = FileProcessor () fp . files = [ Path ( 'foo' ), Path ( 'bar' )] assert fp . validate_files () . get ( \"status\" ) == ValidationStatus . SUCCESS fp . files = [] assert fp . validate_files () . get ( \"status\" ) == ValidationStatus . FAILURE Patching static, class, and instance methods Use case: You have a class for which you want to patch some of the class instance methods, class methods, or static methods. You would need to patch a method of interest and the process of patching the methods is universal for all the methods types. Code: class Address : def __init__ ( self , house : str , street : str , postal_code : str , city : str ): self . house = Address . numerize ( house ) self . street = street self . postal_code = postal_code self . city = city @classmethod def from_tuple ( cls , * address_tuple : Tuple ): return Address ( * address_tuple ) @staticmethod def numerize ( value ): return int ( value ) def to_string ( self ): return f \" { self . house }{ self . street } \\n { self . postal_code } \\n { self . city } \" def print ( self ): fmt = self . to_string () print ( fmt ) return fmt def is_valid_address ( address_tuple ): try : Address . from_tuple ( * address_tuple ) return True except Exception : return False Test: def test_is_valid_address (): with patch ( 'static_class.Address.from_tuple' , lambda * data : True ): assert is_valid_address (( \"1\" , \"New Road\" , \"99999\" , \"City\" )) with patch ( 'static_class.Address.from_tuple' , side_effect = Exception ()): assert not is_valid_address (( \"1\" , \"New Road\" , \"99999\" , \"City\" )) def test_address_construct (): with patch ( 'static_class.Address.numerize' , lambda value : 9999 ): address = Address ( * ( \"1\" , \"New Road\" , \"99999\" , \"City\" )) assert address . city == \"City\" assert address . house == 9999 def test_print (): with patch ( 'static_class.Address.to_string' , lambda value : \"Address formatted\" ): address = Address ( * ( \"1\" , \"New Road\" , \"99999\" , \"City\" )) assert address . print () == \"Address formatted\" Patching a nested class instance attribute Use case: You have a class that when instantiated has an attribute that itself is an instance of a class with additional instance attributes. You are interested in patching this nested attribute to be set to some value. Code: from typing import List class City : def __init__ ( self , name : str ): self . name = name class Address : def __init__ ( self , address : List [ str ]): self . city = City ( address [ 0 ]) self . street = address [ 1 ] self . house = address [ 2 ] class Customer : def __init__ ( self , address : List [ str ]): self . address = Address ( str ) def get_address ( self ): return self . address Test: We want to patch the get_address() method on the class instance and then set a returned object to have a nested attribute set to some value. This may be necessary when you are trying to cover a code branch which will be executed only when a value of the nested attribute is equal to a certain value. from typing import List from unittest.mock import patch , MagicMock def test_customer_address (): with patch ( 'customer.Customer' ) as mock_customer : c = mock_customer . return_value mock_address = MagicMock () # MagicMock lets you define nested attributes mock_address . city . name = \"BigCity\" c . get_address . return_value = mock_address assert c . get_address () . city . name == 'BigCity' Patching class instance method that modifies self Use case: You have a method which doesn't return anything but instead sets or modifies its object ( self ) properties. You cannot patch the mock's method with .return_value and thus need instead modify the values of the self . Code: class Client : def __init__ ( self , guid ): self . guid = guid self . visited = False self . last_time_visited = None def visit ( self ): self . visited = True self . last_time_visited = datetime . now () def process ( self ): self . visit () Test: We would like to test the process() method without letting it to execute the visit() method, but still check that after running the process() method, the Client instance would get certain fields set. from client import Client def setattrs ( obj , ** kwargs ): for k , v in kwargs . items (): setattr ( obj , k , v ) def test_process (): mock_client = Mock () with patch ( 'client.Client' , lambda : mock_client ): mock_client . process . side_effect = lambda : setattrs ( mock_client , visited = True , last_time_visited = datetime . now ()) client = Client ( 99 ) client . process () assert client . visited is True assert isinstance ( client . last_time_visited , datetime ) Patching multiple calls to the same function Use case: You call a function being mocked multiple times in the source code being tested and need to return a different value for each call. For instance, you mock a function call that will check whether a database table exists, then you call a function to delete it, and then use the first function again to make sure that the table does not exist (i.e., it was successfully deleted). You cannot simply mock the function with the patch because it will then return the same value. You have to use the Mock().side_effect property. The side_effect collection can even have an exception initialization if at a certain time you call the mocked function a particular exception is expected to be raised: side_effect = [10, 15, ValueError()] . Code: def get_table ( name ): return True def delete_table ( name ): return True def delete_db_table ( table_name : str ) -> bool : if get_table ( table_name ): delete_table ( table_name ) if not get_table ( table_name ): return True else : raise ValueError ( f \"Failed to delete table { table_name } \" ) return True Test: @patch ( 'db.delete_table' , lambda name : True ) def test_delete_db_table (): get_table_mock = Mock () # db table gets deleted get_table_mock . side_effect = [ True , False ] with patch ( 'db.get_table' , get_table_mock ): assert delete_db_table ( \"LogHistory\" ) # db table fails to be deleted get_table_mock . side_effect = [ True , True ] with patch ( 'db.get_table' , get_table_mock ): with pytest . raises ( ValueError ): delete_db_table ( \"LogHistory\" ) Patching function and its returned object Use case: When you patch a function or a method, it may be the case that the object that it will return may have own methods that you would want to patch. For instance, when patching subprocess.run() that returns a CompletedProcess object, you may want to patch its .check_returncode() method to return some value. Just as with the Mock().side_effect() , it's possible to create side effects for methods of a Mock() object. Code: def call_cmd ( cmd : str ): res = subprocess . run ( cmd ) return res . check_returncode () Test: def test_call_cmd (): run_mock = Mock () run_mock . check_returncode . return_value = \"0\" with patch ( 'subprocess_run.subprocess.run' , lambda cmd : run_mock ): assert call_cmd ( 'du -sh lib' ) == \"0\" run_mock = Mock () run_mock . check_returncode . side_effect = subprocess . CalledProcessError ( 0 , 0 ) with patch ( 'subprocess_run.subprocess.run' , lambda cmd : run_mock ): with pytest . raises ( CalledProcessError ): call_cmd ( 'du -sh non-existing-dir' ) Patching class properties Use case: You have a class with one or more properties decorated with the @property . Mocking them requires special handling using the unittest.mock.PropertyMock object. This is useful when the initialization of an object's property is complex and is not really required for a particular unit test. Code: class Process : def __init__ ( self , pid : int ): self . _pid = pid @property def pid ( self ): return self . _pid def as_string ( self ): return f 'Process( { self . pid } )' Test: def test_process_pid_property (): with patch . object ( Process , '__init__' , lambda self : None ): with patch ( 'class_properties.Process.pid' , new_callable = PropertyMock ) as mock_pid : mock_pid . return_value = 99 process = Process () assert process . as_string () == 'Process(99)' Patching opening a file with open Use case: You have a function that is interacting with the file system by opening a file. A helper function unittest.mock.mock_open can be used to replace the use of open . Code: def read ( path ): with open ( path ) as f : return f . readlines () Test: def test_read_file (): with patch ( 'read_file.open' , mock_open ( read_data = \"lines\" )): assert read ( 'dir/path' ) == [ \"lines\" ] Notes When mocking is required for too many places, it may be the case that your class or a function is too big and would be a good candidate for refactoring. To make mocking for a large (or complex) code unit easier, you can use unittest.mock.MagicMock() instead. When patching code that is interacting with the file system too often, patching all the os , pathlib , and shutil things can get tedious rather quickly. You may want to take a look at the virtual file system, pyfakefs , to be used when testing. Keep in mind that you won't be able to use mock.patch to patch your file system interaction functions when you use pyfakefs because it patches them on its own. When your programs are really scripts that process some files, you may be better off writing good integration tests using real data instead. This could be particularly true when your Python program is relying on using non-Python code such as compiled C (for which you have no source code) to read/process/write data. In this situation, you won't be able to use the pyfakefs virtual file system and mocking all the interaction between your Python programs and external tools can be tedious. If the programs for which you write unit tests interact with external web services over HTTP a lot, you may look into the vcrpy package that can automatically mock your HTTP interactions. The pytest-recording plugin provides handy custom markers. Happy patching!","tags":"python","url":"/patching-mock-python-unit-testing.html","loc":"/patching-mock-python-unit-testing.html"},{"title":"Working with stdout in Python scripts","text":"Overview When working with an existing Python script, particularly a legacy script, or a script that was supposed to be used once and then thrown away but grew into a business critical application (yep, this happens), it can be common to see extensive usage of print or logging statements. Those statements can be spread across the program code and often provide useful information regarding the status of the process while the script is being executed. However, if you have been writing a new script and have finished working on it, or if the script output is not of interest any longer, you most likely wouldn't want to clutter the Python console with print / logging outputs (particularly if the script is part of another larger pipeline). However, the information emitted can still be useful to get logged. Redirecting to a file Instead of removing each print statement (or switching to logging.debug from logging.info ), it is possible to specify to what file the sys.stdout will redirect writing to. This will make the print and logging calls to write to a file on disk instead. import sys # keeping the original reference to the output destination stdout = sys . stdout print ( \"Started script\" ) # redirecting the print statements to the file f = open ( 'log.txt' , 'a' ) sys . stdout = f # main program execution, gets logged to a file print ( \"Getting work done\" ) # setting it to the original output destination sys . stdout = stdout f . close () print ( \"Finished script\" ) Now, when running the program, the print() calls within the main program logic are being redirected to a file on disk. $ python3 program_print.py Started script Finished script $ cat log.txt Getting work done Redirecting to StringIO It is also possible to use the io.StringIO() object to capture everything that will be written to the stdout for the whole script or only a portion of it. import sys from io import StringIO print ( \"Started script\" ) # to capture anything that will be written to the stdout buf = StringIO () stdout = sys . stdout sys . stdout = buf print ( 'Getting work done' ) sys . stdout = stdout # collecting what has been written into a variable captured = buf . getvalue () print ( \"Finished script \\n \" ) print ( captured ) Now, when running the program, the print() calls within the main program logic are being collected into a variable (which is printed here for examination, but can be used for any custom logging). $ python3 program_stringio_var.py Started script Finished script Getting work done Overriding the sys.stdout.write method In both of the examples above, the text that was sent to the original stdout wasn't shown in the console (it's either simply suppressed or captured into a variable). However, it can be sometimes useful to print the output both to the console and put the output into a variable. For this use case, we are essentially after what the tee command does in Linux (which can read stdin and then write it to both the stdout and to a file). In Python, this can be achieved by overriding the sys.stdout.write method. import sys from io import StringIO class StdOutTee : def __init__ ( self , * authors ): self . authors = authors def write ( self , text ): for author in self . authors : author . write ( text ) print ( \"Started script\" ) # to capture anything that will be written to the stdout buf = StringIO () stdout = sys . stdout sys . stdout = StdOutTee ( buf , stdout ) print ( 'Getting work done 1' ) print ( 'Getting work done 2' ) sys . stdout = stdout # collecting what has been written into a variable captured = buf . getvalue () print ( \"Finished script \\n \" ) print ( captured ) Now, when running the program, the print() calls within the main program logic are being collected into a variable (which is printed here for examination, but can be used for any custom logging). However, all the print() statements are printed as well. $ python3 program_tee.py Started script Getting work done 1 Getting work done 2 Finished script Getting work done 1 Getting work done 2 Buffering and flushing When you run a Python program, if the standard output ( stdout ) of its process is redirected to some other target (different from your active terminal), then the output of this process will be buffered into a buffer. Therefore, output of Python programs that have any text sent to the stdout may be buffered and not shown until the newline character ( \\n ) is sent. This program won't print anything in your Python console or terminal when being run: import time for i in range ( 5 ): print ( i , end = \" \" ) time . sleep ( . 2 ) In contrast, if there is a print call (which by default has the newline character as its end parameter ), the output will be shown; however, all the numbers will be printed at once (not one after another with 0.2 second interval) : import time for i in range ( 5 ): print ( i , end = \" \" ) time . sleep ( . 2 ) print () To be able to see each number being printed instead of waiting for the loop to complete and see them all at once, one can change the stdout buffering with the stdbuf utility. However, the end parameter has to be a newline character: $ stdbuf -oL python3 program.py > result.log Alternatively, one can use the flush parameter of the print function: import time for i in range ( 5 ): print ( i , flush = True ) time . sleep ( 2 ) and the call becomes (running tail -F result.log will let you see numbers printed in real time): $ python3 std.py > result.log A solution that does not involve flushing is to set the PYTHONUNBUFFERED environment variable. When this environment variable is set, the stdout of the Python process will be sent to the active terminal in real time (which can be useful for tailing any application logs, particularly inside a Docker container). The same effect can also be achieved by passing the -u parameter: $ python3 -u std.py > result.log Happy printing!","tags":"python","url":"/working-with-stdout-python.html","loc":"/working-with-stdout-python.html"},{"title":"Building cli Python applications with Click","text":"Overview When writing cli tools using Python, if the complexity is low, using a plain argparse may suffice. Despite being a built-in module, it's still very capable and relatively flexible. In fact, a few large open-source projects have survived using argparse without using any custom cli frameworks. For instance, Conan – a popular C/C++ package manager written in Python – and Google API Client for Python – Google's discovery based APIs – are using argparse for their cli interfaces. When argparse limitations get in the way, you may start looking for Python frameworks that allow developing cli applications . There is a post with practical demonstrations of most popular Python cli frameworks that is worth reviewing: Building Beautiful Command Line Interfaces with Python . Building a cli with Click My personal preference for a Python cli framework is Click . It has the functionality I want to have when building cli applications and whenever I needed something a bit peculiar, I was able to find the answers online thanks to posts of Stephen Rauch . To save time for others, I've created a boilerplate repository – click-cli-boilerplate – that contains everything that one would need to get started developing a cli application using Click . It features the Python project source code layout, cli interface and implementation relation, tests, packaging, and docs generation. You will find some brief notes on how to write tests, how to generate the docs using the sphinx-click extension, and how to distribute the cli application as a Python wheel and let users install it with the pipx . Happy cli-ing!","tags":"python","url":"/building-cli-python-apps-with-click.html","loc":"/building-cli-python-apps-with-click.html"},{"title":"Brief overview of using Git LFS for managing binary files","text":"Overview Normally a Git repository is used to manage source code which is stored most often as plain text. Tracking changes for text is very easy because only the changes between two commits would need to be saved, not the whole copies of the files. However, a project source code repository may also contain binary files such as images, compiled code, or archives. Developers from quite a few industries such as gaming or computer-aided design and digital mapping (e.g. textures, CAD drawings, and map style files) often have to manage and store large files. Having files of a few megabytes or hundreds of megabytes in size can be very common in the project source code repository, however, there is nothing wrong with having them there since this is where they really belong. Problem of keeping binary files under Git Because Git cannot track changes between binary files, for each modification of a binary file, a copy of the modified file will be created and stored. This can make the repository unnecessary large and slow to clone and check out. Always overwriting the binary file with the latest file state (to keep only the \"latest\") defeats the purpose of the source code management as one should be able to have access to the history even if it implies storing a hundred of binary files each differing from others by just a few bytes. What is a large file is a subject for discussion. I'd also encourage to think about how often a binary file will change; if it's a couple of megabytes static image used in a background of your terminal app, you may be fine just storing it as is. If it's a dynamic file that will be modified daily by multiple developers, just half a megabyte digital drawing file can bloat the repository for all of time if it's modified often. Having many tiny binary files that are changed often can have a similar effect. Using Git LFS for tracking binary files A more efficient way to store the binary files is to store them not under the Git repository (when a change to a binary file will cause creating its full copy), but in a separate storage system such as Git LFS . This system lets you store in the Git repository only the pointers to versions of the binary files, whereas the files themselves are stored separately. When cloning the repository with the latest master , you will only need to download the latest file, not its whole history. When checking out a feature-branch (that may have another representation of the very same file), another file version will be downloaded. Most of the major source code management providers such as GitHub, GitLab, and BitBucket provide support for Git LFS and enabling it is extremely easy. To learn more about Git LFS and support for large objects in Git, see the excellent video Native Git support for large objects from the Git Merge 2019. Migration of files to LFS The decisions about management of the binary files should be made as early as possible when setting up the repository. This is because it's a lot easier to start using Git LFS when a new repository is created rather than when binary files have already sneaked into the Git history. Ideally, you shouldn't be tracking with Git binary files that are supposed to be modified often. If the files did sneak into the Git history, simply removing the files and then starting storing them in a separate LFS system won't be enough as the Git repository will still have copies of those binary files in the history (in the .git directory). It is possible to remove them completely, but this would require \"rewriting\" the history and would require careful coordination with anyone else using the repository to run a few git rebase --onto sessions. For a repository with a few branches used by a few developers this won't be a problem, but it can become impractical and plain tedious to migrate the files for a large repository with many contributors and many branches. If you do need to move the files out of a \"regular\" Git to the LFS system, refer to the Migrating existing repository data to LFS page section in the Git LFS tutorial. Happy storing!","tags":"git","url":"/overview-using-git-lfs-binary-files.html","loc":"/overview-using-git-lfs-binary-files.html"},{"title":"Running Python tests with tox in a Docker container","text":"Overview When you are working on Python code that is supposed to be running on Python interpreters of multiple versions (and potentially with multiple versions of 3rd party packages), to be able to test that your code works and produces expected result you would need to create isolated virtual environments. Each of these virtual environments will have a certain version of Python and a certain version of each 3rd party package that your programs depend on. By having just a few versions of Python with a couple of versions of a few packages, it becomes rather tedious to create and maintain those virtual environments manually very soon. tox is a tool that can help you with this. Preparing Python virtual environments It is possible to create Python virtual environments manually and then let tox use them, however, you would most likely want tox to generate those virtual environments for you. For tox to use Python interpreters of multiple versions, they have to be installed on your machine. Even though this is possible, it may still be less optimal given that you will most likely need to make system changes (install a system package on Linux, use homebrew on MacOS, or download a Python app or an installer on Windows). Fortunately, tox can be run in a Docker container which will help to prevent cluttering your system. Running tests with tox in Docker: simple configuration To be able to run Python tests with tox in a Docker container, you will need a Dockerfile. FROM ubuntu:18.04 RUN apt-get -qq update RUN apt-get install -y --no-install-recommends \\ python3.7 python3.7-distutils python3.7-dev \\ python3.8 python3.8-distutils python3.8-dev \\ wget \\ ca-certificates RUN wget https://bootstrap.pypa.io/get-pip.py \\ && python3 get-pip.py pip == 19 .1.1 \\ && rm get-pip.py RUN python3.6 --version RUN python3.7 --version RUN python3.8 --version RUN pip3 install tox pytest The tox.ini file where you specify the Python environments. [tox] envlist = py36,py37,py38 skipsdist = True [testenv] deps = pytest commands = pytest The test_module.py containing a simple test function. def test_foo (): assert 2 + 3 == 5 Now you can build an image and then run the tests. $ docker build -t snake . $ docker run -it -v ${ PWD } /:/app snake /bin/sh -c 'cd app; tox' The pytest output will be printed for each of the Python environments in which the tests have been run (posted below with some sections removed for brevity). using tox . ini : / app / tox . ini ( pid 7 ) ... [ 16 ] / app$ / app / . tox / py36 / bin / pytest ================================================= test session starts ================================================= platform linux -- Python 3.6.9, pytest-6.1.2, py-1.9.0, pluggy-0.13.1 cachedir : . tox / py36 / . pytest_cache rootdir : / app collected 1 item test_module . py . [ 100 % ] ================================================== 1 passed in 0 . 01 s ================================================== ... [ 21 ] / app$ / app / . tox / py37 / bin / pytest ================================================= test session starts ================================================= platform linux -- Python 3.7.5, pytest-6.1.2, py-1.9.0, pluggy-0.13.1 cachedir : . tox / py37 / . pytest_cache rootdir : / app collected 1 item test_module . py . [ 100 % ] ================================================== 1 passed in 0 . 01 s ================================================== ... [ 26 ] / app$ / app / . tox / py38 / bin / pytest ================================================= test session starts ================================================= platform linux -- Python 3.8.0, pytest-6.1.2, py-1.9.0, pluggy-0.13.1 cachedir : . tox / py38 / . pytest_cache rootdir : / app collected 1 item test_module . py . [ 100 % ] ================================================== 1 passed in 0 . 01 s ================================================== _______________________________________________________ summary _______________________________________________________ py36 : commands succeeded py37 : commands succeeded py38 : commands succeeded congratulations :) Running tests with tox in Docker: advanced configuration For a more complex use case, for instance, when you are working on a library that depends on some 3rd Python package, say, pandas , you can specify which versions of pandas you'd like to test your project's code with. For the example below, your tests will be run in 6 different environments. The tox.ini configuration file. [tox] envlist = py36-pandas{112,113}, py37-pandas{112,113}, py38-pandas{112,113} skipsdist = True [testenv] deps = pandas112: pandas==1.1.2 pandas113: pandas==1.1.3 pytest commands = pytest The test_pandas.py containing a simple test function to create two data frames and compare them. import pandas as pd from pandas._testing import assert_frame_equal def test_pandas (): df1 = pd . DataFrame ({ 'a' : [ 1 , 2 ], 'b' : [ 3 , 4 ]}) df2 = pd . DataFrame ({ 'a' : [ 1 , 2 ], 'b' : [ 3 , 4 ]}) assert_frame_equal ( df1 , df2 ) The pytest output will be printed for each of the Python environments in which the tests have been run (posted below with some sections removed for brevity). [ 52 ] / app$ / app / . tox / py36 - pandas112 / bin / pytest =================================================================== test session starts =================================================================== platform linux -- Python 3.6.9, pytest-6.1.2, py-1.9.0, pluggy-0.13.1 cachedir : . tox / py36 - pandas112 / . pytest_cache rootdir : / app collected 2 items test_module . py . [ 50 % ] test_pandas . py . [ 100 % ] ==================================================================== 2 passed in 0 . 63 s ==================================================================== [ 73 ] / app$ / app / . tox / py36 - pandas113 / bin / pytest =================================================================== test session starts =================================================================== platform linux -- Python 3.6.9, pytest-6.1.2, py-1.9.0, pluggy-0.13.1 cachedir : . tox / py36 - pandas113 / . pytest_cache rootdir : / app collected 2 items test_module . py . [ 50 % ] test_pandas . py . [ 100 % ] ==================================================================== 2 passed in 0 . 47 s ==================================================================== [ 115 ] / app$ / app / . tox / py37 - pandas112 / bin / pytest =================================================================== test session starts =================================================================== platform linux -- Python 3.7.5, pytest-6.1.2, py-1.9.0, pluggy-0.13.1 cachedir : . tox / py37 - pandas112 / . pytest_cache rootdir : / app collected 2 items test_module . py . [ 50 % ] test_pandas . py . [ 100 % ] ==================================================================== 2 passed in 0 . 51 s ==================================================================== [ 133 ] / app$ / app / . tox / py37 - pandas113 / bin / pytest =================================================================== test session starts =================================================================== platform linux -- Python 3.7.5, pytest-6.1.2, py-1.9.0, pluggy-0.13.1 cachedir : . tox / py37 - pandas113 / . pytest_cache rootdir : / app collected 2 items test_module . py . [ 50 % ] test_pandas . py . [ 100 % ] ==================================================================== 2 passed in 0 . 42 s ==================================================================== [ 174 ] / app$ / app / . tox / py38 - pandas112 / bin / pytest =================================================================== test session starts =================================================================== platform linux -- Python 3.8.0, pytest-6.1.2, py-1.9.0, pluggy-0.13.1 cachedir : . tox / py38 - pandas112 / . pytest_cache rootdir : / app collected 2 items test_module . py . [ 50 % ] test_pandas . py . [ 100 % ] ==================================================================== 2 passed in 0 . 48 s ==================================================================== [ 193 ] / app$ / app / . tox / py38 - pandas113 / bin / pytest =================================================================== test session starts =================================================================== platform linux -- Python 3.8.0, pytest-6.1.2, py-1.9.0, pluggy-0.13.1 cachedir : . tox / py38 - pandas113 / . pytest_cache rootdir : / app collected 2 items test_module . py . [ 50 % ] test_pandas . py . [ 100 % ] ==================================================================== 2 passed in 0 . 38 s ==================================================================== _________________________________________________________________________ summary _________________________________________________________________________ py36 - pandas112 : commands succeeded py36 - pandas113 : commands succeeded py37 - pandas112 : commands succeeded py37 - pandas113 : commands succeeded py38 - pandas112 : commands succeeded py38 - pandas113 : commands succeeded congratulations :) There are quite a few resources online that go deeper into how one can use tox in a Docker container, but this simple layout has been very useful to me in various circumstances and may help others. Happy testing!","tags":"python","url":"/run-python-tests-with-tox-in-docker.html","loc":"/run-python-tests-with-tox-in-docker.html"},{"title":"Using Docker for Python development: cheat sheet","text":"The Docker framework can be an extremely useful tool for any Python developer who wants to run their Python programs (either for development or testing purposes) in a certain isolated environment with a pre-defined set of system and Python packages installed. Docker can also help with testing your Python code against multiple versions of the Python packages in multiple operating systems. The beauty of Docker is that you don't need to understand the intricate details of how Docker technology works to take advantage of it. This blog post contains recipes any Python developer can benefit from regardless how experienced you are with Docker and containerization techniques. Each recipe or scenario is based on a problem that one may need to solve and provides a solution in form of a Dockerfile file, a docker build , and a docker run command. Basic setup The base Docker image you'll be using will likely to be different depending on a number of factors, however, to keep the build time short, I'll be using the alpine image in most cases. Dockerfile contents: FROM python:3.7-alpine CMD [ \"python3\" , \"--version\" ] The build step contains the -t flag which defines the tag name of the image that will be built. The dot ( . ) tells Docker to use the the file named Dockerfile in the current directory. Building a Docker image: $ docker build -t snake . When an image is run, the CMD command found in the Dockerfile is executed. The --rm (remove) flag will make sure to delete the container once the run command is complete. Running a Docker image: $ docker run --rm snake # Python 3.7.9 Experiment with a Python REPL of any version FROM python:3.7.8-alpine CMD [ \"python3\" ] By changing the version, you can get into an interactive Python console for the given version. This is very handy when you want to test how a certain feature works in a newer or an older Python version. When a docker run command is executed, it will run the CMD command and exit, so you won't be able to interact with the REPL. To work with the container in an interactive mode, the -it flag should be used. $ docker run --rm -it snake # Python REPL becomes available Passing a command to a Python Docker image FROM python:3.7.8-alpine When running a Docker container, it's possible to pass a command, optionally, with additional arguments. Python provides support for running a command with the -c option so that it's possible to supply the program code as a string. This can be very handy when you need to have a one-liner for an operation that will return a value you may need later as input for the subsequent operations. $ docker run --rm -it snake python3 --version # Python 3.7.8 $ docker run --rm -it snake python3 -c \"import sys; print(sys.platform)\" # linux Run a Python program from the host in a Docker container FROM python:3.7.8-alpine It is possible to mount a local directory (on your disk) as a volume to a Docker container which is done with the -v parameter. Running the command below will make the app directory files available in the Docker container. This approach can be used when you want to run a Python program in a Docker container likely having a different system environment and Python version installed. Having this Python program (stored at app/main.py ): import sys print ( sys . version_info ) you can execute it with: $ docker run -v ${ PWD } /app:/app snake python3 /app/main.py # sys.version_info(major=3, minor=7, micro=8, releaselevel='final', serial=0) It is also possible to copy files to the Docker image when the image is being built if you don't want to mount any volumes at the run time. FROM python:3.7.8-alpine WORKDIR /opt/project/app COPY ./app CMD [ \"python3\" , \"main.py\" ] You can now run the Docker container to execute the main.py file that was copied: $ docker run --rm snake # sys.version_info(major=3, minor=7, micro=8, releaselevel='final', serial=0) Alternatively, you could also make the CMD command a part of the docker run : FROM python:3.7.8-alpine WORKDIR /opt/project/app COPY ./app Then you pass the arguments to the container from the shell instead: $ docker run --rm snake python3 main.py # sys.version_info(major=3, minor=7, micro=8, releaselevel='final', serial=0) Get into a Docker container shell and run a command FROM python:3.7.8-alpine WORKDIR /opt/project/app COPY ./app . It is possible to start a Docker container and run a shell console to execute arbitrary commands. This resembles connecting to a remote machine via an SSH connection or using a local Bash console. $ docker run --rm -it snake /bin/sh # /opt/project/app # ls # main.py # /opt/project/app # python3 main.py # sys.version_info(major=3, minor=7, micro=8, releaselevel='final', serial=0) Access files created in the Docker container on the host Accessing files that are created by processes run in a Docker container is possible by mounting a volume. FROM python:3.7.8-alpine WORKDIR /opt/project/app Now you can run the container in the interactive mode and attach a shell console: $ docker run -it -v ${ PWD } /app:/opt/project/app snake /bin/sh # > touch foo.bar The foo.bar file will appear on your host disk under the app directory. This makes it possible to create arbitrary files within your Docker container saving them on your host disk making them accessible for further usage locally. Copy files produced by Docker build command to the host In certain cases, a Dockerfile may create new files which can be accessed when starting the container. FROM alpine RUN echo \"some data\" > /home/data.out The trick here is to mount your host's directory to some other directory than container's home and then copy the file(s) you need to this intermediate location. $ docker run --rm -it -v ${ PWD } /datadir:/home/datadir snake # cp /home/data.out /home/datadir/ At this point, the data.out file should appear on your host's disk under the datadir directory in your current working directory. Run Python tests stored on your host inside a Docker container To do this, one would need to have a Docker container with the pip installed and the necessary Python packages that are required by your tests. It can be wise to install a tested version of pip to avoid getting the latest one in case it's broken (this has happened before a few times). FROM python:3.7.8-alpine RUN wget -q https://bootstrap.pypa.io/get-pip.py \\ && python3 get-pip.py pip == 19 .1.1 \\ && rm get-pip.py COPY app/requirements.txt ./ RUN pip install -q --no-cache-dir \\ -r requirements.txt \\ && rm requirements.txt Given that the requirements.txt contains pytest and the app folder has modules with test functions, you should be able to run pytest against the mounted directory: $ docker run --rm -v ${ PWD } /app:/opt/project/app snake pytest /opt/project/ -v # pytest output Because your current working directory is mounted as a volume, the files generated by the commands you run will be available on the host. This means you can run the coverage command in the Docker container and the .coverage file will be available on the host. Given this file, you'll be able to generate the HTML report to view in your host's web browser. Make host environment variables available in a Docker container It is common to use environment variables for storing some settings that Python programs may depend on. Keep in mind that a more robust strategy for storing sensitive information is to use Docker secrets . FROM python:3.7.8-alpine By default, host's environment variables (both permanently stored and temporarily exported) are not available in a Docker container. Given this Python program, import os print ( f 'Token: { os . getenv ( \"SITE_TOKEN\" ) } ' ) you can run it in a Docker container: $ export SITE_TOKEN = 'mytoken' $ docker build -t snake . $ docker run -v ${ PWD } /app:/opt/project/app snake python3 /opt/project/app/main.py # Token: None The -e ( --env ) parameter will let you specify the environment variables you want to propagate into the Docker container. $ export SITE_TOKEN = 'mytoken' $ docker run -e SITE_TOKEN = ${ SITE_TOKEN } -v ${ PWD } /app:/opt/project/app snake \\ python3 /opt/project/app/main.py # Token: mytoken Attach to a running Docker container When running a Python program in a Docker container in debugging mode, it can be useful to be able to pause the program and connect to the container to be able to inspect the file system. A few IDEs such as PyCharm and VSCode provide support for remote Python debugging and will be able to start a Docker container running a Python program and then later tell you its id. This is especially useful when the Python program is expected to produce some files and you would like to inspect them to verify the program produces correct results. If you know the container id, you can attach to it with: $ docker exec -it <container_id> /bin/bash If you don't know the container id, you will need to get it first which can be done with: $ docker ps --filter status = running The CONTAINER ID field will contain the id of the Docker container you will need to attach to. If you have multiple running containers, the one you need is likely to be the first one in the list. Happy containerization!","tags":"python","url":"/docker-for-python-cheat-sheet.html","loc":"/docker-for-python-cheat-sheet.html"},{"title":"Using pyfakefs for unit testing in Python","text":"Overview of unit testing When writing unit tests for programs, it is commonly considered to be a good practice to avoid relying on any part of the system infrastructure such as: network connectivity (you can't get data from a web service) operating system functionality (you can't call grep ) additional software installations (you can't rely on having Microsoft Excel installed) Another suggestion is to avoid making modifications to the files on disk. Testing pieces of code where files may be created or modified often involves patching the functions responsible for writing on disk such as the built-in open function, various os module functions such as os.mkdir and os.makedirs , and pathlib.Path methods such as Path.touch and Path.open . If writing to file system doesn't happen very often, using a few simple patches will suffice. However, for more heavy data-driven programs or programs that are written for any kind of data processing, patching endless number of function calls throughout the code can become rather tedious very soon. Using system temp directory At some point, it may be more efficient to use a more relaxed approach which involves using the tempfile module to create and modify files within the operating system temporary directory which is guaranteed to exist and be writable (at least on POSIX). This approach has some limitations: one wouldn't be able to make changes to files at system paths if this is an essential part of the program functionality unit tests writing on disk will become slower and with many of them can slow down the development-testing iterative cycle running tests in parallel (or using multithreading) can be unreliable as multiple tests may attempt to write/read to/from the very same files at the same time running a test making file system modifications can leave the system in a favourable state for the subsequent tests to be run which can lead to flaky tests Using virtual file system Alternatively, a more robust approach is to not write on disk and instead use a virtual, in-memory file system. For Python, there is a package called pyfakefs that makes it possible. Surprisingly it's not very well known in the Python community and I thought it would be helpful to share the word as I find this package to be indispensable in unit testing of programs which work heavily with files. The package can be used both with unittest and pytest frameworks and under any operating system. Here is a trivial example of writing a unit test for a function that merges content of all files within a given directory into a new file in the same directory. from pathlib import Path from pyfakefs.pytest_plugin import Patcher as FakeFileSystem from utils import merge_files def test_merge_files (): with FakeFileSystem () as _ : dest_dir = Path ( '/opt/data' ) dest_dir . mkdir ( parents = True ) dest_dir . joinpath ( 'file1' ) . write_text ( 'line1 \\n line2 \\n ' ) dest_dir . joinpath ( 'file2' ) . write_text ( 'line3 \\n line4 \\n ' ) merge_files ( source = dest_dir , target = 'result' ) assert dest_dir . joinpath ( 'result' ) . read_text () == 'line1 \\n line2 \\n line3 \\n line4 \\n ' Please refer to the pyfakefs documentation to learn more. Virtual file system caveats A few notes that can help to avoid common pitfalls: make sure not to construct Path objects outside of the patching context (the FakeFileSystem() in the example above) because it will otherwise be pointing to the real file system since the Path class has not been patched yet when using the fake file system for integration tests, keep in mind that you won't be able to use any external tools such as file or cp commands to interact with the fake file system files to verify that you are using the virtual, fake file system in your tests, you can choose to create files in a directory where you won't have modify permissions on your real file system – this will help you identify any cases where pyfakefs support is limited watch closely the permissions the user running the tests have as pyfakefs will operate under the root if run in a Docker container do not use the operating system temporary directory as the fake file system destination directory because pyfakefs doesn't patch the tempfile module Happy faking!","tags":"python","url":"/intro-pyfakefs-python-testing.html","loc":"/intro-pyfakefs-python-testing.html"},{"title":"Brief overview of the reproducible builds concept","text":"Introduction When working with the source code in a project that has multiple build steps (compiling, linking, patching, packaging) when a final \"product\" – a Debian package, an installable application, or an executable with shared libraries – is produced, there are many reasons why it can be useful to be able to get the same binary code (bit-by-bit) from the same source code. If you are able to build your project source code and then re-build it again later (without making any changes to the source code) and the produced artifacts are identical, it is said that your builds are reproducible/deterministic . How can one set up a reproducible build? According to the https://reproducible-builds.org definition: A build is reproducible if given the same source code, build environment and build instructions, any party can recreate bit-by-bit identical copies of all specified artifacts. For a simple project with a small number of movings parts, it may be relatively easy to achieve reproducible builds whereas for a corporate software development project this can be a challenge. There are multiple reasons why the binaries produced by a build operation may differ between builds run from the same source code. There are a few resources that will help you get started: An introduction to deterministic builds with C/C++ provides a gentle introduction to the concept and its importance and benefits Reproducible builds will help you learn more about software development practices around the reproducible builds. Elements of indeterminism Two most common issues are timestamps (when the source code is built) which may be saved into the produced binaries and path information (the location of the source code files on disk) which can also be included into the output binaries. However, many other things can have impact and make two binaries different (they may have the same size, but still be different when doing bit-by-bit comparison). Some of the things you will have control from the build system tools perspective such as compilers and linkers. For instance, you can control the order in which files are being processed as file systems generally do not make any promises that when you iterate the files in a given directory, they will be retrieved in the same order at all times. Other things may be defined in your custom post-processing logic – for instance, the order in which you set certain properties on a binary (such as RPATH patching) can also result in two different binaries. Sample project I have created a GitHub repository with the source code files that have been used in the Conan article An introduction to deterministic builds with C/C++ and it is available at reproducible-builds-example . This example project demonstrates the concept of reproducible builds with a few C++ source files and CMake build steps. A great tool that will help you compare the binaries in your effort to achieve reproducible builds is Diffoscope . It is extremely powerful and has functionality for generating HTML reports showing the difference between two objects you are comparing. This makes it so much easier to see why your binaries are different. Below is a screenshot of the HTML report that shows the difference between two executables. Happy diffing!","tags":"build-systems","url":"/intro-reproducible-builds.html","loc":"/intro-reproducible-builds.html"},{"title":"Using quicktype.io service to create Python interfaces from JSON","text":"Introduction For the last few years I had to write a few simple Python wrappers around a couple of external services. There are many advantages to having a nice Pythonic interface into libraries and tools that are written in other programming languages. It often makes it easier to automate, more pleasant to interact with, and faster to program in general. Bindings and wrappers Some wrappers simply expose the original interfaces without adding anything – these are plain bindings and this is often the case for C++ libraries that have Python bindings such Qt with PyQt . Python code you'd write using plain Python bindings may not feel very Pythonic (due to camelCase ) and because you often have to write programs using other, non-Pythonic, paradigms such as obj.setColor('Red') instead of obj.color = 'Red' . It is, in fact, not uncommon to write Python wrappers around Python bindings for C++ libraries simply because the Python bindings do not make Python developers who use them much more productive. Another group of Python wrapping effort exists around wrapping web services interaction to avoid dealing with cumbersome HTTP requests construction, response processing, and service communication. Likewise, wrapping a CLI tool in Python can be very useful if this is the only way to interact with the underlying software. Working with JSON No matter how you are getting back a JSON response – from a web service or from a CLI tool – you will need to process it to either present the result to the end user or to manage it in some other way. When dealing with JSON data, the built-in json module comes in handy and extracting the information you need out of a JSON object is trivial. You could also take advantage of higher level HTTP communication library such as requests . At the beginning, the code may look something like this: import requests data = requests . get ( 'https://s2k7tnzlhrpw.statuspage.io/api/v2/status.json' ) . json () status = data [ 'status' ][ 'description' ] updated_at = data [ 'page' ][ 'updated_at' ] print ( f \"Status: { status } \\n Updated at: { updated_at } \" ) Output: Status: All Systems Operational Updated at: 2020-08-12T08:08:25.828Z Interacting with the returned JSON objects using only the json module will suffice for smaller scripts and ad-hoc web service interrogation. If you'd like to build a Python wrapper around a large REST interface with many endpoints, however, it may be useful to think about having higher level abstractions for the data entities you deal with. The code snippet above has a number of issues: it relies on having the data elements present when using accessing JSON objects (you could work around it using the .get() method – data.get('status', {}).get('description', 'N/A') but it is still very fragile) as JSON objects keys are represented as strings, it's impossible to run any static type checker (and it has additional complications – refactoring becomes really hard) it makes it hard to reason about the data entities as their data type is not obvious (and you would have to provide a type hint for each JSON object such as status: Dict[str, str] = data['status'] which will become tedious very quickly) Representation of JSON as Python classes To make it easier to interact with JSON objects, they can be used to construct instances of Python classes which are much easier to work with: they provide nice abstraction, they are easy to write unit tests for, and the code that uses them can be inspected with a static analysis tool such as mypy . import requests from typing import Optional from datetime import datetime from dateutil import parser class Page : id : Optional [ str ] name : Optional [ str ] url : Optional [ str ] time_zone : Optional [ str ] updated_at : Optional [ datetime ] def __init__ ( self , id : Optional [ str ], name : Optional [ str ], url : Optional [ str ], time_zone : Optional [ str ], updated_at : Optional [ str ]): self . id = id self . name = name self . url = url self . time_zone = time_zone self . updated_at = parser . parse ( updated_at ) class Status : indicator : Optional [ str ] description : Optional [ str ] def __init__ ( self , indicator : Optional [ str ], description : Optional [ str ]): self . indicator = indicator self . description = description class System : page : Optional [ Page ] status : Optional [ Status ] def __init__ ( self , data ): self . page = Page ( ** data [ 'page' ]) self . status = Status ( ** data [ 'status' ]) data = requests . get ( 'https://s2k7tnzlhrpw.statuspage.io/api/v2/status.json' ) . json () system = System ( data ) status = system . status . description updated_at = system . page . updated_at . strftime ( ' %d /%m/%Y' ) print ( f \"Status: { status } \\n Updated at: { updated_at } \" ) Output: Status: All Systems Operational Updated at: 2020-08-12T08:08:25.828Z Having these classes will solve the issues that the original code snippet had. You can now extend the classes with more fields and add additional logic to any class – the Page class can have a local time zone property or the Status.description can be an instance of the StatusType(Enum) class, for instance. Autogeneration of Python classes from JSON It would be very useful if one could generate Python classes declarations from an API specification file. Swagger tools make it possible to generate an API specification which one could then convert into a collection of Python classes. This approach is very useful but the generated Python classes would be simply data classes without any logic – your fields with the date would be strings, not datetime objects. I think it works best for APIs that change often, during the development when you are iterating on the API design, or when having the raw data classes is sufficient. Another approach is to auto-generate a collection of Python classes from the API specification and extend their initialization logic and to add additional fields/methods as required. This approach has worked well for me and would be particularly useful for any internal tooling when you have control over the API changes. I found the QuickType.io – the service that can convert JSON into typesafe code in many languages including Python – to be really helpful. The classes declared in the snippet above have been generated by quicktype.io from JSON and then modified so that the root class System will have other class instances as its fields. That is, you just have to provide the root JSON object and the root class System will populate all its fields with respective classes as required. For this, a handy Python feature of unpacking keyword arguments with ** is used. This way, the quicktype.io service generates all the boilerplate Python code needed and then some additional modification can be done (e.g. to overload the __repr__ magic method to dump a JSON representation of the class instance). I think you will see the value of using a Python class to represent a JSON object very quickly and with the help of quicktype.io , autogeneration of Python data classes is incredibly easy. Happy automating!","tags":"python","url":"/quicktype-json-class-generation.html","loc":"/quicktype-json-class-generation.html"},{"title":"Building Python extension modules for C++ code with pybind11","text":"Introduction If you ever needed to provide interface to the C/C++ code from your Python modules, you may have used Python extension modules . They are typically created when there is an existing C++ project and it is required to make it accessible via Python bindings. Alternatively, when performance becomes critical, a certain part of the Python project can be written in C/C++ and made accessible to the rest of the Python codebase via some kind of interface. Quite a few large C++ libraries and frameworks have associated Python bindings – they can be used for prototyping or simply to speed up the development as writing a Python program is supposed to take less time than writing an equivalent C++ program. Exposing your library interface with another popular language, such as Python, will also make your project more accessible for programmers who are not very familiar with C++. Refer to excellent RealPython: Python Bindings: Calling C or C++ From Python article to learn more. Python bindings There are quite a few options on how you can make your C++ code accessible from Python. However, I have personally worked only with SWIG and pybind11 so far. For now, let's focus on pybind11 . It's extremely easy to set up on Linux or Windows and you should be able to create a compiled Python extension module ( .so for Linux and .pyd for Windows) very quickly. The pybind11 documentation does provide excellent reference information with a ton of examples. However, those examples often demonstrate features in isolation and I thought it would be useful to share an example of a more complete \"library\" where multiple examples are combined into something that looks like a MVP. Writing C++ code Here is the C++ file, Geometry.cpp , I've written to demonstrate the pybind11 features. It showcases constructing custom Point class instances, finding the distance between them in 2D and 3D space, and overloading C++ comparison operators among a few other things. #include <pybind11/pybind11.h> #include <pybind11/operators.h> #include <string> #include <sstream> #include <iomanip> #include <cmath> namespace py = pybind11 ; using namespace std ; class Point { public : Point ( const double & x , const double & y ) : x ( x ), y ( y ) { z = numeric_limits < double >:: quiet_NaN (); py :: print ( \"Constructing a point with z set to nan\" ); } Point ( const double & x , const double & y , const double & z ) : x ( x ), y ( y ), z ( z ) { py :: print ( \"Constructing a point with z set to a user given value\" ); } double x ; double y ; double z ; string shapeType = \"Point\" ; double distanceTo ( Point point , bool in3D ) { if ( in3D ) { if ( ! isnan ( z ) && ! isnan ( point . z )) { return sqrt ( pow (( point . x - x ), 2 ) + pow (( point . y - y ), 2 ) + pow (( point . z - z ), 2 )); } else { py :: print ( \"Cannot measure distance between points in XY and XYZ space\" ); return numeric_limits < double >:: quiet_NaN (); } } else { return sqrt ( pow (( point . x - x ), 2 ) + pow (( point . y - y ), 2 )); } } bool static areEqual ( Point left , Point right ) { if ( isnan ( left . z ) && isnan ( right . z )) { return left . y == right . y && left . x == right . x ; } else if ( isnan ( left . z ) &#94; isnan ( right . z )) { return false ; } else { return left . y == right . y && left . x == right . x && left . z == right . z ; } } friend bool operator == ( const Point & left , const Point & right ) { return areEqual ( left , right ); } friend bool operator != ( const Point & left , const Point & right ) { return ! areEqual ( left , right ); } bool is3D () const { return ! isnan ( z ); } }; PYBIND11_MODULE ( Geometry , m ) { m . doc () = \"C++ toy geometry library\" ; py :: class_ < Point > ( m , \"Point\" , \"Point shape class implementation\" ) . def ( py :: init < const double & , const double &> (), py :: arg ( \"x\" ), py :: arg ( \"y\" )) . def ( py :: init < const double & , const double & , const double &> (), py :: arg ( \"x\" ), py :: arg ( \"y\" ), py :: arg ( \"z\" )) . def_readonly ( \"x\" , & Point :: x ) . def_readonly ( \"y\" , & Point :: y ) . def_readonly ( \"z\" , & Point :: z ) . def_readonly ( \"shapeType\" , & Point :: shapeType ) . def ( \"distanceTo\" , & Point :: distanceTo , py :: arg ( \"point\" ), py :: arg ( \"in3D\" ) = false , \"Distance to another point\" ) . def ( \"is3D\" , & Point :: is3D , \"Whether a point has a valid z coordinate\" ) . def ( py :: self == py :: self ) . def ( py :: self != py :: self ) . def ( \"__repr__\" , []( const Point & point ) { stringstream xAsString , yAsString , zAsString ; xAsString << std :: setprecision ( 17 ) << point . x ; yAsString << std :: setprecision ( 17 ) << point . y ; zAsString << std :: setprecision ( 17 ) << point . z ; if ( point . z != 0 ) { return \"Point (\" + xAsString . str () + \", \" + yAsString . str () + \", \" + zAsString . str () + \")\" ; } else { return \"Point (\" + xAsString . str () + \", \" + yAsString . str () + \")\" ; } }); } Building a Python extension module Once you have the Geometry.cpp file on disk and pybind11 installed, you should be able to compile the C++ code and link it to the Python headers: $ c++ -O3 -Wall -shared -std = c++11 -fPIC ` python3 -m pybind11 --includes ` Geometry.cpp -o Geometry ` python3-config --extension-suffix ` If you are not very familiar with Bash, command in the backticks in the command above are evaluated by the shell before the main command. The python3 -m pybind11 --includes part is used to get the location of Python header files and the python3-config --extension-suffix part is used to get the suffix for the shared library name – for CPython 3.6 on a 64bit Ubuntu, the Geometry.cpython-36m-x86_64-linux-gnu.so file will be created. Now, once you have the shared library file, it can be imported and used pretty much as if it was a regular Python module. Using a Python extension module Let's see our library in action by running the file python3 use_geometry.py containing the code below: import math from Geometry import Point # runtime dispatch of init constructors print ( Point . __init__ . __doc__ ) # a method signature and its docstring print ( Point . distanceTo . __doc__ ) p1 = Point ( 10 , 20 ) p2 = Point ( 20 , 30 ) p3 = Point ( 20 , 30 ) p4 = Point ( 50 , 60 , 45.67 ) p5 = Point ( 50 , 60 , 45.67 ) assert p1 . distanceTo ( p2 ) == math . sqrt ( 200 ) # check operator overloading works assert p1 != p2 assert p2 == p3 assert not p2 != p3 assert p4 == p5 assert not p4 == p3 assert math . isnan ( p1 . z ) # check distance between 3D points p1 = Point ( 50 , 60 , 45 ) p2 = Point ( 50 , 60 , 75 ) print ( p1 . distanceTo ( p2 , in3D = True )) # check __repr__ print ( p1 ) print ( p2 ) The produced output: __init__(*args, **kwargs) Overloaded function. 1. __init__(self: Geometry.Point, x: float, y: float) -> None 2. __init__(self: Geometry.Point, x: float, y: float, z: float) -> None distanceTo(self: Geometry.Point, point: Geometry.Point, in3D: bool = False) -> float Distance to another point Constructing a point with z set to nan Constructing a point with z set to nan Constructing a point with z set to nan Constructing a point with z set to a user given value Constructing a point with z set to a user given value Constructing a point with z set to a user given value Constructing a point with z set to a user given value Distance between Point (50, 60, 45) and Point (50, 60, 75) is 30.0 Representation of the p1 is \"Point (50, 60, 45)\" Representation of the p2 is \"Point (50, 60, 75)\" This is of course a very trivial example of pybind11 usage, however there have been successful attempts to use pybind11 for binding existing large C++ libraries such as CGAL and Point Cloud Library . See an example of wrapping some of the CGAL functionality with pybind11 and Python bindings for the Point Cloud Library to learn more. Happy binding!","tags":"python","url":"/pybind11-python-bindings.html","loc":"/pybind11-python-bindings.html"},{"title":"What I think a great tech support should look like","text":"Being a software engineer implies that you may be interacting with customers particularly if you are working in a small company. If you are wearing multiple hats or if you are in professional services or tech support, you will likely be contacted by your customers about issues they may experience using your product – a web site, a desktop or a mobile app, or some hardware equipment. Whatever it is, I believe it's best to have a clear plan of actions outlined which you can follow when addressing a customer's issue. No matter how fine-grained your protocol of communication with the customer already is, you can always incorporate some of the ideas I have about how I'd like to work with a customer that needs help. I do understand that it may be difficult or unrealistic to follow the steps below exactly, but this is my vision of a great support. Customer gets in touch Customer reaches out to you because some functionality on the web site of a business system your team built doesn't work as they expect. Pace yourself Do not attempt to provide any solution or ask any questions just yet. If your guess will be accurate – you ask to enable JavaScript in a web browser or to log out and log back in – the customer is less likely to share any details with you because they can now continue working. You, on the other hand, are missing a chance to document the incident to see if it would be possible to prevent it from happening again and to share it with your team members. What was the version of the web browser where the JavaScript is not enabled by default? Did they really have to log out and log back in or a simple refresh of a web page would be enough (you may tweak server-side caching settings)? You may never figure this out and even though the problem is \"solved\" and the customer is happy, in the long run this was a loss. Show empathy No matter what channel of communication is being used – a phone call or an email – the first thing you do is to show some empathy: being an engineer, you know better than anyone how frustrating it can be when a computer doesn't do what you want it to do. Collect information You can start collecting the information you will need to troubleshoot now; don't ask for information you can collect yourself, but sometimes it may still be useful – even if you can SSH into a server to get the version of a software installed, it would still be very useful to ask the customer to tell you the version they see on the web page as an older version may indicate a web page caching issue. You would ideally have a pre-defined template document where you can fill all the information you may need so that you don't have think about it during a phone call. Share information Once you have gathered all the information, make sure to share it with the customer. If on the phone, read it back to them. If it's an email conversation, either share the complete document or provide access to the internal support system (if any) where they would be able to check that the information they've shared with you is accurate. This will help to avoid any misunderstandings and provide traceability as the customer will acknowledge that the information they have provided is correct. Doing troubleshooting Depending on how urgent the request is, you may or may not have time to do some manual production inspection before telling the customer how to fix their problem. If they are in a middle of a presentation to a board of directors, it may be best to tell them how to fix the issue immediately. If it is not time critical, you may want to ask for some time to log in into the production environment to record as much as information as you possibly can. For instance, they told you that they are still able to use the system despite not being logged in. Instead of telling them to log off and log in hoping it will fix the problem (did you know that hope is not a strategy ?), you may want to log in into the server to find out what's going on with your authentication service while the user hasn't left their web browser session (provided you don't do any verbose logging of this type of events already). This issue may indicate some serious problem that is worth investigating further because it may manifest itself again at some point. Update on the progress If the problem requires more time and you will likely need to spend hours if not days working on it, it's best to let the customer know about the progress. They would be able to find it very helpful to see that you are working on their issue and ideally how much time you've spent (a support ticket can be \"in progress\" for 5 days, but the engineer may have been working on it for just 1 hour). Provide a temporary solution if applicable If it's an option, make sure to provide customer with a workaround to let them continue to do their business. If they need to process some files, offer to do it manually for them if possible. If their business operation depends on a feature from their \"basic\" plan that doesn't work and you know that an alternative feature from the \"advanced\" plan would work, upgrade their account for some time while you are troubleshooting. Tell about proposed solution Once you have identified the issue and have been able to solve it, tell the user what was happening and what you have done to solve it. Adjust the language depending on how technical things get as required but don't be afraid to offer them a chance to learn; many customers would find it helpful to understand how the product they use operate under the hood. Ask them to verify that the solution you've implemented works for them (after you have done everything you possibly could to verify this yourself first). Document your findings After you found the resolution to a problem, make sure to document not only what has to be done to fix it, but also what have you attempted to get done which didn't seem to help. For instance, you thought that the problem may be due to a broken database table index and have decided to re-build it. That didn't help and then you think that perhaps recalculating the table statistics may help. You do that now and, yes, the problem is gone. However, documenting that for this problem recalculating the table statistics is necessary may be misleading as re-building the table index may also be required. When you or a colleague of yours will be reading the incident documentation, they will know what have you attempted before finding the solution. Wherever possible, any changes to any environment should be happening via code or a terminal to make it easy to record as making changes in the GUIs are generally known to be very hard to document. A problem that can be solved purely by customers themselves (invalidating the web browser cache or to change some setting within the user interface of the business system itself) is a great candidate to be added into the user documentation. Happy supporting!","tags":"tech-support","url":"/my-version-of-a-great-tech-support.html","loc":"/my-version-of-a-great-tech-support.html"}]};