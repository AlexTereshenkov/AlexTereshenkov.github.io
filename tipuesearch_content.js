var tipuesearch = {"pages":[{"title":"Using pyfakefs for unit testing in Python","text":"Overview of unit testing When writing unit tests for programs, it is commonly considered to be a good practice to avoid relying on any part of the system infrastructure such as: network connectivity (you can't get data from a web service) operating system functionality (you can't call grep ) additional software installations (you can't rely on having Microsoft Excel installed) Another suggestion is to avoid making modifications to the files on disk. Testing pieces of code where files may be created or modified often involves patching the functions responsible for writing on disk such as the built-in open function, various os module functions such as os.mkdir and os.makedirs , and pathlib.Path methods such as Path.touch and Path.open . If writing to file system doesn't happen very often, using a few simple patches will suffice. However, for more heavy data-driven programs or programs that are written for any kind of data processing, patching endless number of function calls throughout the code can become rather tedious very soon. Using system temp directory At some point, it may be more efficient to use a more relaxed approach which involves using the tempfile module to create and modify files within the operating system temporary directory which is guaranteed to exist and be writable (at least on POSIX). This approach has some limitations: one wouldn't be able to make changes to files at system paths if this is an essential part of the program functionality unit tests writing on disk will become slower and with many of them can slow down the development-testing iterative cycle running tests in parallel (or using multithreading) can be unreliable as multiple tests may attempt to write/read to/from the very same files at the same time running a test making file system modifications can leave the system in a favourable state for the subsequent tests to be run which can lead to flaky tests Using virtual file system Alternatively, a more robust approach is to not write on disk and instead use a virtual, in-memory file system. For Python, there is a package called pyfakefs that makes it possible. Surprisingly it's not very well known in the Python community and I thought it would be helpful to share the word as I find this package to be indispensable in unit testing of programs which work heavily with files. The package can be used both with unittest and pytest frameworks and under any operating system. Here is a trivial example of writing a unit test for a function that merges content of all files within a given directory into a new file in the same directory. from pathlib import Path from pyfakefs.pytest_plugin import Patcher as FakeFileSystem from utils import merge_files def test_merge_files (): with FakeFileSystem () as _ : dest_dir = Path ( '/opt/data' ) dest_dir . mkdir ( parents = True ) dest_dir . joinpath ( 'file1' ) . write_text ( 'line1 \\n line2 \\n ' ) dest_dir . joinpath ( 'file2' ) . write_text ( 'line3 \\n line4 \\n ' ) merge_files ( source = dest_dir , target = 'result' ) assert dest_dir . joinpath ( 'result' ) . read_text () == 'line1 \\n line2 \\n line3 \\n line4 \\n ' Please refer to the pyfakefs documentation to learn more. Virtual file system caveats A few notes that can help to avoid common pitfalls: make sure not to construct Path objects outside of the patching context (the FakeFileSystem() in the example above) because it will otherwise be pointing to the real file system since the Path class has not been patched yet when using the fake file system for integration tests, keep in mind that you won't be able to use any external tools such as file or cp commands to interact with the fake file system files to verify that you are using the virtual, fake file system in your tests, you can choose to create files in a directory where you won't have modify permissions on your real file system – this will help you identify any cases where pyfakefs support is limited watch closely the permissions the user running the tests have as pyfakefs will operate under the root if run in a Docker container do not use the operating system temporary directory as the fake file system destination directory because pyfakefs doesn't patch the tempfile module Happy faking!","tags":"python","url":"/intro-pyfakefs-python-testing.html","loc":"/intro-pyfakefs-python-testing.html"},{"title":"Brief overview of the reproducible builds concept","text":"Introduction When working with the source code in a project that has multiple build steps (compiling, linking, patching, packaging) when a final \"product\" – a Debian package, an installable application, or an executable with shared libraries – is produced, there are many reasons why it can be useful to be able to get the same binary code (bit-by-bit) from the same source code. If you are able to build your project source code and then re-build it again later (without making any changes to the source code) and the produced artifacts are identical, it is said that your builds are reproducible/deterministic . How can one set up a reproducible build? According to the https://reproducible-builds.org definition: A build is reproducible if given the same source code, build environment and build instructions, any party can recreate bit-by-bit identical copies of all specified artifacts. For a simple project with a small number of movings parts, it may be relatively easy to achieve reproducible builds whereas for a corporate software development project this can be a challenge. There are multiple reasons why the binaries produced by a build operation may differ between builds run from the same source code. There are a few resources that will help you get started: An introduction to deterministic builds with C/C++ provides a gentle introduction to the concept and its importance and benefits Reproducible builds will help you learn more about software development practices around the reproducible builds. Elements of indeterminism Two most common issues are timestamps (when the source code is built) which may be saved into the produced binaries and path information (the location of the source code files on disk) which can also be included into the output binaries. However, many other things can have impact and make two binaries different (they may have the same size, but still be different when doing bit-by-bit comparison). Some of the things you will have control from the build system tools perspective such as compilers and linkers. For instance, you can control the order in which files are being processed as file systems generally do not make any promises that when you iterate the files in a given directory, they will be retrieved in the same order at all times. Other things may be defined in your custom post-processing logic – for instance, the order in which you set certain properties on a binary (such as RPATH patching) can also result in two different binaries. Sample project I have created a GitHub repository with the source code files that have been used in the Conan article An introduction to deterministic builds with C/C++ and it is available at reproducible-builds-example . This example project demonstrates the concept of reproducible builds with a few C++ source files and CMake build steps. A great tool that will help you compare the binaries in your effort to achieve reproducible builds is Diffoscope . It is extremely powerful and has functionality for generating HTML reports showing the difference between two objects you are comparing. This makes it so much easier to see why your binaries are different. Below is a screenshot of the HTML report that shows the difference between two executables. Happy diffing!","tags":"build-systems","url":"/intro-reproducible-builds.html","loc":"/intro-reproducible-builds.html"},{"title":"Using quicktype.io service to create Python interfaces from JSON","text":"Introduction For the last few years I had to write a few simple Python wrappers around a couple of external services. There are many advantages to having a nice Pythonic interface into libraries and tools that are written in other programming languages. It often makes it easier to automate, more pleasant to interact with, and faster to program in general. Bindings and wrappers Some wrappers simply expose the original interfaces without adding anything – these are plain bindings and this is often the case for C++ libraries that have Python bindings such Qt with PyQt . Python code you'd write using plain Python bindings may not feel very Pythonic (due to camelCase ) and because you often have to write programs using other, non-Pythonic, paradigms such as obj.setColor('Red') instead of obj.color = 'Red' . It is, in fact, not uncommon to write Python wrappers around Python bindings for C++ libraries simply because the Python bindings do not make Python developers who use them much more productive. Another group of Python wrapping effort exists around wrapping web services interaction to avoid dealing with cumbersome HTTP requests construction, response processing, and service communication. Likewise, wrapping a CLI tool in Python can be very useful if this is the only way to interact with the underlying software. Working with JSON No matter how you are getting back a JSON response – from a web service or from a CLI tool – you will need to process it to either present the result to the end user or to manage it in some other way. When dealing with JSON data, the built-in json module comes in handy and extracting the information you need out of a JSON object is trivial. You could also take advantage of higher level HTTP communication library such as requests . At the beginning, the code may look something like this: import requests data = requests . get ( 'https://s2k7tnzlhrpw.statuspage.io/api/v2/status.json' ) . json () status = data [ 'status' ][ 'description' ] updated_at = data [ 'page' ][ 'updated_at' ] print ( f \"Status: { status } \\n Updated at: { updated_at } \" ) Output: Status: All Systems Operational Updated at: 2020-08-12T08:08:25.828Z Interacting with the returned JSON objects using only the json module will suffice for smaller scripts and ad-hoc web service interrogation. If you'd like to build a Python wrapper around a large REST interface with many endpoints, however, it may be useful to think about having higher level abstractions for the data entities you deal with. The code snippet above has a number of issues: it relies on having the data elements present when using accessing JSON objects (you could work around it using the .get() method – data.get('status', {}).get('description', 'N/A') but it is still very fragile) as JSON objects keys are represented as strings, it's impossible to run any static type checker (and it has additional complications – refactoring becomes really hard) it makes it hard to reason about the data entities as their data type is not obvious (and you would have to provide a type hint for each JSON object such as status: Dict[str, str] = data['status'] which will become tedious very quickly) Representation of JSON as Python classes To make it easier to interact with JSON objects, they can be used to construct instances of Python classes which are much easier to work with: they provide nice abstraction, they are easy to write unit tests for, and the code that uses them can be inspected with a static analysis tool such as mypy . import requests from typing import Optional from datetime import datetime from dateutil import parser class Page : id : Optional [ str ] name : Optional [ str ] url : Optional [ str ] time_zone : Optional [ str ] updated_at : Optional [ datetime ] def __init__ ( self , id : Optional [ str ], name : Optional [ str ], url : Optional [ str ], time_zone : Optional [ str ], updated_at : Optional [ str ]): self . id = id self . name = name self . url = url self . time_zone = time_zone self . updated_at = parser . parse ( updated_at ) class Status : indicator : Optional [ str ] description : Optional [ str ] def __init__ ( self , indicator : Optional [ str ], description : Optional [ str ]): self . indicator = indicator self . description = description class System : page : Optional [ Page ] status : Optional [ Status ] def __init__ ( self , data ): self . page = Page ( ** data [ 'page' ]) self . status = Status ( ** data [ 'status' ]) data = requests . get ( 'https://s2k7tnzlhrpw.statuspage.io/api/v2/status.json' ) . json () system = System ( data ) status = system . status . description updated_at = system . page . updated_at . strftime ( ' %d /%m/%Y' ) print ( f \"Status: { status } \\n Updated at: { updated_at } \" ) Output: Status: All Systems Operational Updated at: 2020-08-12T08:08:25.828Z Having these classes will solve the issues that the original code snippet had. You can now extend the classes with more fields and add additional logic to any class – the Page class can have a local time zone property or the Status.description can be an instance of the StatusType(Enum) class, for instance. Autogeneration of Python classes from JSON It would be very useful if one could generate Python classes declarations from an API specification file. Swagger tools make it possible to generate an API specification which one could then convert into a collection of Python classes. This approach is very useful but the generated Python classes would be simply data classes without any logic – your fields with the date would be strings, not datetime objects. I think it works best for APIs that change often, during the development when you are iterating on the API design, or when having the raw data classes is sufficient. Another approach is to auto-generate a collection of Python classes from the API specification and extend their initialization logic and to add additional fields/methods as required. This approach has worked well for me and would be particularly useful for any internal tooling when you have control over the API changes. I found the QuickType.io – the service that can convert JSON into typesafe code in many languages including Python – to be really helpful. The classes declared in the snippet above have been generated by quicktype.io from JSON and then modified so that the root class System will have other class instances as its fields. That is, you just have to provide the root JSON object and the root class System will populate all its fields with respective classes as required. For this, a handy Python feature of unpacking keyword arguments with ** is used. This way, the quicktype.io service generates all the boilerplate Python code needed and then some additional modification can be done (e.g. to overload the __repr__ magic method to dump a JSON representation of the class instance). I think you will see the value of using a Python class to represent a JSON object very quickly and with the help of quicktype.io , autogeneration of Python data classes is incredibly easy. Happy automating!","tags":"python","url":"/quicktype-json-class-generation.html","loc":"/quicktype-json-class-generation.html"},{"title":"Building Python extension modules for C++ code with pybind11","text":"Introduction If you ever needed to provide interface to the C/C++ code from your Python modules, you may have used Python extension modules . They are typically created when there is an existing C++ project and it is required to make it accessible via Python bindings. Alternatively, when performance becomes critical, a certain part of the Python project can be written in C/C++ and made accessible to the rest of the Python codebase via some kind of interface. Quite a few large C++ libraries and frameworks have associated Python bindings – they can be used for prototyping or simply to speed up the development as writing a Python program is supposed to take less time than writing an equivalent C++ program. Exposing your library interface with another popular language, such as Python, will also make your project more accessible for programmers who are not very familiar with C++. Refer to excellent RealPython: Python Bindings: Calling C or C++ From Python article to learn more. Python bindings There are quite a few options on how you can make your C++ code accessible from Python. However, I have personally worked only with SWIG and pybind11 so far. For now, let's focus on pybind11 . It's extremely easy to set up on Linux or Windows and you should be able to create a compiled Python extension module ( .so for Linux and .pyd for Windows) very quickly. The pybind11 documentation does provide excellent reference information with a ton of examples. However, those examples often demonstrate features in isolation and I thought it would be useful to share an example of a more complete \"library\" where multiple examples are combined into something that looks like a MVP. Writing C++ code Here is the C++ file, Geometry.cpp , I've written to demonstrate the pybind11 features. It showcases constructing custom Point class instances, finding the distance between them in 2D and 3D space, and overloading C++ comparison operators among a few other things. #include <pybind11/pybind11.h> #include <pybind11/operators.h> #include <string> #include <sstream> #include <iomanip> #include <cmath> namespace py = pybind11 ; using namespace std ; class Point { public : Point ( const double & x , const double & y ) : x ( x ), y ( y ) { z = numeric_limits < double >:: quiet_NaN (); py :: print ( \"Constructing a point with z set to nan\" ); } Point ( const double & x , const double & y , const double & z ) : x ( x ), y ( y ), z ( z ) { py :: print ( \"Constructing a point with z set to a user given value\" ); } double x ; double y ; double z ; string shapeType = \"Point\" ; double distanceTo ( Point point , bool in3D ) { if ( in3D ) { if ( ! isnan ( z ) && ! isnan ( point . z )) { return sqrt ( pow (( point . x - x ), 2 ) + pow (( point . y - y ), 2 ) + pow (( point . z - z ), 2 )); } else { py :: print ( \"Cannot measure distance between points in XY and XYZ space\" ); return numeric_limits < double >:: quiet_NaN (); } } else { return sqrt ( pow (( point . x - x ), 2 ) + pow (( point . y - y ), 2 )); } } bool static areEqual ( Point left , Point right ) { if ( isnan ( left . z ) && isnan ( right . z )) { return left . y == right . y && left . x == right . x ; } else if ( isnan ( left . z ) &#94; isnan ( right . z )) { return false ; } else { return left . y == right . y && left . x == right . x && left . z == right . z ; } } friend bool operator == ( const Point & left , const Point & right ) { return areEqual ( left , right ); } friend bool operator != ( const Point & left , const Point & right ) { return ! areEqual ( left , right ); } bool is3D () const { return ! isnan ( z ); } }; PYBIND11_MODULE ( Geometry , m ) { m . doc () = \"C++ toy geometry library\" ; py :: class_ < Point > ( m , \"Point\" , \"Point shape class implementation\" ) . def ( py :: init < const double & , const double &> (), py :: arg ( \"x\" ), py :: arg ( \"y\" )) . def ( py :: init < const double & , const double & , const double &> (), py :: arg ( \"x\" ), py :: arg ( \"y\" ), py :: arg ( \"z\" )) . def_readonly ( \"x\" , & Point :: x ) . def_readonly ( \"y\" , & Point :: y ) . def_readonly ( \"z\" , & Point :: z ) . def_readonly ( \"shapeType\" , & Point :: shapeType ) . def ( \"distanceTo\" , & Point :: distanceTo , py :: arg ( \"point\" ), py :: arg ( \"in3D\" ) = false , \"Distance to another point\" ) . def ( \"is3D\" , & Point :: is3D , \"Whether a point has a valid z coordinate\" ) . def ( py :: self == py :: self ) . def ( py :: self != py :: self ) . def ( \"__repr__\" , []( const Point & point ) { stringstream xAsString , yAsString , zAsString ; xAsString << std :: setprecision ( 17 ) << point . x ; yAsString << std :: setprecision ( 17 ) << point . y ; zAsString << std :: setprecision ( 17 ) << point . z ; if ( point . z != 0 ) { return \"Point (\" + xAsString . str () + \", \" + yAsString . str () + \", \" + zAsString . str () + \")\" ; } else { return \"Point (\" + xAsString . str () + \", \" + yAsString . str () + \")\" ; } }); } Building a Python extension module Once you have the Geometry.cpp file on disk and pybind11 installed, you should be able to compile the C++ code and link it to the Python headers: $ c++ -O3 -Wall -shared -std = c++11 -fPIC ` python3 -m pybind11 --includes ` Geometry.cpp -o Geometry ` python3-config --extension-suffix ` If you are not very familiar with Bash, command in the backticks in the command above are evaluated by the shell before the main command. The python3 -m pybind11 --includes part is used to get the location of Python header files and the python3-config --extension-suffix part is used to get the suffix for the shared library name – for CPython 3.6 on a 64bit Ubuntu, the Geometry.cpython-36m-x86_64-linux-gnu.so file will be created. Now, once you have the shared library file, it can be imported and used pretty much as if it was a regular Python module. Using a Python extension module Let's see our library in action by running the file python3 use_geometry.py containing the code below: import math from Geometry import Point # runtime dispatch of init constructors print ( Point . __init__ . __doc__ ) # a method signature and its docstring print ( Point . distanceTo . __doc__ ) p1 = Point ( 10 , 20 ) p2 = Point ( 20 , 30 ) p3 = Point ( 20 , 30 ) p4 = Point ( 50 , 60 , 45.67 ) p5 = Point ( 50 , 60 , 45.67 ) assert p1 . distanceTo ( p2 ) == math . sqrt ( 200 ) # check operator overloading works assert p1 != p2 assert p2 == p3 assert not p2 != p3 assert p4 == p5 assert not p4 == p3 assert math . isnan ( p1 . z ) # check distance between 3D points p1 = Point ( 50 , 60 , 45 ) p2 = Point ( 50 , 60 , 75 ) print ( p1 . distanceTo ( p2 , in3D = True )) # check __repr__ print ( p1 ) print ( p2 ) The produced output: __init__(*args, **kwargs) Overloaded function. 1. __init__(self: Geometry.Point, x: float, y: float) -> None 2. __init__(self: Geometry.Point, x: float, y: float, z: float) -> None distanceTo(self: Geometry.Point, point: Geometry.Point, in3D: bool = False) -> float Distance to another point Constructing a point with z set to nan Constructing a point with z set to nan Constructing a point with z set to nan Constructing a point with z set to a user given value Constructing a point with z set to a user given value Constructing a point with z set to a user given value Constructing a point with z set to a user given value Distance between Point (50, 60, 45) and Point (50, 60, 75) is 30.0 Representation of the p1 is \"Point (50, 60, 45)\" Representation of the p2 is \"Point (50, 60, 75)\" This is of course a very trivial example of pybind11 usage, however there have been successful attempts to use pybind11 for binding existing large C++ libraries such as CGAL and Point Cloud Library . See an example of wrapping some of the CGAL functionality with pybind11 and Python bindings for the Point Cloud Library to learn more. Happy binding!","tags":"python","url":"/pybind11-python-bindings.html","loc":"/pybind11-python-bindings.html"},{"title":"What I think a great tech support should look like","text":"Being a software engineer implies that you may be interacting with customers particularly if you are working in a small company. If you are wearing multiple hats or if you are in professional services or tech support, you will likely be contacted by your customers about issues they may experience using your product – a web site, a desktop or a mobile app, or some hardware equipment. Whatever it is, I believe it's best to have a clear plan of actions outlined which you can follow when addressing a customer's issue. No matter how fine-grained your protocol of communication with the customer already is, you can always incorporate some of the ideas I have about how I'd like to work with a customer that needs help. I do understand that it may be difficult or unrealistic to follow the steps below exactly, but this is my vision of a great support. Customer gets in touch Customer reaches out to you because some functionality on the web site of a business system your team built doesn't work as they expect. Pace yourself Do not attempt to provide any solution or ask any questions just yet. If your guess will be accurate – you ask to enable JavaScript in a web browser or to log out and log back in – the customer is less likely to share any details with you because they can now continue working. You, on the other hand, are missing a chance to document the incident to see if it would be possible to prevent it from happening again and to share it with your team members. What was the version of the web browser where the JavaScript is not enabled by default? Did they really have to log out and log back in or a simple refresh of a web page would be enough (you may tweak server-side caching settings)? You may never figure this out and even though the problem is \"solved\" and the customer is happy, in the long run this was a lost. Show empathy No matter what channel of communication is being used – a phone call or an email – the first thing you do is to show some empathy: being an engineer, you know better than anyone how frustrating it can be when a computer doesn't do what you want it to do. Collect information You can start collecting the information you will need to troubleshoot now; don't ask for information you can collect yourself, but sometimes it may still be useful – even if you can SSH into a server to get the version of a software installed, it would still be very useful to ask the customer to tell you the version they see on the web page as an older version may indicate a web page caching issue. You would ideally have a pre-defined template document where you can fill all the information you may need so that you don't have think about it during a phone call. Share information Once you have gathered all the information, make sure to share it with the customer. If on the phone, read it back to them. If it's an email conversation, either share the complete document or provide access to the internal support system (if any) where they would be able to check that the information they've shared with you is accurate. This will help to avoid any misunderstandings and provide traceability as the customer will acknowledge that the information they have provided is correct. Doing troubleshooting Depending on how urgent the request is, you may or may not have time to do some manual production inspection before telling the customer how to fix their problem. If they are in a middle of a presentation to a board of directors, it may be best to tell them how to fix the issue immediately. If it is not time critical, you may want to ask for some time to log in into the production environment to record as much as information as you possibly can. For instance, they told you that they are still able to use the system despite not being logged in. Instead of telling them to log off and log in hoping it will fix the problem (did you know that hope is not a strategy ?), you may want to log in into the server to find out what's going on with your authentication service while the user hasn't left their web browser session (provided you don't do any verbose logging of this type of events already). This issue may indicate some serious problem that is worth investigating further because it may manifest itself again at some point. Update on the progress If the problem requires more time and you will likely need to spend hours if not days working on it, it's best to let the customer know about the progress. They would be able to find it very helpful to see that you are working on their issue and ideally how much time you've spent (a support ticket can be \"in progress\" for 5 days, but the engineer may have been working on it for just 1 hour). Provide a temporary solution if applicable If it's an option, make sure to provide customer with a workaround to let them continue to do their business. If they need to process some files, offer to do it manually for them if possible. If their business operation depends on a feature from their \"basic\" plan that doesn't work and you know that an alternative feature from the \"advanced\" plan would work, upgrade their account for some time while you are troubleshooting. Tell about proposed solution Once you have identified the issue and have been able to solve it, tell the user what was happening and what you have done to solve it. Adjust the language depending on how technical things get as required but don't be afraid to offer them a chance to learn; many customers would find it helpful to understand how the product they use operate under the hood. Ask them to verify that the solution you've implemented works for them (after you have done everything you possibly could to verify this yourself first). Document your findings After you found the resolution to a problem, make sure to document not only what has to be done to fix it, but also what have you attempted to get done which didn't seem to help. For instance, you thought that the problem may be due to a broken database table index and have decided to re-build it. That didn't help and then you think that perhaps recalculating the table statistics may help. You do that now and, yes, the problem is gone. However, documenting that for this problem recalculating the table statistics is necessary may be misleading as re-building the table index may also be required. When you or a colleague of yours will be reading the incident documentation, they will know what have you attempted before finding the solution. Wherever possible, any changes to any environment should be happening via code or a terminal to make it easy to record as making changes in the GUIs are generally known to be very hard to document. A problem that can be solved purely by customers themselves (invalidating the web browser cache or to change some setting within the user interface of the business system itself) is a great candidate to be added into the user documentation. Happy supporting!","tags":"tech-support","url":"/my-version-of-a-great-tech-support.html","loc":"/my-version-of-a-great-tech-support.html"}]};