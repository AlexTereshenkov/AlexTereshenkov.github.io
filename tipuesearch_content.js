var tipuesearch = {"pages":[{"title":"Compiling Python code: testing pyramid foundation","text":"Overview If you start working on a legacy project, you often want to start by understanding how \"runnable\" code is. If the project has tests, you are lucky and can start by reading the tests code and running them carefully (potentially in a sandbox environment if the tests may have side effects such as connecting to a production database and deleting tables). If the project doesn't have any tests (or there are ones for certain segments only), it may be helpful to see whether the code compiles at all. This is an approach one would normally take for a compiled language such as Java – if you got an open-source project, for instance, it's common to ask yourself – does this project build at all? Because Python is not a compiled language (in the same sense as Java or C++ are), your options to check for project sanity are quite limited. You may want to run some code quality and static program analysis tools, but they may not give you the most accurate picture. In contrast to static code analysis, you may also employ dynamic program analysis and there a few Python tools worth taking a look. A very basic approach to start with, however, is to see whether the project files are syntactically correct. Compiling Python code To verify that Python module contains syntactically valid code, you can use the compileall module which can be used both from a command line and in Python programs. The compileall tool will compile your source code into bytecode files ( .pyc files in the `__pycache__ directories) and if there are any syntax errors, the compilation will fail reporting the problem. Having these pieces of code in a Python module: def hello ( name : str ): print ( \"Hello \" + name ) print ( foobar ) hello ( \"user\" ) class User : def __init__ ( self ) -> None : self . uid = 0 def greet ( self ): print ( \"Hello!\" ) print ( self . uid ) print ( self . foobar ) user = User () user . greet () and running the compileall on the directory containing the file above: $ python3 -m compileall projectdir Listing 'projectdir' ... Compiling 'projectdir/main.py' ... You may be surprised, but the compilation succeeds. The file above is syntactically correct, even though you may have noticed that foobar variable/attribute is undefined; running the program will fail with the NameError error (in the function call) and with the AttributeError error (in the class instance). Due to the very dynamic nature of Python (global scope, monkey patching, adding/removing class instance variables), when Python code is compiled into the bytecode, the compiler can't be certain whether the foobar variable is accessible in the global scope (introduced by another Python module) or whether .foobar attribute will be added at the runtime. Compiling Java code Java, in contrast, doesn't have the concept of the global scope (in Python sense) and is more conservative in how much can happen at the runtime, so the Java compiler can be of more help: class User { private int uid = 0 ; public void greet () { System . out . println ( \"Hello!\" ); System . out . println ( this . uid ); System . out . println ( this . foobar ); } } Running the compiler: $ javac main.java main.java:8: error: cannot find symbol System.out.println ( this.foobar ) ; &#94; symbol: variable foobar 1 error Using compileall to confirm Python version compatibility If you start refactoring a legacy project, and you don't have a clear picture whether all Python modules are being exercised when tests are run (you can spot those by looking at a test coverage report after running the tests), you may want to start by fixing any syntax errors (e.g. if you are migrating to Python 3 from Python 2). Running compileall would also help to confirm that project is compatible with a certain Python version. For example, you have to guarantee compatibility with Python 3.6 and 3.7 and therefore cannot use any new syntax from later versions. Compiling your Python sources into bytecode using a Python interpreter of a certain version is a very cheap way to confirm that the code is compatible. For instance, you can compile all project code using a particular Python version to make sure it doesn't feature any new syntax you cannot yet support such as assignment expressions : items = [ 1 , 2 , 3 , 4 , 5 ] if ( n := len ( items )) > 3 : print ( f \"List is too long (there are { n } items, expected < 3)\" ) Happy compiling and testing!","tags":"python","url":"/python-compiling-code-base-testing-pyramid.html","loc":"/python-compiling-code-base-testing-pyramid.html"},{"title":"Python basics - Pandas","text":"This is one of the posts in a series of introductory Python articles. These posts explain programming principles in a simplified way and show very basic Python code that should help folks learning Python to get a better understanding of some concepts. In this post, I'll share some notes about working with a popular data science and data analysis library - pandas . I have simplified this dataset that is part of the Census from this this portal to illustrate how pandas work. Introduction pandas is one of the most popular data analysis and data munging library implemented as a Python package. pandas provides an easy-to-use interface for exploring all kinds of datasets. It has hundreds of useful functions and operations, so it is impossible to go through all of them. We will start exploring the basics of pandas first by loading a .csv file into a pandas data frame. import pandas as pd pd . set_option ( 'display.max_rows' , 7 ) df = pd . read_csv ( '~/data/counties.csv' ) df STATE COUNTY POP2010 EST2010 0 Alabama Autauga County 54571.0 54597 1 Alabama Baldwin County 182265.0 182265 2 Alabama Barbour County 27457.0 27455 ... ... ... ... ... 3139 Wyoming Uinta County 21118.0 21121 3140 Wyoming Washakie County 8533.0 8528 3141 Wyoming Weston County 7208.0 7208 3142 rows × 4 columns Now we would like to create a new .csv file with only counties in the Oregon state. df = pd . read_csv ( '~/data/counties.csv' ) oregon_counties = df [( df . STATE == 'Oregon' )] oregon_counties . to_csv ( '~/data/oregon.csv' , index = False ) pd . read_csv ( '~/data/oregon.csv' ) STATE COUNTY POP2010 EST2010 0 Oregon Baker County 16134.0 16131 1 Oregon Benton County 85579.0 85581 2 Oregon Clackamas County 375992.0 375996 ... ... ... ... ... 33 Oregon Washington County 529710.0 529862 34 Oregon Wheeler County 1441.0 1439 35 Oregon Yamhill County 99193.0 99216 36 rows × 4 columns This kind of operations and many others that you will learn now can be done using any RDBMS such SQL Server, MySQL, or PostgreSQL or an office program such as Excel. However, not all of your data will originate or be stored in a RDBMS. We could load .csv files into a RDBMS to do this kind of work, but if there is no need to manage the data over time in the database, we may use the wrong tool for the job. Basic operations As you see, it is very easy to load external data into something that is called a DataFrame . You can think of this as a relational table that consists of rows and columns. This is what you would expect to have after importing a .csv file into an Excel sheet. Let's explore some of the basic operations that are available for a DataFrame object: # getting only the first 5 rows, equivalent to df[0:5] df . head () STATE COUNTY POP2010 EST2010 0 Alabama Autauga County 54571.0 54597 1 Alabama Baldwin County 182265.0 182265 2 Alabama Barbour County 27457.0 27455 3 Alabama Bibb County 22915.0 22915 4 Alabama Blount County 57322.0 57322 # sort in-place to get most populated counties df . sort_values ( 'POP2010' , ascending = False , inplace = True ) df . head () STATE COUNTY POP2010 EST2010 204 California Los Angeles County 9818605.0 9819968 610 Illinois Cook County 5194675.0 5195026 2623 Texas Harris County 4092459.0 4093176 103 Arizona Maricopa County 3817117.0 3817365 222 California San Diego County 3095313.0 3095349 # filter by condition, show only counties in California state df [ df . STATE == 'California' ] STATE COUNTY POP2010 EST2010 204 California Los Angeles County 9818605.0 9819968 222 California San Diego County 3095313.0 3095349 215 California Orange County 3010232.0 3008989 ... ... ... ... ... 210 California Modoc County 9686.0 9682 231 California Sierra County 3240.0 3239 187 California Alpine County 1175.0 1175 58 rows × 4 columns # show rows with missing values for POP2010 column df [ df . POP2010 . isnull ()] STATE COUNTY POP2010 EST2010 9 Alabama Cherokee County NaN 25979 # show only counties within the two states and sort by state and then by population df [ df . STATE . isin ([ 'Delaware' , 'Texas' ])] . sort_values ( [ 'STATE' , 'POP2010' ], ascending = [ True , False ])[: 10 ] STATE COUNTY POP2010 EST2010 317 Delaware New Castle County 538479.0 538484 318 Delaware Sussex County 197145.0 197103 316 Delaware Kent County 162310.0 162350 ... ... ... ... ... 2749 Texas Travis County 1024266.0 1024444 2593 Texas El Paso County 800647.0 800633 2565 Texas Collin County 782341.0 781419 10 rows × 4 columns # getting total population for all counties in a state df . POP2010 [ df . STATE == 'Texas' ] . sum () 25145561.0 # get number of counties within each state df . STATE . value_counts () Texas 254 Georgia 159 Virginia 133 ... Hawaii 5 Delaware 3 District of Columbia 1 Name: STATE, Length: 51, dtype: int64 # sum population of counties within a state; # list sorted by population (omitting all other columns for brevity) res = df [[ 'STATE' , 'POP2010' ]] . groupby ( 'STATE' ) . sum () res . sort_values ( 'POP2010' , ascending = False ) POP2010 STATE California 37253956.0 Texas 25145561.0 New York 19378102.0 ... ... Vermont 625741.0 District of Columbia 601723.0 Wyoming 563626.0 51 rows × 1 columns pandas has a concept of boolean indexing which provides powerful techniques for selecting rows using functions and various criteria. In the example below, each item in the bool_index list variable is either True or False depending on whether an item in the column STATE when passed into the startswith() function returned True or False . This list is used then to select rows in df - those rows for which there is True in the bool_index will be returned. Here we return those counties with population larger than a certain value which are in a state with the name of the state starting with W . bool_index = [ x . startswith ( 'W' ) for x in df [ 'STATE' ]] df [( df [ 'POP2010' ] < 6000 ) & bool_index ] STATE COUNTY POP2010 EST2010 3072 Wisconsin Iron County 5916.0 5916 3044 West Virginia Wirt County 5717.0 5714 3127 Wyoming Hot Springs County 4812.0 4812 ... ... ... ... ... 2987 Washington Wahkiakum County 3978.0 3979 3132 Wyoming Niobrara County 2484.0 2484 2964 Washington Garfield County 2266.0 2266 9 rows × 4 columns As you have seen, with pandas it is very easy to get useful insights about the data and do some data filtering, sorting, and SQL like selections. What would require many lines of code in plain Python (using collections.Counter , creating intermediate data structures, and copies of data) could be done in just one line in pandas . Using this package can make you a more productive developer or an analyst. It can also simply be very handy to be able to clean the data as needed when using a graphical user interface isn't very convenient. Plotting pandas is able to load data from all kinds of sources such as csv , Excel , HTML , and SQL databases. However, it is also possible to construct a DataFrame from all kinds of Python data structures such as dictionaries, lists, and tuples as well as from numpy arrays. This means that if any existing program already creates such a data structure, it's very likely that you'll be able to construct a DataFrame from it. pandas also has built-in plotting functions which use matplotlib features - one of the popular Python plotting libraries. % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) df = pd . read_csv ( '~/data/states.csv' ) df . sort_values ( 'POP2010' , ascending = False , inplace = True ) fig = df [[ 'STATE' , 'POP2010' , 'EST2019' ]] . head ( 10 ) . plot ( kind = 'bar' , x = 'STATE' , alpha = 0.8 , legend = True , figsize = ( 15 , 7 ), title = \"Population change trend\" ) fig . legend ([ 'Population 2010 (census)' , 'Population 2019 (estimate)' ]) fig . set_xlabel ( '' ) fig . set_ylabel ( 'Population (millions)' ) Text(0, 0.5, 'Population (millions)') df [ 'POPDIFF' ] = ( df [ 'EST2019' ] - df [ 'POP2010' ]) / df [ 'EST2019' ] df . sort_values ( 'POPDIFF' , ascending = False , inplace = True ) fig = df [[ 'STATE' , 'POPDIFF' ]] . tail ( 15 ) . plot ( kind = 'bar' , x = 'STATE' , figsize = ( 15 , 7 ), legend = None , title = 'Population change estimate from 2010 to 2019' ) fig . set_xlabel ( '' ) fig . set_ylabel ( 'Population change (millions)' ) fig . axhline ( linewidth = 2 , color = 'g' ) As you can see it is relatively easy to plot images with pandas and matplotlib . There are so many other options and graph types to choose from; you may explore more of those on the matplotlib home page . Happy analyzing!","tags":"python-basics","url":"/python-basics-pandas.html","loc":"/python-basics-pandas.html"},{"title":"Python basics - Collections","text":"This is one of the posts in a series of introductory Python articles. These posts explain programming principles in a simplified way and show very basic Python code that should help folks learning Python to get a better understanding of some concepts. In this post, I'll share some notes about working with useful containers from the collections module and various data structures that beginners may find useful. collections.defaultdict When iterating over some text files or any other kind of data structures, you may want to construct a dictionary with the key referring to a single object (e.g., a string) and the values referring to a list of objects (e.g., strings). However, when you create a new dictionary, there are no keys present, so you first have to add an empty list as the value for each of the keys and then start appending strings to the respective lists. Let's see how it works on a simpler example first. We cannot append a city name to the dictionary's key value as there is no list created yet, so we have to initialize the value making it an empty list for all new keys we find. d = {} data = [( 'Zone' , 'West' ), ( 'Zone' , 'East' ), ( 'Cluster' , 'US' ), ( 'Cluster' , 'EU' )] for key , value in data : if key not in d : d [ key ] = [] d [ key ] . append ( value ) print ( d ) # {'Zone': ['West', 'East'], 'Cluster': ['US', 'EU']} Using the collections.defaultdict , it is possible to create a dictionary structure where every key will already have a pre-assigned value of a certain type, such as list or int . This results in much cleaner code. from collections import defaultdict dd = defaultdict ( list ) data = [( 'Zone' , 'West' ), ( 'Zone' , 'East' ), ( 'Cluster' , 'US' ), ( 'Cluster' , 'EU' )] for key , value in data : dd [ key ] . append ( value ) print ( dd ) # defaultdict(<class 'list'>, {'Zone': ['West', 'East'], 'Cluster': ['US', 'EU']}) This could also be done using the dictionary's setdefault() method. This means that a default value, which is an empty list in this case, will always be computed if the key is not present in the dictionary yet. sd = {} data = [( 'Zone' , 'West' ), ( 'Zone' , 'East' ), ( 'Cluster' , 'US' ), ( 'Cluster' , 'EU' )] for key , value in data : sd . setdefault ( key , []) . append ( value ) print ( sd ) # {'Zone': ['West', 'East'], 'Cluster': ['US', 'EU']} Another neat feature of a dictionary is that it is possible to get a default value if a key is not present. This is done by using the get() method for which you can supply a default value. In the code snippet below, as there is no Port key in the dictionary, instead of getting None , we would like to get some default value instead. d = { 'LogLevel' : 'Verbose' , 'Database' : 'Production' } print d . get ( 'LogLevel' ) print d . get ( 'Database' ) print d . get ( 'Machine' ) print d . get ( 'Port' , 1433 ) # Verbose # Production # None # 1433 collections.namedtuple Another useful class from the collections module is namedtuple . This one becomes very handy when working with tuples of many items particularly when you need to access the individual items within a tuple. Usually records returned from a database table are represented as lists of tuples. Named tuples let you access fields of the returned records by name rather than by index. When working with a tuple of potentially a hundred fields, it is easy to make mistakes in indexing and access a wrong field. Compare these two snippets and see how more readable the second one is. With the named tuple, you can refer to a field by its name instead of using the [index] syntax: # using tuple indexing for accessing database table fields envs = [ ( \"Development\" , \"west-us\" , 27134 ), ( \"Staging\" , \"east-us\" , 48628 ), ( \"Production\" , \"west-eu\" , 78951 ), ] for env in envs : print ( \"Environment {name} in {cluster} had {requests} requests\" . format ( name = env [ 0 ], cluster = env [ 1 ], requests = env [ 2 ])) # Environment Development in west-us had 27134 requests # Environment Staging in east-us had 48628 requests # Environment Production in west-eu had 78951 requests The same information represented with the help of named tuples: # using named tuples for accessing database table fields from collections import namedtuple envs = [ ( \"Development\" , \"west-us\" , 27134 ), ( \"Staging\" , \"east-us\" , 48628 ), ( \"Production\" , \"west-eu\" , 78951 ), ] Environment = namedtuple ( 'Environment' , [ 'Name' , 'Cluster' , 'Requests' ]) environments = [ Environment ( * env ) for env in envs ] print ( environments [ 0 ]) # named tuple object for env in environments : print ( \"Environment {name} in {cluster} had {requests} requests\" . format ( name = env . Name , cluster = env . Cluster , requests = env . Requests )) # Environment(Name='Development', Cluster='west-us', Requests=27134) # Environment Development in west-us had 27134 requests # Environment Staging in east-us had 48628 requests # Environment Production in west-eu had 78951 requests List and dictionary comprehensions One may need to construct a dictionary from some other data structure such as a JSON file or a collection of records such as a list of tuples. Regardless of the data structure that is being used, a dictionary comprehension provides an intuitive and easy to read interface for creating a dictionary in a very similar way to how lists are constructed with the help of list comprehensions. envs = [ ( \"Development\" , \"west-us\" , 27134 ), ( \"Staging\" , \"east-us\" , 48628 ), ( \"Production\" , \"west-eu\" , 78951 ), ] d = { env [ 0 ]: env [ 1 ] for env in envs } print ( d ) # {'Development': 'west-us', 'Staging': 'east-us', 'Production': 'west-eu'} A dictionary comprehension can be arbitrarily complex: envs = [ ( \"Development\" , \"west-us\" , 27134 ), ( \"Staging\" , \"east-us\" , 48628 ), ( \"Production\" , \"west-eu\" , 78951 ), ] d = { env [ 0 ]: env [ 1 ] . upper () for env in envs if 'us' in env [ 1 ]} print ( d ) # {'Development': 'WEST-US', 'Staging': 'EAST-US'} It can also be useful to combine dictionary comprehensions with list comprehensions when constructing a collection of dictionaries. envs = [ ( \"Development\" , \"west-us\" , 27134 ), ( \"Staging\" , \"east-us\" , 48628 ), ( \"Production\" , \"west-eu\" , 78951 ), ] d = [{ \"environment\" : { \"type\" : env [ 0 ], \"cluster\" : env [ 1 ], \"requests\" : env [ 2 ]}} for env in envs ] print ( d ) # [ # {'environment': {'type': 'Development', 'cluster': 'west-us', 'requests': 27134}}, # {'environment': {'type': 'Staging', 'cluster': 'east-us', 'requests': 48628}}, # {'environment': {'type': 'Production', 'cluster': 'west-eu', 'requests': 78951}}, # ] In order to create a list, we've used the list comprehension which is a very powerful technique for constructing structures based on some other data structure in a single line. Set comprehensions and set theory operations You can think of sets as of dictionaries of keys with no values. A set can contain only unique values which makes it suitable for all kinds of tasks, for example, deleting duplicates in a list. Sets are also very handy for any kind of data comparison operations such as comparing values of a collection when order and duplication of values don't matter. Sets are created using the set(iterable) expression. input_values = [ 1 , 2 , 3 , 3 , 4 , 4 , 4 , 5 ] print ( set ( input_values )) # {1, 2, 3, 4, 5} A set comprehension created by using the {} curly brackets, or braces, could be used to get only unique values out of some data collection without constructing a list first: envs = [ ( \"Development\" , \"west-us\" , 27134 ), ( \"Staging1\" , \"east-us\" , 48628 ), ( \"Staging2\" , \"east-us\" , 31456 ), ( \"Production\" , \"west-eu\" , 78951 ), ] clusters = { env [ 1 ] for env in envs } print ( clusters ) # {'west-us', 'west-eu', 'east-us'} Set theory is a branch of mathematics that provides logic for working with sets. It defines certain rules that make it possible to perform all kinds of logical operations between two sets: intersection: find items that are common to both sets symmetric difference: find items that are either in the first or in the second set, but not in both sets union: get all items in both sets and add them into a single set with no duplicates difference: find items that are in the first set, but not in the second set # intersection list1 = [ 'Development' , 'Staging' , 'Production' ] list2 = [ 'Development' , 'Staging' , 'Blue' , 'Green' ] print ( set ( list1 ) & set ( list2 )) # {'Development', 'Staging'} # symmetric difference list1 = [ 'Development' , 'Staging' , 'Production' ] list2 = [ 'Development' , 'Staging' , 'Blue' , 'Green' ] print set ( list1 ) &#94; set ( list2 ) # {'Production', 'Green', 'Blue'} # union list1 = [ 'Development' , 'Staging' , 'Production' ] list2 = [ 'Development' , 'Staging' , 'Blue' , 'Green' ] print set ( list1 ) | set ( list2 ) # {'Production', 'Development', 'Blue', 'Green', 'Staging'} # difference list1 = [ 'Development' , 'Staging' , 'Blue' , 'Green' ] list2 = [ 'Development' , 'Staging' ] print ( set ( list1 ) - set ( list2 )) # {'Green', 'Blue'} However, many other operations such as searching whether a list is a part of another list (that is, a list is a sublist of another list) can be easily done with a plain list comprehension: inventory = [ 'Development' , 'Staging' , 'Blue' , 'Green' ] demand = [ 'Development' , 'Staging' , 'Production' ] # finding common items print ([ item for item in inventory if item in demand ]) # ['Development', 'Staging'] # finding items unique to inventory print ([ item for item in inventory if item not in demand ]) # ['Blue', 'Green'] Enumerated sequences At some point of time, you may work with a collection of items and you may need to pair each of the item with its index. The index may need to start at some arbitrary number and increase as you work your way up. A simple way to do that would be to use a counter variable. This may be required to have when inserting rows into a database table or for any other operation that would require having a unique identifier for each of the items. states = [ 'UNKNOWN' , 'ACTIVE' , 'DEPRECATED' ] count = 10 states_indexed = [] for state in states : states_indexed . append (( count , state )) count += 1 print ( states_indexed ) # [(10, 'UNKNOWN'), (11, 'ACTIVE'), (12, 'DEPRECATED')] A more elegant solution to this would be to use a built-in function enumerate() that does exactly that. states = [ 'UNKNOWN' , 'ACTIVE' , 'DEPRECATED' ] print ([( index , state ) for index , state in enumerate ( states , start = 10 )]) # [(10, 'UNKNOWN'), (11, 'ACTIVE'), (12, 'DEPRECATED')] Happy structuring!","tags":"python-basics","url":"/python-basics-collections.html","loc":"/python-basics-collections.html"},{"title":"Python basics - Classes","text":"This is one of the posts in a series of introductory Python articles. These posts explain programming principles in a simplified way and show very basic Python code that should help folks learning Python to get a better understanding of some concepts. In this post, I'll share some notes about writing and working with classes in Python that beginners may find useful. Using an existing class Python class is just a template of an object that we are using to model a certain entity or real world object, its properties and its behavior. For instance, a Python class can represent a file on a computer file system; such a class will have certain properties (or attributes ) and behaviors (or methods ). This way, a file class may have properties such as name, owner, permissions, and edit time. Its methods would be actions this object could do: for instance, we could ask file class to be renamed or moved and it also could tell us who its parent is. pathlib.Path is a class that you may have already used probably without reflecting too much about its properties. We will start exploring the class concept with a simple Path class. from pathlib import Path path = Path ( '/home/username/data/assets.csv' ) We can create a new instance of the class Path by assigning a variable to be equal to a class instance with certain default values supplied. A class constructor (the initialization method) has a number of default values it can take when you create an instance of this class. This far, we have created a path object which is a Path class instance that has many properties such as name and suffix . Because this instance is also a Python object, we can access its properties using the dot notation. print ( panh . name , path . suffix ) Let's create an instance of another Path class. Note that it is possible to use object's methods on multiple class instances. For instance, it is possible to construct a Path object from two existing Path objects: from pathlib import Path user_dir = Path ( '/home/user' ) assets_file = Path ( 'data/assets.csv' ) path = Path ( user_dir , assets_file ) print ( path ) # /home/user/data/assets.csv The pathlib.Path object also has methods . In other words, we can perform a certain operation on this object (you can think of methods as functions that are run on this specific object) and get some result back, just like we do when we run a function. # read file path . read_text () # remove file path . unlink () path = Path ( '/home/user/data/assets.csv' ) path . relative_to ( '/home/user' ) # PosixPath('data/assets.csv') We have used the relative_to method of the pathlib.Path object and this method returned another object, PosixPath that has again own properties and methods. Creating a new class This far, we have only used existing classes that are present in pathlib module, but in fact you have already used many Python classes without ever thinking of that. For instance, variables you create in Python, such as lists and strings, are instances of the classes list and str , respectively. To create a class in Python, you use a special keyword class . Let's model an entity that is not present in pathlib to learn about classes and how they work. We will create a class that will represent an existing directory on the file system. Class definitions we write (just like we do with the function definitions using the def statements) must be executed before they can be used further in the code. class Directory : \"\"\"A file system directory.\"\"\" def __init__ ( self , path ): self . path = path directory = Directory ( '/home/username/data' ) print ( directory ) # <__main__.Directory object at 0x00000284D5E6B7F0> print ( directory . path ) # /home/username/data The only thing you need to specify is the special function __init__ (constructor) that will run every time you will create an instance of this class. Because we will work with an existing directory, we have to supply a path to that directory. Passing the file system path as the input argument to the __init__ function will let user specify the path to the directory when creating a class instance. The directory variable will represent the Directory class object. A special word self that we used as the first argument is just a convention to refer to the object itself and you shouldn't worry about it for now. What is good to know though is that you can add your own properties to the class that will provide some useful information to the user. Let's add a property that will tell us the name of that directory: class Directory : \"\"\"A file system directory.\"\"\" def __init__ ( self , path ): self . path = path self . name = Path ( path ) . name directory = Directory ( '/home/username/data' ) print ( directory . name ) # data However, when we printed the directory variable out earlier, it didn't give us any useful information about the directory, only it's internal object representation which was rather cryptic. Thankfully, in Python you can override how the objects will be represented when they are printed out as well as when you access the object inside the interactive console (REPL). class Directory : \"\"\"A file system directory.\"\"\" def __init__ ( self , path ): self . path = path self . name = Path ( path ) . name def __repr__ ( self ): return f 'Directory(\" { self . path } \")' def __str__ ( self ): return str ( self . path ) directory = Directory ( '/home/username/data' ) print ( directory ) # /home/username/data # inside REPL directory # Directory(\"/home/username/data\") As you will see, it is possible not only to get some basic information about the object, but actually call other functions and do a lot more. An object property can be a result of some calculation or data look-up. Let's add another useful property that will report names of the files inside the directory: class Directory : \"\"\"A file system directory.\"\"\" def __init__ ( self , path ): self . path = path self . name = Path ( path ) . name self . files = self . get_files () def get_files ( self ): return [ item . name for item in Path ( self . path ) . glob ( \"*\" ) if item . is_file ()] def __repr__ ( self ): return f 'Directory(\" { self . path } \")' def __str__ ( self ): return str ( self . path ) directory = Directory ( '/home/username/data' ) print ( directory . files ) Class instance methods As you see, it can be very convenient for a developer to have a class defined in another module and then just import the module, create a class instance and start using this object. You may wonder why couldn't we just write a function with the help of Path ? We definitely could, but the true power of classes lies in how class instance objects can interact with each other. What can be implemented with the help of multiple functions can be often done with a more concise and elegant code of a class definition. To see that in action, let's define a new class - CsvFile that will represent a .csv file on disk. class CsvFile : \"\"\"A csv file.\"\"\" def __init__ ( self , path ): self . path = path This is no different from what we have already did with the Directory class. Let's add another property that will represent the data schema, that is, all the .csv file columns. The columns attribute will return the fields in the order they are stored in the .csv file (assuming the .csv file is comma-separated). class CsvFile : \"\"\"A csv file.\"\"\" def __init__ ( self , path ): self . path = path self . columns = self . get_columns () def get_columns ( self ): with open ( self . path ) as fh : return next ( fh ) . strip () . split ( ',' ) csv_file = CsvFile ( \"/home/username/data/assets.csv\" ) print ( csv_file . columns ) Now, what if we would like to check if two .csv files have the same schema? assets2019 = CsvFile ( \"/home/username/data/assets2019.csv\" ) assets2020 = CsvFile ( \"/home/username/data/assets2020.csv\" ) print ( assets2019 . columns == assets2020 . columns ) But what if we want to let users of our CsvFile class to compare the schema of two .csv files without taking into account the order of columns? Thinking of a database table, the concept of a column order doesn't make sense. Let's create a method that would compare the columns of two .csv files: class CsvFile : \"\"\"A csv file.\"\"\" def __init__ ( self , path ): self . path = path self . columns = self . get_columns () def get_columns ( self ): with open ( self . path ) as fh : return next ( fh ) . strip () . split ( ',' ) def schema_match ( self , other ): \"\"\"Compare the columns of two csv files.\"\"\" return set ( self . columns ) == set ( other . columns ) assets2019 = CsvFile ( \"/home/username/data/assets2019.csv\" ) assets2020 = CsvFile ( \"/home/username/data/assets2020.csv\" ) print ( assets2019 . schema_match ( assets2020 )) print ( assets2019 . columns == assets2020 . columns ) The method schema_match() we wrote above takes as arguments self (the class instance itself) and other (another class instance object). It accesses the columns property for each of the objects and then compares the sets to make a decision whether the schema of the files matches. If two .csv files from the example above would have the same columns stored in different orders, the assets2019.columns == assets2020.columns would return False , but since their schema is identical, assets2019.schema_match(assets2020) would return True . We could also extend our method to provide more fine-grained control over the comparison protocol. Is comparison case sensitive? Are duplicate fields ignored? The method could even take an comparison configuration object where we could have defined all the comparison settings. Advanced class behavior When modeling a class behavior, it may be helpful to provide the logic of how the object should behave in various situations. For instance, it is possible to compare two Path objects: Path ( '/data/path' ) == Path ( '/data/path' ) # True Path ( '/data/path' ) == Path ( '/data/file' ) # False For the .csv files, at least in our business domain specification, we could say that two files are identical if they have the same name and the same schema. To do this, we have to tell Python to use a special method when deciding whether two objects are the same with the __eq__ method: from pathlib import Path class CsvFile : \"\"\"A csv file.\"\"\" def __init__ ( self , path ): self . path = path self . name = Path ( path ) . name self . columns = self . get_columns () def get_columns ( self ): with open ( self . path ) as fh : return next ( fh ) . strip () . split ( ',' ) def schema_match ( self , other ): \"\"\"Compare the columns of two csv files.\"\"\" return set ( self . columns ) == set ( other . columns ) def __eq__ ( self , other ): return self . name == other . name and self . schema_match ( other ) print ( CsvFile ( \"/home/username/data/assets2019.csv\" ) == CsvFile ( \"/home/username/data/assets2020.csv\" )) It can also be useful to use an object in a for loop. For instance, we can let users of the Directory class iterate through the class instance and access every file stored within it. In order to do this, one has to use a special __iter__ method. from pathlib import Path class Directory : \"\"\"A file system directory.\"\"\" def __init__ ( self , path ): self . path = path self . name = Path ( path ) . name self . files = self . get_files () def get_files ( self ): return [ item . name for item in Path ( self . path ) . glob ( \"*\" ) if item . is_file ()] def __iter__ ( self ): for f in self . files : yield f directory = Directory ( \"/home/username/data\" ) for filename in directory : print ( filename ) # would return files one by one scanner = iter ( directory ) next ( scanner ) All you need to define essentially is what kind of iteration you want to provide your users with. The __iter__() , called generator function , works so that each time you use a yield statement, it will give you back the next item and then move to the next item in the list ready to be served. It may be helpful to learn a bit more about the generator expression as this technique may be necessary to use when dealing with large sequences or when the memory resources available are scarce. Keep in mind that the scanner generator from the example above is different from the directory (being iterable) because you can only iterate over the scanner once. This is because generators do not store all the iterable values in the computer memory, but generate them on-demand. You could construct a list of files while iterating the generator either in a standard for loop or with the help of list comprehension: scanner = iter ( directory ) files = [ item for item in scanner ] Keep in mind that when using the list comprehension, the entire list of items is created in memory. In contrast, while accessing the items in .files one by one, the generator accesses the items on-demand. This makes it possible to work with extremely large sequences without running out of memory. Happy classing!","tags":"python-basics","url":"/python-basics-classes.html","loc":"/python-basics-classes.html"},{"title":"Python basics - Functions","text":"This is one of the posts in a series of introductory Python articles. These posts explain programming principles in a simplified way and show very basic Python code that should help folks learning Python to get a better understanding of some concepts. In this post, I'll share some notes about writing and working with functions in Python that beginners may find useful. Defining functions Functions are used to include a piece of code that may be called multiple times from various parts of your program or code base. If you have multiple Python modules and each of them needs to execute the same piece of code, it makes sense to move this common code snippet into a separate module where you would make this code available as a function. There are multiple advantages of using functions (instead of having duplicate code present across multiple modules). Most importantly, if you define functions, when the code needs to be changed, it is changed only in one place and there is less code to maintain and read. It also becomes easier to debug and to refactor the code. Let's create a new function that will sum values within a collection. def calculate_sum ( collection ): \"\"\"Return a sum of numbers within a collection.\"\"\" result = 0 for i in collection : result = result + i return result calculate_sum ([ 1 , 2 , 3 , 4 , 5 ]) # 15 Python functions are defined using the def keyword; a function can also have some input arguments specified. The arguments can have arbitrary names, but it's helpful to name arguments based on what kind of values they will refer to. The string in the triple quotes Return a sum of numbers within a collection is called a docstring as this one provides documentation over what this function does. In order for this docstring to be accessible by Python internally and within your favorite IDE, it has to be on the first line after the def line. Thereafter, the code that should be executed is added. The input arguments can be used within the function body as one usually wants to do some work with the values that have been supplied. However, it's not uncommon to use functions that do the same thing over and over again without really asking for any input data. Maybe you want to delete all files within a known location before proceeding any further and there is no need to specify the folder name when calling the function. The last statement return , is always the last row of function code that will be executed. One usually wants to collect the results of the function execution, and this is exactly what the return statement does. In other words, in the code below, the variable result will refer to the value that the function returns, in this case, the sum of the input numbers. result = calculate_sum ([ 1 , 5 , 7 ]) Function doesn't have to return anything, such as with the case of cleaning up the folder, but it may be helpful to provide the caller with some feedback on the execution status (perhaps, a list of files that have been deleted). Return values for functions Keep in mind that function can return not only scalar values such as numbers or strings, but essentially any kind of Python object including None , False / True , a class instance, a dictionary and so forth. What is even more interesting, is that even though a function can indeed return just a single object, this object can be a container of multiple objects. Let's build a function that will return a tuple of multiple pieces of statistics: from collections import Counter def get_collection_stats ( collection ): \"\"\"Return a tuple of average, mode, max, and min values\"\"\" avg_val = int ( sum ( collection ) / len ( collection )) max_val = max ( collection ) min_val = min ( collection ) mode_val = Counter ( collection ) . most_common ( 1 )[ 0 ][ 0 ] return ( avg_val , max_val , min_val , mode_val ) ( avg_value , max_value , min_value , mode_value ) = get_collection_stats ([ 3 , 4 , 5 , 4 , 6 , 24 , 32 , 4 , 12 ]) print ( \"avg: \" , avg_value ) print ( \"max: \" , max_value ) print ( \"min: \" , min_value ) print ( \"mode: \" , mode_value ) # ('avg:', 10) # ('max:', 32) # ('min:', 3) # ('mode:', 4) This function could have also returned a dictionary with keys being the statistical values of interest and one would access them with result[\"max_value\"] and result[\"min_value\"] . Making functions flexible When creating new functions, it is often a good idea to make them environment agnostic, that is, independent of the current environment you are calling this function from. Let's build a function that will return some data stored in a file somewhere on your machine: def read_file (): \"\"\"Get contents of a file.\"\"\" with open ( '/home/username/project/data/assets.csv' ) as f : lines = f . readlines () return lines You may think that you will use this code just once and it will always run on your machine. However, as it turns out for many developers, the same code is later re-run on other machines and in other circumstances which makes it necessary to modify the code, such as update the machine or user name and adjust the file path. On one hand, it's impossible to foresee all things that could potentially may differ, and, of course, YAGNI . On the other hand, it is a good idea to try to avoid hard-coding anything that could be either obtained from user as an input argument or even better be retrieved on-the-fly by using Python built-in or external modules. There are dozens of other hardware, operating system, and Python related settings and parameters that can be obtained with the help of Python: import socket # Getting machine name print ( socket . gethostname ()) import sys # Getting Windows version print ( sys . getwindowsversion ()) # Getting Python version that is currently used and whether it is 32/64bit print ( sys . version ) import os # Getting current logged in user name and home path; # os.environ gives access to the system environment variables print ( os . environ [ 'HOME' ]) print ( os . environ [ 'PATH' ]) You could have retrieved the machine name from user, making it an input argument. However, since you can always obtain this at the execution time, there is really no need to do that unless the function needs to read the file on a different computer. Working with function arguments Input arguments of a function may have a default value. This makes it possible for a user calling a function to define only those arguments that don't have any default value. For these arguments, users will need to supply values explicitly: def clean_collection ( input_collection , max_value = 50 , keep_strings = False ): \"\"\"Return a collection of values that are less than max_value and, optionally, without strings.\"\"\" output_list = [] for item in input_collection : if isinstance ( item , str ) and keep_strings : output_list . append ( item ) elif isinstance ( item , int ): if not max_value or item <= max_value : output_list . append ( item ) return output_list clean_collection ([ 1 , 52 , 'text2' , 73 , 10 , 9 , 'text1' , 4 , 7 ], 70 , True ) # [1, 52, 'text2', 10, 9, 'text1', 4, 7] The keep_strings boolean argument has a pre-defined default value which will force function to exclude the strings in the input list. If an item in the input list will be larger than max_value argument specified, it will be excluded. As your functions are small and don't have many arguments, specifying them in the same sequence as they were declared is relatively easy. However, some functions may take 10, 20, or more input arguments, such as pandas.read_csv function. This makes it difficult to understand what argument has been provided when calling the function. Therefore, it might be a good idea to supply the argument value together with its name when calling the function. This is particularly true when calling functions with a long list of arguments. clean_list ( input_list = [ 1 , 52 , 73 , 10 , 9 , 'text1' , 4 , 7 ], max_value = 70 , keep_strings = True ) Now, since the function has a default value for the max_value argument, you can omit this one when calling the function. However, it is not allowed to provide the non-keyword argument after the keyword argument: # correct syntax clean_list ( input_list = [ 1 , 52 , 73 , 10 , 9 , 'text1' , 4 , 7 ], keep_strings = True ) # incorrect syntax clean_list ( input_list = [ 1 , 52 , 73 , 10 , 9 , 'text1' , 4 , 7 ], 50 ) Making the second function call results in a syntax error because Python interpreter is confused: for what argument have you supplied the 50 in the last call? To avoid this, always supply both value and the argument name - this is known as passing arguments by name. It is also worth mentioning that you can supply the arguments in an arbitrary order: clean_list ( keep_strings = False , input_list = [ 1 , 52 , 73 , 10 , 9 , 'text1' , 4 , 7 ], max_value = 40 ) clean_list ( keep_strings = True , input_list = [ 1 , 52 , 73 , 10 , 9 , 'text1' , 4 , 7 ]) Function naming You can think of a function as a means of doing something. Thus, verbs are usually used when naming them. get_current_date is a good name for a function returning a current date; current_date is a poor name as it makes readers think that it is a variable referring to the current date. Even though there are no strict rules on how functions should be named, it might be a good idea to follow the common conventions for using certain verbs in certain contexts ( get , set , save , read , write , dump , and so forth). Importing functions When you saved your function in another module (Python file on disk), you can import this function by using the same import statement you use for importing standard library modules (which are also Python files stored in the Python installation directory). There are some best practices that are worth following. It is helpful to organize the imports in a particular order and import from modules only those functions and objects you are planning to use. So, instead of import pandas you could do from pandas import DataFrame . Your code will run quicker as there are fewer objects to import and it also gets easier to see what kind of objects you intend to use in your code. Another thing is that you won't need to type pandas.DataFrame as using the DataFrame will suffice (because we have imported the object from pandas , so it knows where it belongs). You could have also done from pandas import DataFrame as df using the alias for the imported module and then use df object in your code instead of typing the whole name. This is something you will often see with the numpy sample codes: import numpy as np . A potentially dangerous from numpy import * will result in importing all of the objects found in the numpy package. Not only is this slow as lots of objects have to be loaded, but can also lead to the name clashing when you use the same name for your variables that have been used for the variables defined in the package you import. If you need to import multiple modules from the package, it is possible to do multiple imports in one line: from numpy import ndarray as nd , recarray as rec Now you don't need to write numpy.ndarray as nd will suffice. However, make sure to stick to conventions; if numpy is often imported as np in the community of numpy developers, it's best to stick to this standard so that your code is more readable by others. Importing functions from other path When you need to import a Python module stored somewhere else (not inside your project directory), there are multiple ways of doing this, but arguably the most straightforward one is to customize the sys.path property which contains all the folders where Python will look for the module names you supply. Imagine there is a Python module /home/username/projectA/utils.py . You are working in another directory, such as /home/username/projectB . If your Python module you are working with would be stored in the same directory as utils.py , you could just write import utils . However, as we are not in the same folder, we have to append /home/username/projectA to the path so Python could find the utils module. There are often better ways to work with multiple directories that doesn't involve modifying the system path directly. Your projects could be installed in editable mode as packages into your development environment so that they'd become available on your system path. Many IDEs such as PyCharm and VSCode provide way to add project directories as sources directories (via settings) to let Python interpreter find the modules. Behind the scene, however, they are still extending the system path for Python, so it's very useful to understand how this works. Anonymous (lambda) functions In Python, there are two ways of defining a function: using the def statement and lambda keyword. We have already created new functions using the def statement, so let's create a new function using lambda that will take an input list of integers and return True if an item in the list is even. is_even = lambda x : x % 2 == 0 for i in [ 1 , 2 , 3 , 4 ]: print ( is_even ( i )) # False # True # False # True The code above could be rewritten without using the lambda : def is_even ( x ): return x % 2 == 0 for i in [ 1 , 2 , 3 , 4 ]: print ( is_even ( i )) # False # True # False # True lambda has a slightly different syntax than a function and they have no return statement. But they could be handy for creating use once, throw away functions (that are unnamed). You might want use them when you need to do something with a data structure and you don't want to create a function for that: print ( list ( filter ( lambda x : x % 2 == 0 , [ 1 , 2 , 3 , 4 ]))) # [2, 4] # without lambda data = [ 1 , 2 , 3 , 4 ] print ([ i for i in data if i % 2 == 0 ]) # [2, 4] Lambda functions are also very helpful when providing the criteria for sorting a collection: data = [( 10 , \"b\" ), ( 20 , \"c\" ), ( 30 , \"a\" )] # sorting a collection of tuples based on the second elemen in each tuple sorted ( data , key = lambda item : item [ 1 ]) # [(30, 'a'), (10, 'b'), (20, 'c')] You may see lambda used in other developers' code so it's helpful to understand how they work. Writing convenience functions As your Python scripts get larger, you may find that you have some tiny snippets of Python code that are being used in multiple places. At this point of time, you may consider wrapping those snippets into functions which you could call. As those functions don't do anything except hiding verbose calls to existing functions and methods (potentially with different arguments), they are sometimes called convenience functions . For instance, you often need to find out the parent's directory name of a file. This can be done with the Path(\"filepath\").parent.name from the pathlib module. However, as this line is fairly long, your code may get cluttered if it's used everywhere. You could wrap this single line into a tiny convenience function: from pathlib import Path def parent ( filepath ): \"\"\"Get name of the parent directory of a filepath.\"\"\" return Path ( filepath ) . parent . name Or it could be that a 3rd party package you use to work with the file system breaks when attempting to delete a file that doesn't exist. To avoid cluttering your programs with repetitive checks for the file existence, you may like to wrap these calls into a tiny function to make your code less verbose: import filesystem # some 3rd party package from pathlib import Path def delete ( filepath ): \"\"\"Delete a file ignoring non-existing files.\"\"\" if Path ( filepath ) . exists (): filesystem . deep_delete ( filepath ) return True return False Be cautious creating such convenience, or helper , functions though, as it may be not worth it. Having multiple helper functions used all around your project may make it difficult to read for other developers who are not familiar with them (unless your team have agreed on certain conventions). If you do use them, make sure to document their use properly. Also, be careful choosing a valid and meaningful name to avoid name collision with functions and classes from other Python modules. Happy functioning!","tags":"python-basics","url":"/python-basics-functions.html","loc":"/python-basics-functions.html"},{"title":"Thinking about programming languages with dynamic and static type system","text":"After writing programs for a while in several programming languages, both professionally and for learning, I started thinking about benefits and drawbacks of programming languages with dynamic and static type systems. As with many other aspects of the software engineering industry, there are developers advocating for using interpreted, dynamically typed languages such as Python 1 . Likewise, there are ones who wouldn't consider using such a language for anything other than a minor script or a utility program and rather stick to compiled, statically typed languages such as C++ and Java 2 . In this post, I'd like to outline my considerations and share some thoughts and personal experiences. Given that I am mostly familiar with Python and Java, I'll use these two languages when reasoning about the use cases. Interpreted vs compiled languages Speed of development When there isn't a lot of time available for development or when the business requirements are not clear (so it's important to be able to make future changes easily), a team may decide to use an interpreted language. It becomes possible to complete writing a program significantly faster, however, the performance of an interpreted program will likely be worse. This is not necessarily a negative thing if the program run time is acceptable given the performance constraints, if any. Rapid prototyping Being able to complete an initial prototype fast makes an option to use a dynamic language very attractive. There may be a need to redo the implementation completely – plan to throw one away; you will anyhow 3 . It is suggested to prototype in an interpreted language before coding in a compiled language such as C 4 because general-purpose scripting languages make it very easy to construct framework of a program relying on external tools only when there is a special-purpose task 5 . There has been a claim that code can be written 5 to 10 times faster in a scripting language, but would run 10 to 20 times faster in a traditional compiled systems language 6 . An approach to implement a performance critical parts in a compiled language leaving the rest of the code in a dynamic language is so ubiquitous that perhaps the argument that dynamic languages shouldn't be used because they are \"slow\" is not relevant any longer. Suitability If most developers would probably agree on the prototyping story of scripting languages, what about writing a sophisticated piece of software with a dynamic language? Some of these languages were created for purposes other than building complex production software such as to teach programming, write short text-processing utilities, or be a glue language to integrate existing programs within a larger system. Some of the dynamic languages, however, were intended by their designers for general-purpose use, to support \"programming in the large\" 5 . However, as the size of the software increases, so often does the complexity of source code management. What often is taken in a statically typed, compiled language for granted (type checking, project-wide refactoring tools, dependency inference) becomes harder and harder when using a dynamically typed, interpreted language. Type safety Type annotations have been introduced to Python fairly recently and there are many code bases which are still not typed or typed only partially. Lots of software written in Python was written by professionals in adjacent domains such as engineering, finance, and data science. Those open-source projects often lack a vigorous build pipeline that would be considered a norm in a statically typed language commercial setting (code coverage, linting, static analysis). Therefore, I think in the eyes of Java developers, many large Python projects may look less mature and less robust for this very reason. When experimenting using a Python framework and getting a runtime type error, a programmer writing in a statically typed, compiled language may thus find a dynamic language not suitable for large scale development. Succeeding developing in dynamically typed language However, I believe it is possible to write great software with a dynamically typed, interpreted language. To be able to do that, a few things are required. Expectations Set the same expectations for programs written in a dynamically typed language that are set for programs written in a statically typed language. The programs written in Python can be made much easier to maintain and reason about if they follow the software engineering best practices which can be drawn from various sources (e.g., source code complexity , readability , and type safety ) Design principles Apply the design principles and implementation restrictions from statically typed languages in your dynamically typed language. For example, Java won't let you define multiple classes in the same file, so make sure in your Python project, you put only closely related units of work into a single file. MISRA C guidelines prohibit using recursion, so make sure you don't use recursion in your Python programs. Strongly and statically typed languages force you to specify the method return type, so you may want to make sure all your Python functions have a return type . Aggressive build pipeline In line with the DevOps practices, when building a project in CI, the build pipeline should be looking for reasons to reject a build. If any of the steps - formatting, linting, spell checking, testing, type checking – fails, the code coverage percentage goes down, or in case of any other declared event, then the build fails and the code can't be merged. This principle of \"Reject it\" 7 is applicable to any programming language, of course, but is particularly relevant for dynamic languages given the flexibility they provide. There are a few examples of highly successful Python projects that follow the principles outlined above such as the wemake-services Python linter and Pants build system . Conclusion An argument against enforcing stricter development constraints for a dynamic language could be raised. It may indeed take longer time to produce a program where all function parameters and return values have type annotations, variables with the same name are not declared multiple times, and public methods have docstrings. There is no point in such vigorous approach when prototyping, however, as soon as the code is integrated into some bigger unit, the expectations should get higher. Writing robust, easy to extend and maintain large software in Python isn't going to be a whole lot faster than writing it in Java. I believe Python should be chosen to be the language of implementation for reasons other than speed of development (even though it may often be the case), but because of its expressiveness, extensive built-in library and public package repository, community and tooling. The lessons learned about the program design, modularization, and code health when building large scale commercial, enterprise, or open-source software in compiled languages should be explored by dynamic language developers. Having many successful products created in dynamic languages, it is for certain that writing large programs in dynamically typed languages is very much possible. In the end, typing system doesn't define the success of a software product; it's all about the development processes and people. Python can also be said to be compiled given the program's source code is first compiled into bytecode, but we can omit this detail to make the distinction clearer. ↩ Java can be said to be either interpreted or compiled, but we can omit this detail to make the distinction clearer. ↩ Brooks, F.H. The Mythical Man-Month, Addison-Wesley, 1975. ↩ Eric S. Raymond. The Art of Unix Programming, Addison-Wesley, 2003. ↩ Michael L. Scott. Programming Language Pragmatics, 4th edition, Elsevier, 2016. ↩ ↩ John K. Ousterhout. Scripting: Higher-level programming for the 21st century. IEEE Computer, 31(3):23-30, March 1998. ↩ Michael T. Nygard. Release It!, 2nd Edition, January 2018. ↩","tags":"computer-science","url":"/dynamic-static-typing-languages.html","loc":"/dynamic-static-typing-languages.html"},{"title":"Adding a dependency to your Python project: a practical guide","text":"After managing Python projects for quite a few years now, I've learned several things one should think about when bringing a 3rd party dependency to a project. In this post, I'll cover some of the points that are worth considering when deciding to rely on code external to your organization. This post is concerned with a long-living code that is part of a larger piece of software and not a throw-away one-time use Python script to which these concerns do not apply. What are the consequences of bringing external dependencies? When your software starts depending on the 3rd party packages: it takes longer to build a project (downloading and resolving dependencies) you become dependent on an external project that is not guaranteed to be maintained or developed the risks of having incompatible dependencies is increased (particularly when you have multiple external dependencies each having an extensive number of dependencies) upgrading versions of other external dependencies may be more brittle due to potential dependencies conflicts Before bringing in an external dependency, it may be helpful to find out whether it is really needed. In a general case, I believe it's useful to be reluctant to adding any external dependencies unless the benefits of bringing them outweigh the associated cost. If the Python program you write can complete a task without using any 3rd party code, keep it that way. Any code that you haven't written yourself (or is not originated and maintained within your team or organization) that becomes part of your software adds additional risks and maintenance costs. Using a Python library may indeed save development time for the team, however, adding a new dependency should be justified; very often it may not be worth it. Say your program needs to read an input .csv file, apply some filter on the data rows, and produce a new .csv file with a subset of the original one. pandas library would make doing this very easy – this could likely be done in a single line of code. However, unless the program needs to read very large files and do it very often (so there are some performance constraints), you are better off using Python standard library csv module that makes working with .csv files fairly easy. In contrast, when writing a new machine learning library, it would be a pity not to take advantage of existing numerical computation libraries such as numpy and scipy because they are likely to be core foundation of the project. In this case, it is very unlikely that you would need to implement own data structures that would meet the functional and performance requirements of your project, and overall be a better solution than an existing battle tested library. It may also be the case that a 3rd party dependency that you already use in your project provides the desired functionality. For instance, when looking for linear algebra tools, scipy.linalg contains all the functions in numpy.linalg , so if you depend on numpy , you may already have everything you need. Explore the external dependency When you have identified a dependency to bring in, it may be worth spending some time learning more about it. Do the research and explore Snyk Advisor , the project's source code repository , code quality reports , and the PyPI project page to learn more about the project. Library maintenance status and release cadence High commit frequency would indicate active development, and projects that are actively developed are more favorable than stale ones. It is also helpful to see the project owners being receptive to contributions from users which implies that you'd likely be able to submit patches for the bugs that may impact your project. Having a comprehensive test harness with a decent code coverage is highly desired as well. Number of downloads from the PyPI This should give an idea whether the project is used by other organizations and individuals. Any known security issues and vulnerabilities If this is applicable, you may want to explore any reported security issues or use static analysis tools before deciding whether to take in a dependency or not. Developer community A project with a single contributor can be considered to be less reliable than a project with multiple contributors - what happens if the only maintainer leaves the project? Reported issues The number of issues is likely to be proportional to the project popularity, but it's possible to get an idea whether project authors are responsive and work on resolving issues. Python version compatibility If a library of interest uses features of Python 3.9, you may not be able to use it in Python 3.6 environment. There may be a backport of the future Python version functionality for an older version (such as dataclasses ), but it may still stop you from embedding the external library if it is not supported on the version of the target Python runtime environment. Dependencies A project with no (or fewer) external dependencies is easier to integrate than a project with extensive number of external dependencies. Each dependency brings alone its own (transitive) dependencies which increases the chances of dependencies conflicts at dependency resolution time. Licensing A 3rd party package license may or may now allow further distribution of the code or using it in a commercial product. Distribution formats Having only source distributions ( sdist ) may imply that you may need to build the wheel(s) ( bdist_wheel ) to make a binary distribution accessible for your own project build process. Unless you are able to build the wheels yourself and make them available via a binary repository manager such as a hosted PyPI repository or in some other way, libraries with wheels published on PyPI are more preferable. Ideally, there should be wheels available in PyPI for all of your current target architectures (e.g., MacOS wheels for development and Linux for production deployment). Source code programming languages Having a non-pure Python package with certain bits written in a compiled language such as C or Rust would make building binary distributions harder since you'll need to have the necessary toolchain set up and may imply building multiple wheels for each target platform and/or architecture. Adding the dependency If after evaluating a library you have decided to bring it into your Python project, you would need to declare that dependency. The way you do it would depend on your dependency management approach - this may be done via a number of ways such as by using a requirements file, a Poetry project file, or a Pants/Bazel project file. Ideally, constraints file is used to make sure that the same versions of all the transitive dependencies of a project are used, even when a new version of the 3rd party library will be released. The reality is that for any Python project of decent size and complexity it may be difficult to not use 3rd party code. The necessity to add a dependency should be discussed within the team. If there are multiple libraries that provide functionality of interest, write a document that would let you compare them based on the criteria I've mentioned above. Bringing in external dependencies is an important step for your project and should be done with care and planning. Happy depending!","tags":"python","url":"/adding-python-dependency.html","loc":"/adding-python-dependency.html"},{"title":"Introduction to higher order functions with Python","text":"Overview Python is not considered to be a functional language, however it does support the functional paradigm. The Python documentation provides a gentle introduction to functional programming with excellent narrative. As I continue to learn ML , I wanted to share a few interesting concepts around higher-order functions with a few examples written in Python. What is a higher-order function? In Python, functions are first-class which means that a developer can pass functions as arguments to other functions, a function can return a function, and a function can be assigned to a variable or stored in some data structure. A higher-order function, in contrast, is a function that either (or both) takes one or more functions as arguments returns a function as its result The map built-in function is an excellent example of a higher order function as you pass a function as an argument: >>> list ( map ( lambda x : x ** x , [ 1 , 2 , 3 ])) [ 1 , 4 , 27 ] A decorator is also a higher-order function because it takes a function as an argument and returns a function: def twice ( func ): def caller ( ** args ): func ( ** args ) func ( ** args ) return caller @twice def work (): print ( \"Doing work\" ) The work function will be executed twice because it has been decorated with the twice decorator: >>> work () Doing work Doing work Examples of higher-order functions Call a function multiple times re-using the result A function that given x , will call f(x) the n times. The result of each call will become input for the subsequent call. For instance, do_ntimes(lambda x: x+x, 3, 1) is 8 because 1 + 1 = 2 and now it's 2 + 2 = 4 and then finally 4 + 4 = 8 . This happens using a recursive call . def do_ntimes ( f , n , x ): \"\"\"Higher-order function that will do an operation f on x the n times. >>> do_ntimes(lambda x: x+x, 3, 1) 8 >>> do_ntimes(lambda x: x+x, 10, 1) 1024 \"\"\" if n == 0 : return x else : return f ( do_ntimes ( f , n - 1 , x )) Function composition with two functions Function composition is the process of combining two function calls into a single one. def compose_two ( f , g ): \"\"\"Function composition of two functions. >>> compose_two(f=lambda x: x + 10, g=lambda x: x - 5)(10) 15 >>> compose_two(f=lambda x: x - 25, g=lambda x: x * 10)(10) 75 \"\"\" fg = lambda x : f ( g ( x )) return fg It is possible to pass the lambda functions directly or by assigning them to variables first: >>> compose_two ( f = lambda x : x + 10 , g = lambda x : x - 5 )( 10 ) 15 >>> add10 = lambda x : x + 10 >>> minus5 = lambda x : x - 5 >>> compose_two ( f = add10 , g = minus5 )( 10 ) 15 Reducing to calculate the factorial In addition to a recursion based solution (or a plain loop), it's also possible to calculate factorial of a number using the functools.reduce : from functools import reduce from operator import mul def factorial_reduce ( n ): \"\"\"Calculate factorial. >>> factorial_reduce(5) 120 \"\"\" return reduce ( mul , range ( 1 , n + 1 ), 1 ) Reducing a chain of function calls The function composition example can be rewritten in a standalone call. This is done using the built-in functools.reduce() function call. The result of the first function execution becomes the input argument for the next function. In the example below, the operation is ( ( (0 + 10) * 3 ) / 2 ) . >>> reduce ( lambda res , f : f ( res ), [ lambda n : n + 10 , lambda n : n * 3 , lambda n : n / 2 ], 0 ) 15.0 Function composition with arbitrary number of functions Building up on the example above, the compose function can take arbitrary arguments wrapped up in a tuple. def compose ( * functions ): \"\"\"Function composition of arbitrary number of functions. >>> compose(*(lambda x: x + 2, lambda x: x + 5, lambda x: x - 6))(10) 11 >>> compose(*(lambda x: x + 2,))(10) 12 \"\"\" fg = lambda value : reduce ( lambda res , f : f ( res ), functions , value ) return fg Currying to check if elements are sorted When one is currying a function, one is converting a function that takes multiple arguments into a sequence of functions that each take a single argument. def are_3elems_sorted (): \"\"\"Use currying to check if three elements are sorted. >>> ((are_3elems_sorted()(1))(2))(3) True >>> ((are_3elems_sorted()(1))(4))(3) False \"\"\" return lambda x : lambda y : lambda z : z > y > x Happy functioning!","tags":"python","url":"/python-higher-order-functions-intro.html","loc":"/python-higher-order-functions-intro.html"},{"title":"Introducton to recursion with Python","text":"Overview Recursion can be one of the programming concepts that can be a bit of a challenge to understand. I believe this is because the recursive call to a function is not written in a procedural way so that one cannot see the actual sequence of operations that will be taken. Running a program containing a recursive function call with a debugger to be able to step through each line can be very useful. I've spent a few weeks learning ML and was fascinated by the recursion implementation in functional languages and ML in particular. It can be mind bending for anyone who has learned programming through non-functional programming languages that do not encourage use of recursion be it Python or Java. However, I had quite a few \"aha!\" moments when writing recursive functions in ML and I definitely understand recursion much better now. In this post, I wanted to share a few simple recursive functions that are written in Python. I believe practicing writing recursive functions is fun and is a great mind-expanding exercise. Declarative iterations or recursions? I believe the consensus about recursion is that it's appropriate to use recursion in situations when a recursive solution would be better expressed in a recursive call. That is, there isn't a problem that can't be solved without using recursion or if there is one, it must be so rare you'll probably never face it. However, it's very useful to know what recursion is to be able to understand the code that others have written and the caveats associated with using recursion in your programming language. Let's take a look at a few simple cases. Examples of recursive functions If you run any of the of the programs below (each featuring a recursive function call) in a debugger to be able to step through it, you'll see that for each recursive function call, a new frame is added on stack . Python imposes a limit on the number of the recursive calls ( see maximum recursion depth ) which guards against a stack overflow. This means that for most of the real life data, it won't be safe to use the examples below. This is why it can be useful to recognize a recursive call and evaluate whether it's safe to use or not. The coding guidelines for some industries may even explicitly ask to avoid using recursion and replace it with a loop or an iterative algorithm. You can also attempt to solve these tasks as an exercise before looking at my solutions. Each function has a doctest which you can use to check your understanding of the problem. The problems are sorted by difficulty in ascending order. Sum values in an array of integers Given an array of integers, find the sum of all its elements. This can be solved by simply using the built-in sum() which is what most Python developers would use: sum ([ 1 , 2 , 3 , 4 , 5 ]) However, this task can also be solved using a simple for loop and a result variable: total = 0 for number in [ 1 , 2 , 3 , 4 , 5 ]: total += number print ( total ) # 15 And of course, a recursive solution can be written as well: def sum_list ( values ): \"\"\"Sum values in an array of integers. >>> sum_list([]) 0 >>> sum_list([1]) 1 >>> sum_list([1,2,3,4]) 10 \"\"\" if len ( values ) == 0 : return 0 elif len ( values ) == 1 : return values [ 0 ] else : return values [ 0 ] + sum_list ( values [ 1 :]) The way I used to reason about this function call is that I decompose the result of each recursive call which can be helpful to understand what is going on. The way I see the line return values[0] + sum_list(values[1:]) for the initial array of [1,2,3,4] is 1 + sum_list([2,3,4]) => 1 + (2 + sum_list([3,4])) => 1 + 2 + (3 + sum_list([4])) => 1 + 2 + 3 + 4 => 10 Produce range of integers Given a positive integer, produce a list of integers from the given integer down to 0. def countdown ( num ): \"\"\"Get a list of integers from the given positive integer down to 0. >>> countdown(5) [5, 4, 3, 2, 1, 0] \"\"\" if num == 0 : return 0 elif num == 1 : return [ 1 , 0 ] else : return [ num ] + countdown ( num - 1 ) Merge two arrays of items Given two arrays of items, merge them into a single array keeping the order of items. def merge ( arr1 , arr2 ): \"\"\"Merge two arrays of items. >>> merge([1,2,3], [4,5,6]) [1, 2, 3, 4, 5, 6] >>> merge([1,2,3], []) [1, 2, 3] >>> merge([], [4,5,6]) [4, 5, 6] \"\"\" if not arr1 : return arr2 else : return [ arr1 [ 0 ]] + merge ( arr1 [ 1 :], arr2 ) Sum integers in a list of pairs Given an array of non-empty pairs (with each pair containing two integers), find the sum of all numbers in the array. def sum_list_of_pairs ( arr ): \"\"\"Sum integers in a list of pairs. >>> sum_list_of_pairs([]) 0 >>> sum_list_of_pairs([(1,2)]) 3 >>> sum_list_of_pairs([(10,20), (30,40), (50,60)]) 210 \"\"\" if not arr : return 0 else : return arr [ 0 ][ 0 ] + arr [ 0 ][ 1 ] + sum_list_of_pairs ( arr [ 1 :]) Get first elements of sub-arrays in array def firsts ( arr ): \"\"\"Get first elements of all collections in a given array. >>> firsts([]) [] >>> firsts([(1,2,3), (4,5,6)]) [1, 4] >>> firsts([(1,2), (3,4), (5,6)]) [1, 3, 5] \"\"\" if not arr : return [] else : return [ arr [ 0 ][ 0 ]] + firsts ( arr [ 1 :]) Get digits of a number in a given base (up to base of 10) def get_digits_in_base ( num , base ): \"\"\"Get digits of a number in a given base (up to base of 10). >>> get_digits_in_base(192837, 10) ['1', '9', '2', '8', '3', '7'] >>> get_digits_in_base(16, 2) ['1', '0', '0', '0', '0'] >>> get_digits(1000, 16) ['3', 'E', '8'] \"\"\" if num == 0 : return [] quotient , remainder = divmod ( num , base ) return get_digits_in_base ( quotient , base ) + [ str ( remainder )] Get digits of a number in a given base (up to base of 16) def get_digits ( num , base ): \"\"\"Get digits of a number in a given base (up to base of 16). >>> get_digits_in_base(192837, 10) ['1', '9', '2', '8', '3', '7'] >>> get_digits_in_base(16, 2) ['1', '0', '0', '0', '0'] >>> get_digits_in_base(256, 16) ['1', '0', '0'] \"\"\" lookup = '0123456789ABCDEF' if num < base : return [ lookup [ num ]] else : quotient , remainder = divmod ( num , base ) return get_digits ( quotient , base ) + [ lookup [ remainder ]] Tail recursion optimizations A special, perhaps more difficult concept to understand, is tail recursion. The recursive functions we have above have a pattern in the return statement. They return a value and the call to itself (such as return array[0] + func(array[1:]) )). However, if a function would only call itself recursively and wouldn't need to keep any intermediate data, it would be considered a tail recursive function. That is, since there is no intermediate data to maintain (which is why the stack frames are needed), we can essentially replace the current stack frame where the function is calling itself with the new stack frame since we don't need to get back to it – we would only be interested in the final recursive call that would have the final value we need. Factorial of a positive integer This function is not tail-recursive because we need to maintain the value of n that we will multiply with the result of factorial(n-1) . def factorial ( n ): \"\"\"Get factorial of n. >>> factorial(0) 1 >>> factorial(5) 120 \"\"\" if n == 0 : return 1 else : return n * factorial ( n - 1 ) The reason we would want to make our function tail-recursive is because some programming languages provide what is called a tail call optimization . This is how a functional programming language such as Scala supports hundreds of thousands recursive function calls, see here . Python does not have tail call optimization ( here's why ), but let's rewrite the factorial function in a tail-recursive fashion anyway to practice. The idea around tail-call optimization is to use some kind of accumulator, the acc parameter in the example below, that tracks the intermediate data and is passed across the subsequent calls thus eliminating the need to keep the previous stack frames. def factorial_tail ( n , acc ): \"\"\"Get factorial of n in a tail-recursion fashion. >>> factorial_tail(0, 1) 1 >>> factorial_tail(5, 1) 120 \"\"\" if n == 0 : return acc else : return factorial_tail ( n - 1 , acc * n ) Let's write a few more examples of functions with tail call. Sum values in an array (tail call) def sum_values_tail ( arr , acc ): \"\"\"Sum values in an array in a tail-recursion fashion. >>> sum_values_tail([1,2,3], 0) 6 >>> sum_values_tail([], 0) 0 \"\"\" if len ( arr ) == 0 : return acc else : return sum_values_tail ( arr [ 1 :], arr [ 0 ] + acc ) Reverse an array (tail call) def reverse_list_tail ( arr , final ): \"\"\"Reverse list in a tail-recursion fashion. >>> reverse_list_tail([], []) [] >>> reverse_list_tail([1], []) [1] >>> reverse_list_tail([1,2,3], []) [3, 2, 1] \"\"\" if len ( arr ) == 0 : return final else : return reverse_list_tail ( arr [ 1 :], [ arr [ 0 ]] + final ) Reduce (tail call) def reduce_tail ( func , acc , arr ): \"\"\"Reduce (fold) an array to a single value applying a function. >>> reduce_tail(lambda x,y: x * y, 1, [1,2,3,4]) 24 >>> reduce_tail(lambda x,y: x + y, 0, [1,2,3,4]) 10 \"\"\" if len ( arr ) == 0 : return acc else : return reduce_tail ( func , func ( acc , arr [ 0 ]), arr [ 1 :]) Mutual recursion Another more advanced concept is mutual recursion which is a type of recursion when two functions are defined with referral to each other. In this example, given an array of integers, we want to check if it follows a certain pattern, [1,2,1,2...1,2] in this particular case. Once the function responsible to confirm that the first item of the array is 1 is done, it calls another function that confirms that the first item of the array is 2 , and then it calls the first function that confirms that the first item of the array is 1 and the cycle repeats. def pattern_match_one_two ( arr ): \"\"\"Check if a sequence is a pattern 1,2 repeated using a mutual recursion. >>> pattern_match_one_two([1,2,1,2,1,2]) True >>> pattern_match_one_two([1,2,1,2,3,1,2]) False >>> pattern_match_one_two([2,1,2,1,2]) False \"\"\" if len ( arr ) == 0 : return True else : if arr [ 0 ] == 1 : return needs_two ( arr [ 1 :]) else : return False def needs_two ( arr ): if len ( arr ) == 0 : return True else : if arr [ 0 ] == 2 : return pattern_match_one_two ( arr [ 1 :]) else : return False Hope this short introduction helped you understand recursion better and you find the code examples illustrative and useful. Happy recursive coding!","tags":"python","url":"/python-recursion-intro.html","loc":"/python-recursion-intro.html"},{"title":"Python raising SyntaxError when having too many nested for loops","text":"Overview When writing programs in any programming language, it is common to see some syntax or runtime errors. For instance, in Python, it is easy to mess up the indentation in a file after merging files from different codebases. Likewise, one can make an off-by-one error when accessing an array which will be found at the runtime only. Some other types of errors, however, are very rare and it is likely that you will not see many of them in your lifetime as a Python programmer. For instance, if you never use recursion to process a large array, you may never be hit by the maximum recursion depth limitation that exists to guard against a stack overflow. In this post, I document my findings around an issue I have faced when attempting to run auto-generated Python code that contained many nested for loops. Use case I was working on a simple code generation library that given input numeric matrix would produce boilerplate Python program code that could be extended further. I thought it would be useful to experiment how the tool would behave on a matrix of many dimensions because that would involve creating quite a few nested loops. I've been planning to start using itertools.product instead of relying on nested for loops, but wanted to experiment before refactoring. The generated Python code looked like this: for i in range ( 1 ): print ( 0 ) for i in range ( 1 ): print ( 1 ) for i in range ( 1 ): print ( 2 ) # all the way to the 20th nested \"for\" loop for i in range ( 1 ): print ( 18 ) for i in range ( 1 ): print ( 19 ) for i in range ( 1 ): print ( 20 ) You can generate this Python code programmatically if you'd like to experiment: loop = \"\"\" {for_spaces} for i in range(1): {print_spaces} print( {loop_number} ) \\n \"\"\" code = \"\" for i in range ( 0 , 21 , 1 ): code += loop . format ( for_spaces = \" \" * 2 * i , print_spaces = \" \" * 2 * i + \" \" , loop_number = i , ) print ( code ) A very useful tool I've been using occasionally to verify that Python module contains syntactically valid code is compileall which can be used both from a command line and in Python programs. compileall tool will compile your source code into bytecode files ( .pyc ) and if there are any syntax errors, the compilation will fail reporting the problem. Bytecompiling the following Python code: print \"hello!\" produces $ python3 -m compileall code.py Compiling 'code.py' ... *** File \"code.py\" , line 1 print \"hello!\" &#94; SyntaxError: Missing parentheses in call to 'print' . Did you mean print ( \"hello!\" ) ? compileall has also been very useful when migrating legacy codebases from Python 2 to Python 3 when it was used for the first-level sanity check. Too many statically nested blocks Bytecompiling the module with 20+ nested for loops: $ python3 -m compileall too_many_nested_for_loops.py Compiling 'too_many_nested_for_loops.py' ... *** File \"too_many_nested_for_loops.py\" , line None SyntaxError: too many statically nested blocks It turns out that Python has a limit on how many nested blocks (so not just for loops) one is allowed to have. This seems to be a design decision that was made when the CPython interpreter was developed. CPython has a concept of a stack, namely blockstack , which is used to execute code blocks, and it's maximum size is 20. This is an internal implementation detail which I'd unlikely ever hit dealing with human written Python code, but I find it to be very exciting to be able to see a low level detail of CPython design. This Stackoverflow question provides a more thorough explanation of this limit. Happy coding!","tags":"python","url":"/python-too-many-nested-loops.html","loc":"/python-too-many-nested-loops.html"},{"title":"Some helpful Bash notes","text":"Overview There are quite a few resources online on Bash scripting which are extremely useful. I particularly recommend Awesome Bash and The Art of Command Line . There is little point writing yet another Bash tutorial, however, I'd like to share a few helpful notes which others who just start using a command line may find useful. Multiple arguments to the same command Many Unix commands accept multiple arguments, one after another: $ ls *.jar *.vsix tmp-142zodmSW1YLvRv.vsix tmp-417e1PTGYuSLOnV.vsix winstone10385665803316081333.jar tmp-192iFDUu55RqcPk.vsix tmp-417qbhJlHnXzkZl.vsix winstone4439219046698760640.jar tmp-31894bwaoyq9zL7em.vsix tmp-549k8JeuzrFyHMC.vsix winstone4470366079702491377.jar $ touch foo.bar foo.baz $ ls foo.* foo.bar foo.baz One line for loop It's common to see a for loop that spans over multiple lines in shell scripts: for jarfile in *.jar ; do file ${ jarfile } ; done However, working with the for loop spanning over multiple lines in terminal can be cumbersome. Fortunately, the for loop can be put into a single line: $ for jarfile in *.jar ; do file ${ jarfile } ; done winstone10385665803316081333.jar: Java archive data ( JAR ) winstone4439219046698760640.jar: Java archive data ( JAR ) winstone4470366079702491377.jar: Java archive data ( JAR ) Reading standard input Some programs are limited and may not accept files as input arguments. Another use case is when you have to pass sensitive data such as passwords as input to programs using a command line interface (so that it doesn't end up in the terminal history). For example, you may need to produce a semicolon separated list of files (and some program has already produced a list of files): $ cat files.txt winstone10385665803316081333.jar winstone4439219046698760640.jar winstone4470366079702491377.jar $ tr '\\n' ';' < files.txt winstone10385665803316081333.jar ; winstone4439219046698760640.jar ; winstone4470366079702491377.jar ; % Submit multiline input to a command When you need to supply a multiline input to a command, particularly if this needs to happen interactively, you can use a here document which can be used within a shell script file or at a prompt: $ tr '[:lower:]' '[:upper:]' << END first line and second line and third line END FIRST LINE AND SECOND LINE AND THIRD LINE Another useful feature is to be able to write multiline input to a file. This can be very handy when you have to create a multiline file (potentially with non trivial indentation) and the machine you are connected to does not have any text editors available. $ cat << EOF > dummy.txt The file contents to be written: line 1 and line 2 EOF $ cat dummy.txt The file contents to be written: line 1 and line 2 Single and double quotes Single quotes do not let filename and variable expansion to happen in the quoted text. Be very careful mixing the single and double quotes! $ export SITE_TOKEN = 'mytoken' $ echo \" $SITE_TOKEN \" mytoken $ echo '$SITE_TOKEN' $SITE_TOKEN Grouping commands and values using curly braces It can be very useful to be able to run multiple commands, one after another, and save the output to a file. This would let you avoid having multiple lines in your script (where each line would be appending to the file). For instance, to create a log of some system operation: $ { date ; whoami ; echo \"----\" ; ls /var } > log $( date '+%Y-%m-%d' ) .txt $ head -n 5 log2020-12-15.txt Tue Dec 15 22 :21:59 GMT 2020 username ---- backups cache crash Happy shelling!","tags":"bash","url":"/some-helpful-bash-notes.html","loc":"/some-helpful-bash-notes.html"},{"title":"Using python3-apt Debian package for system package management with Python","text":"For Debian-based systems such as Ubuntu, most package management happens via the apt system package. It provides a friendly command line interface, however, there aren't many robust ways to use it in some other way other than via a terminal. Fortunately, there is a Python package, python3-apt , which provides a Python 3 interface to the libapt-pkg library. With this package, you'll be able to list installed packages, check what packages are available for installation, install new packages and so much more. Installation python3-apt package should be installed with apt . The sources are currently available at GitLab: python-apt repo . There isn't a great amount of resources that will help you get started with python3-apt , but the official documentation is very comprehensive. To experiment with the basic usage of the python3-apt package, let's define a Docker image: FROM ubuntu:18.04 RUN apt-get -qq update RUN apt-get install -y --no-install-recommends \\ python3-apt \\ ca-certificates \\ gnupg and build it: $ docker build -t python-apt-docker . Installing a package Let's install a package, make sure that it's available, and then delete it. This can be useful when you need to install a package, but simply using subprocess to make a system command call won't suffice. Parsing apt output is very unreliable and is strongly discouraged. import subprocess import shutil import apt # check that tree is not installed print ( f \"tree executable location: { shutil . which ( 'tree' ) } \" ) # update the cache cache = apt . cache . Cache () cache . update () cache . open () # mark packages you'd like to install package = cache [ 'tree' ] package . candidate = package . versions . get ( '1.7.0-5' ) package . mark_install () cache . commit () # open the cache again and inspect installed package cache = apt . cache . Cache () pkg = cache . get ( 'tree' ) print ( f \"Package installed: { pkg . is_installed } ; version: { pkg . installed . source_version } \" ) print ( f \"Package location: { shutil . which ( 'tree' ) } \" ) # delete the package pkg . mark_delete () cache . commit () # open the cache again and check that the tree package is gone cache = apt . cache . Cache () print ( f \"tree executable location: { shutil . which ( 'tree' ) } \" ) Let's run this code in a Docker container: $ docker run -it --rm -v ${ PWD } /:/project python-apt-docker /bin/bash -c 'python3 /project/install_tree.py' tree executable location: None debconf: delaying package configuration, since apt-utils is not installed Selecting previously unselected package tree. ( Reading database ... 5200 files and directories currently installed. ) Preparing to unpack .../tree_1.7.0-5_amd64.deb ... Unpacking tree ( 1 .7.0-5 ) ... Setting up tree ( 1 .7.0-5 ) ... Package installed: True ; version: 1 .7.0-5 Package location: /usr/bin/tree ( Reading database ... 5207 files and directories currently installed. ) Removing tree ( 1 .7.0-5 ) ... tree executable location: None Checking what packages are installed Let's find out what Python related packages are available in a system. This can be handy for a script that makes sure that system dependencies have been installed. import re import apt_pkg apt_pkg . init_config () apt_pkg . init_system () pkgs = [ pkg for pkg in apt_pkg . Cache () . packages if re . compile ( r 'python' ) . match ( pkg . name )] for pkg in [ pkg for pkg in pkgs if pkg . current_state == apt_pkg . CURSTATE_INSTALLED ]: print ( pkg . name ) Let's run this code in a Docker container: $ docker run -it --rm -v ${ PWD } /:/project python-apt-docker /bin/bash -c 'python3 /project/list_pythons.py' Reading package lists... Done Building dependency tree Reading state information... Done python3.6 python-apt-common python3 python3.6-minimal python3-minimal python3-apt Adding additional Debian sources If the default sources available for system packages do not provide the packages you need (which is a common case for legacy packages or corporate environment Debian pools), it is possible to add additional apt sources using SourcesList on demand. Adding public sources import subprocess import aptsources.sourceslist as sourceslist # showing original sources sources = sourceslist . SourcesList () uris_before = set ([ source . uri for source in sources . list ]) print ( uris_before ) # adding a custom apt source source = ( \"deb [trusted=yes]\" , \"http://download.virtualbox.org/virtualbox/debian\" , \"bionic\" , [ \"contrib\" ]) sources . add ( * source ) sources . save () # showing extended sources uris_after = set ([ source . uri for source in sources . list ]) print ( uris_after ) # printing the contents of the sources.list file process = subprocess . Popen ([ \"tail\" , \"/etc/apt/sources.list\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE ) out , errors = process . communicate () print ( out . decode ()) print ( errors . decode ()) # install a package from the VirtualBox Debian pool ... Let's run this code in a Docker container: $ docker run -it --rm -v ${ PWD } /:/project python-apt-docker /bin/bash -c 'python3 /project/install_from_public.py' { '' , 'http://archive.ubuntu.com/ubuntu/' , 'http://archive.canonical.com/ubuntu' , 'http://security.ubuntu.com/ubuntu/' } { '' , 'http://archive.ubuntu.com/ubuntu/' , 'http://archive.canonical.com/ubuntu' , 'http://security.ubuntu.com/ubuntu/' , 'http://download.virtualbox.org/virtualbox/debian' } # deb http://archive.canonical.com/ubuntu bionic partner # deb-src http://archive.canonical.com/ubuntu bionic partner deb http://security.ubuntu.com/ubuntu/ bionic-security main restricted # deb-src http://security.ubuntu.com/ubuntu/ bionic-security main restricted deb http://security.ubuntu.com/ubuntu/ bionic-security universe # deb-src http://security.ubuntu.com/ubuntu/ bionic-security universe deb http://security.ubuntu.com/ubuntu/ bionic-security multiverse # deb-src http://security.ubuntu.com/ubuntu/ bionic-security multiverse deb [ trusted = yes ] http://download.virtualbox.org/virtualbox/debian bionic contrib Adding private sources To add private sources (that may require authentication), you would need to make some changes. In the Docker image, you may need to download the GPG key from the server that hosts Debian packages. The key can be added later using the apt-key add command. You would also need to add authentication file(s) to the /etc/apt/ directory so that apt would be able to use the authentication details when attempting to download the Debian packages from the private repository. apt_auth.conf files can be handled using a built-in netrc Python module so writing a custom file parser is not necessary. Once the sources.list has the private repository listed, you can install the packages from the repository that requires authentication in the same way as we have installed the tree package earlier. Happy packaging!","tags":"python","url":"/use-apt-from-python.html","loc":"/use-apt-from-python.html"},{"title":"Patching with unittest.mock for Python testing: cheat sheet","text":"Overview of patching Python built-in unittest framework provides the mock module which gets very handy when writing unit tests. It also provides the patch entity which can be used as a function decorator, class decorator or a context manager. The basic idea behind patching is that it lets you temporarily change the object that is being used when your test runs. For example, if you are testing a function that needs to read some data out of a database, you can patch it so that you don't need to communicate with any external services when unit tests run. Given this piece of code, we are interested in testing the get_customers() function. We have tests in place for functions ( validate_input and fetch ) that this function calls, so that we can ignore them. def get_customers ( city : str ): validate_input ( object_type = CITY , value = city ) customers = fetch ( query = f 'customers;city= { city } ' ) return customers To patch those functions, instead of calling them at the execution time, anonymous lambda functions could be used. For the validate_input() function, we are not interested in the return value; however, for the fetch function we are. from unittest.mock import patch from utils import get_customers @patch ( 'utils.validate_input' , lambda object_type , value : None ) def test_get_customers (): expected_customers = [ 'Customer1' , 'Customer2' ] with patch ( 'utils.fetch' , lambda query : expected_customers ): actual_customers = get_customers ( city = 'Paris' ) assert actual_customers == expected_customers The patched fetch function doesn't have to return any meaningful value as we will only be testing whether it returned what the patching function is supposed to return. However, I often find it to be easier to understand the context and business logic when some more real values are used. Note that the patch() was used both as a context manager and as a test function decorator. In the rest of the post, we will explore typical use cases for patching and mocking. Patching class initialization Use case: You want to test a class method, but initializing a class instance would require a lot of additional mocking to pass valid input parameters. You also want to avoid any real initialization operations, but would still want to have some of the class instance variables set. Code: class FileProcessor : def __init__ ( self , files : List [ Path ], process_config : ProcessorConfiguration ): self . files = files self . process_config = process_config def validate_files ( self ): # operate on the files and return some validation result ... Test: def test_validate_files (): with patch . object ( FileProcessor , '__init__' , lambda self : None ): fp = FileProcessor () fp . files = [ Path ( 'foo' ), Path ( 'bar' )] assert fp . validate_files () . get ( \"status\" ) == ValidationStatus . SUCCESS fp . files = [] assert fp . validate_files () . get ( \"status\" ) == ValidationStatus . FAILURE Patching static, class, and instance methods Use case: You have a class for which you want to patch some of the class instance methods, class methods, or static methods. You would need to patch a method of interest and the process of patching the methods is universal for all the methods types. Code: class Address : def __init__ ( self , house : str , street : str , postal_code : str , city : str ): self . house = Address . numerize ( house ) self . street = street self . postal_code = postal_code self . city = city @classmethod def from_tuple ( cls , * address_tuple : Tuple ): return Address ( * address_tuple ) @staticmethod def numerize ( value ): return int ( value ) def to_string ( self ): return f \" { self . house }{ self . street } \\n { self . postal_code } \\n { self . city } \" def print ( self ): fmt = self . to_string () print ( fmt ) return fmt def is_valid_address ( address_tuple ): try : Address . from_tuple ( * address_tuple ) return True except Exception : return False Test: def test_is_valid_address (): with patch ( 'static_class.Address.from_tuple' , lambda * data : True ): assert is_valid_address (( \"1\" , \"New Road\" , \"99999\" , \"City\" )) with patch ( 'static_class.Address.from_tuple' , side_effect = Exception ()): assert not is_valid_address (( \"1\" , \"New Road\" , \"99999\" , \"City\" )) def test_address_construct (): with patch ( 'static_class.Address.numerize' , lambda value : 9999 ): address = Address ( * ( \"1\" , \"New Road\" , \"99999\" , \"City\" )) assert address . city == \"City\" assert address . house == 9999 def test_print (): with patch ( 'static_class.Address.to_string' , lambda value : \"Address formatted\" ): address = Address ( * ( \"1\" , \"New Road\" , \"99999\" , \"City\" )) assert address . print () == \"Address formatted\" Patching a class instance attribute Use case: You have a class that when instantiated has an attribute. You are interested in patching this attribute to be set to some value. Code: # country:capital mapping countries_capitals = { \"France\" : \"Paris\" } class City : def __init__ ( self , name : str ): self . name = name def get_capital ( country : str ): city = get_city ( countries_capitals . get ( country )) return city . name def get_city ( name : str ): return City ( name ) Test: We want to patch the get_city() function when testing the get_capital() method. from unittest.mock import patch from class_instance_fields import get_capital def test_get_capital (): with patch ( 'class_instance_fields.get_city' ) as mock : mock . return_value . name = \"Capital\" assert get_capital ( \"Country\" ) == \"Capital\" Patching a nested class instance attribute Use case: You have a class that when instantiated has an attribute that itself is an instance of a class with additional instance attributes. You are interested in patching this nested attribute to be set to some value. Code: from typing import List class City : def __init__ ( self , name : str ): self . name = name class Address : def __init__ ( self , address : List [ str ]): self . city = City ( address [ 0 ]) self . street = address [ 1 ] self . house = address [ 2 ] class Customer : def __init__ ( self , address : List [ str ]): self . address = Address ( str ) def get_address ( self ): return self . address Test: We want to patch the get_address() method on the class instance and then set a returned object to have a nested attribute set to some value. This may be necessary when you are trying to cover a code branch which will be executed only when a value of the nested attribute is equal to a certain value. from typing import List from unittest.mock import patch , MagicMock def test_customer_address (): with patch ( 'customer.Customer' ) as mock_customer : c = mock_customer . return_value mock_address = MagicMock () # MagicMock lets you define nested attributes mock_address . city . name = \"BigCity\" c . get_address . return_value = mock_address assert c . get_address () . city . name == 'BigCity' Patching class instance method that modifies self Use case: You have a method which doesn't return anything but instead sets or modifies its object ( self ) properties. You cannot patch the mock's method with .return_value and thus need instead modify the values of the self . Code: class Client : def __init__ ( self , guid ): self . guid = guid self . visited = False self . last_time_visited = None def visit ( self ): self . visited = True self . last_time_visited = datetime . now () def process ( self ): self . visit () Test: We would like to test the process() method without letting it to execute the visit() method, but still check that after running the process() method, the Client instance would get certain fields set. from client import Client def setattrs ( obj , ** kwargs ): for k , v in kwargs . items (): setattr ( obj , k , v ) def test_process (): mock_client = Mock () with patch ( 'client.Client' , lambda : mock_client ): mock_client . process . side_effect = lambda : setattrs ( mock_client , visited = True , last_time_visited = datetime . now ()) client = Client ( 99 ) client . process () assert client . visited is True assert isinstance ( client . last_time_visited , datetime ) Patching multiple calls to the same function Use case: You call a function being mocked multiple times in the source code being tested and need to return a different value for each call. For instance, you mock a function call that will check whether a database table exists, then you call a function to delete it, and then use the first function again to make sure that the table does not exist (i.e., it was successfully deleted). You cannot simply mock the function with the patch because it will then return the same value. You have to use the Mock().side_effect property. The side_effect collection can even have an exception initialization if at a certain time you call the mocked function a particular exception is expected to be raised: side_effect = [10, 15, ValueError()] . Code: def get_table ( name ): return True def delete_table ( name ): return True def delete_db_table ( table_name : str ) -> bool : if get_table ( table_name ): delete_table ( table_name ) if not get_table ( table_name ): return True else : raise ValueError ( f \"Failed to delete table { table_name } \" ) return True Test: @patch ( 'db.delete_table' , lambda name : True ) def test_delete_db_table (): get_table_mock = Mock () # db table gets deleted get_table_mock . side_effect = [ True , False ] with patch ( 'db.get_table' , get_table_mock ): assert delete_db_table ( \"LogHistory\" ) # db table fails to be deleted get_table_mock . side_effect = [ True , True ] with patch ( 'db.get_table' , get_table_mock ): with pytest . raises ( ValueError ): delete_db_table ( \"LogHistory\" ) Patching function and its returned object Use case: When you patch a function or a method, it may be the case that the object that it will return may have own methods that you would want to patch. For instance, when patching subprocess.run() that returns a CompletedProcess object, you may want to patch its .check_returncode() method to return some value. Just as with the Mock().side_effect() , it's possible to create side effects for methods of a Mock() object. Code: def call_cmd ( cmd : str ): res = subprocess . run ( cmd ) return res . check_returncode () Test: def test_call_cmd (): run_mock = Mock () run_mock . check_returncode . return_value = \"0\" with patch ( 'subprocess_run.subprocess.run' , lambda cmd : run_mock ): assert call_cmd ( 'du -sh lib' ) == \"0\" run_mock = Mock () run_mock . check_returncode . side_effect = subprocess . CalledProcessError ( 0 , 0 ) with patch ( 'subprocess_run.subprocess.run' , lambda cmd : run_mock ): with pytest . raises ( CalledProcessError ): call_cmd ( 'du -sh non-existing-dir' ) Patching class properties Use case: You have a class with one or more properties decorated with the @property . Mocking them requires special handling using the unittest.mock.PropertyMock object. This is useful when the initialization of an object's property is complex and is not really required for a particular unit test. Code: class Process : def __init__ ( self , pid : int ): self . _pid = pid @property def pid ( self ): return self . _pid def as_string ( self ): return f 'Process( { self . pid } )' Test: def test_process_pid_property (): with patch . object ( Process , '__init__' , lambda self : None ): with patch ( 'class_properties.Process.pid' , new_callable = PropertyMock ) as mock_pid : mock_pid . return_value = 99 process = Process () assert process . as_string () == 'Process(99)' Patching opening a file with open Use case: You have a function that is interacting with the file system by opening a file. A helper function unittest.mock.mock_open can be used to replace the use of open . Code: def read ( path ): with open ( path ) as f : return f . readlines () Test: def test_read_file (): with patch ( 'read_file.open' , mock_open ( read_data = \"lines\" )): assert read ( 'dir/path' ) == [ \"lines\" ] Notes When mocking is required for too many places, it may be the case that your class or a function is too big and would be a good candidate for refactoring. To make mocking for a large (or complex) code unit easier, you can use unittest.mock.MagicMock() instead. When patching code that is interacting with the file system too often, patching all the os , pathlib , and shutil things can get tedious rather quickly. You may want to take a look at the virtual file system, pyfakefs , to be used when testing. Keep in mind that you won't be able to use mock.patch to patch your file system interaction functions when you use pyfakefs because it patches them on its own. When your programs are really scripts that process some files, you may be better off writing good integration tests using real data instead. This could be particularly true when your Python program is relying on using non-Python code such as compiled C (for which you have no source code) to read/process/write data. In this situation, you won't be able to use the pyfakefs virtual file system and mocking all the interaction between your Python programs and external tools can be tedious. If the programs for which you write unit tests interact with external web services over HTTP a lot, you may look into the vcrpy package that can automatically mock your HTTP interactions. The pytest-recording plugin provides handy custom markers. Happy patching!","tags":"python","url":"/patching-mock-python-unit-testing.html","loc":"/patching-mock-python-unit-testing.html"},{"title":"Working with stdout in Python scripts","text":"Overview When working with an existing Python script, particularly a legacy script, or a script that was supposed to be used once and then thrown away but grew into a business critical application (yep, this happens), it can be common to see extensive usage of print or logging statements. Those statements can be spread across the program code and often provide useful information regarding the status of the process while the script is being executed. However, if you have been writing a new script and have finished working on it, or if the script output is not of interest any longer, you most likely wouldn't want to clutter the Python console with print / logging outputs (particularly if the script is part of another larger pipeline). However, the information emitted can still be useful to get logged. Redirecting to a file Instead of removing each print statement (or switching to logging.debug from logging.info ), it is possible to specify to what file the sys.stdout will redirect writing to. This will make the print and logging calls to write to a file on disk instead. import sys # keeping the original reference to the output destination stdout = sys . stdout print ( \"Started script\" ) # redirecting the print statements to the file f = open ( 'log.txt' , 'a' ) sys . stdout = f # main program execution, gets logged to a file print ( \"Getting work done\" ) # setting it to the original output destination sys . stdout = stdout f . close () print ( \"Finished script\" ) Now, when running the program, the print() calls within the main program logic are being redirected to a file on disk. $ python3 program_print.py Started script Finished script $ cat log.txt Getting work done Redirecting to StringIO It is also possible to use the io.StringIO() object to capture everything that will be written to the stdout for the whole script or only a portion of it. import sys from io import StringIO print ( \"Started script\" ) # to capture anything that will be written to the stdout buf = StringIO () stdout = sys . stdout sys . stdout = buf print ( 'Getting work done' ) sys . stdout = stdout # collecting what has been written into a variable captured = buf . getvalue () print ( \"Finished script \\n \" ) print ( captured ) Now, when running the program, the print() calls within the main program logic are being collected into a variable (which is printed here for examination, but can be used for any custom logging). $ python3 program_stringio_var.py Started script Finished script Getting work done Overriding the sys.stdout.write method In both of the examples above, the text that was sent to the original stdout wasn't shown in the console (it's either simply suppressed or captured into a variable). However, it can be sometimes useful to print the output both to the console and put the output into a variable. For this use case, we are essentially after what the tee command does in Linux (which can read stdin and then write it to both the stdout and to a file). In Python, this can be achieved by overriding the sys.stdout.write method. import sys from io import StringIO class StdOutTee : def __init__ ( self , * authors ): self . authors = authors def write ( self , text ): for author in self . authors : author . write ( text ) print ( \"Started script\" ) # to capture anything that will be written to the stdout buf = StringIO () stdout = sys . stdout sys . stdout = StdOutTee ( buf , stdout ) print ( 'Getting work done 1' ) print ( 'Getting work done 2' ) sys . stdout = stdout # collecting what has been written into a variable captured = buf . getvalue () print ( \"Finished script \\n \" ) print ( captured ) Now, when running the program, the print() calls within the main program logic are being collected into a variable (which is printed here for examination, but can be used for any custom logging). However, all the print() statements are printed as well. $ python3 program_tee.py Started script Getting work done 1 Getting work done 2 Finished script Getting work done 1 Getting work done 2 Buffering and flushing When you run a Python program, if the standard output ( stdout ) of its process is redirected to some other target (different from your active terminal), then the output of this process will be buffered into a buffer. Therefore, output of Python programs that have any text sent to the stdout may be buffered and not shown until the newline character ( \\n ) is sent. This program won't print anything in your Python console or terminal when being run: import time for i in range ( 5 ): print ( i , end = \" \" ) time . sleep ( . 2 ) In contrast, if there is a print call (which by default has the newline character as its end parameter ), the output will be shown; however, all the numbers will be printed at once (not one after another with 0.2 second interval) : import time for i in range ( 5 ): print ( i , end = \" \" ) time . sleep ( . 2 ) print () To be able to see each number being printed instead of waiting for the loop to complete and see them all at once, one can change the stdout buffering with the stdbuf utility. However, the end parameter has to be a newline character: $ stdbuf -oL python3 program.py > result.log Alternatively, one can use the flush parameter of the print function: import time for i in range ( 5 ): print ( i , flush = True ) time . sleep ( 2 ) and the call becomes (running tail -F result.log will let you see numbers printed in real time): $ python3 std.py > result.log A solution that does not involve flushing is to set the PYTHONUNBUFFERED environment variable. When this environment variable is set, the stdout of the Python process will be sent to the active terminal in real time (which can be useful for tailing any application logs, particularly inside a Docker container). The same effect can also be achieved by passing the -u parameter: $ python3 -u std.py > result.log Happy printing!","tags":"python","url":"/working-with-stdout-python.html","loc":"/working-with-stdout-python.html"},{"title":"Building cli Python applications with Click","text":"Overview When writing cli tools using Python, if the complexity is low, using a plain argparse may suffice. Despite being a built-in module, it's still very capable and relatively flexible. In fact, a few large open-source projects have survived using argparse without using any custom cli frameworks. For instance, Conan – a popular C/C++ package manager written in Python – and Google API Client for Python – Google's discovery based APIs – are using argparse for their cli interfaces. When argparse limitations get in the way, you may start looking for Python frameworks that allow developing cli applications . There is a post with practical demonstrations of most popular Python cli frameworks that is worth reviewing: Building Beautiful Command Line Interfaces with Python . Building a cli with Click My personal preference for a Python cli framework is Click . It has the functionality I want to have when building cli applications and whenever I needed something a bit peculiar, I was able to find the answers online thanks to posts of Stephen Rauch . To save time for others, I've created a boilerplate repository – click-cli-boilerplate – that contains everything that one would need to get started developing a cli application using Click . It features the Python project source code layout, cli interface and implementation relation, tests, packaging, and docs generation. You will find some brief notes on how to write tests, how to generate the docs using the sphinx-click extension, and how to distribute the cli application as a Python wheel and let users install it with the pipx . Happy cli-ing!","tags":"python","url":"/building-cli-python-apps-with-click.html","loc":"/building-cli-python-apps-with-click.html"},{"title":"Brief overview of using Git LFS for managing binary files","text":"Overview Normally a Git repository is used to manage source code which is stored most often as plain text. Tracking changes for text is very easy because only the changes between two commits would need to be saved, not the whole copies of the files. However, a project source code repository may also contain binary files such as images, compiled code, or archives. Developers from quite a few industries such as gaming or computer-aided design and digital mapping (e.g. textures, CAD drawings, and map style files) often have to manage and store large files. Having files of a few megabytes or hundreds of megabytes in size can be very common in the project source code repository, however, there is nothing wrong with having them there since this is where they really belong. Problem of keeping binary files under Git Because Git cannot track changes between binary files, for each modification of a binary file, a copy of the modified file will be created and stored. This can make the repository unnecessary large and slow to clone and check out. Always overwriting the binary file with the latest file state (to keep only the \"latest\") defeats the purpose of the source code management as one should be able to have access to the history even if it implies storing a hundred of binary files each differing from others by just a few bytes. What is a large file is a subject for discussion. I'd also encourage to think about how often a binary file will change; if it's a couple of megabytes static image used in a background of your terminal app, you may be fine just storing it as is. If it's a dynamic file that will be modified daily by multiple developers, just half a megabyte digital drawing file can bloat the repository for all of time if it's modified often. Having many tiny binary files that are changed often can have a similar effect. Using Git LFS for tracking binary files A more efficient way to store the binary files is to store them not under the Git repository (when a change to a binary file will cause creating its full copy), but in a separate storage system such as Git LFS . This system lets you store in the Git repository only the pointers to versions of the binary files, whereas the files themselves are stored separately. When cloning the repository with the latest master , you will only need to download the latest file, not its whole history. When checking out a feature-branch (that may have another representation of the very same file), another file version will be downloaded. Most of the major source code management providers such as GitHub, GitLab, and BitBucket provide support for Git LFS and enabling it is extremely easy. To learn more about Git LFS and support for large objects in Git, see the excellent video Native Git support for large objects from the Git Merge 2019. Migration of files to LFS The decisions about management of the binary files should be made as early as possible when setting up the repository. This is because it's a lot easier to start using Git LFS when a new repository is created rather than when binary files have already sneaked into the Git history. Ideally, you shouldn't be tracking with Git binary files that are supposed to be modified often. If the files did sneak into the Git history, simply removing the files and then starting storing them in a separate LFS system won't be enough as the Git repository will still have copies of those binary files in the history (in the .git directory). It is possible to remove them completely, but this would require \"rewriting\" the history and would require careful coordination with anyone else using the repository to run a few git rebase --onto sessions. For a repository with a few branches used by a few developers this won't be a problem, but it can become impractical and plain tedious to migrate the files for a large repository with many contributors and many branches. If you do need to move the files out of a \"regular\" Git to the LFS system, refer to the Migrating existing repository data to LFS page section in the Git LFS tutorial. Happy storing!","tags":"git","url":"/overview-using-git-lfs-binary-files.html","loc":"/overview-using-git-lfs-binary-files.html"},{"title":"Running Python tests with tox in a Docker container","text":"Overview When you are working on Python code that is supposed to be running on Python interpreters of multiple versions (and potentially with multiple versions of 3rd party packages), to be able to test that your code works and produces expected result you would need to create isolated virtual environments. Each of these virtual environments will have a certain version of Python and a certain version of each 3rd party package that your programs depend on. By having just a few versions of Python with a couple of versions of a few packages, it becomes rather tedious to create and maintain those virtual environments manually very soon. tox is a tool that can help you with this. Preparing Python virtual environments It is possible to create Python virtual environments manually and then let tox use them, however, you would most likely want tox to generate those virtual environments for you. For tox to use Python interpreters of multiple versions, they have to be installed on your machine. Even though this is possible, it may still be less optimal given that you will most likely need to make system changes (install a system package on Linux, use homebrew on MacOS, or download a Python app or an installer on Windows). Fortunately, tox can be run in a Docker container which will help to prevent cluttering your system. Running tests with tox in Docker: simple configuration To be able to run Python tests with tox in a Docker container, you will need a Dockerfile. FROM ubuntu:18.04 RUN apt-get -qq update RUN apt-get install -y --no-install-recommends \\ python3.7 python3.7-distutils python3.7-dev \\ python3.8 python3.8-distutils python3.8-dev \\ wget \\ ca-certificates RUN wget https://bootstrap.pypa.io/get-pip.py \\ && python3 get-pip.py pip == 19 .1.1 \\ && rm get-pip.py RUN python3.6 --version RUN python3.7 --version RUN python3.8 --version RUN pip3 install tox pytest The tox.ini file where you specify the Python environments. [tox] envlist = py36,py37,py38 skipsdist = True [testenv] deps = pytest commands = pytest The test_module.py containing a simple test function. def test_foo (): assert 2 + 3 == 5 Now you can build an image and then run the tests. $ docker build -t snake . $ docker run -it -v ${ PWD } /:/app snake /bin/sh -c 'cd app; tox' The pytest output will be printed for each of the Python environments in which the tests have been run (posted below with some sections removed for brevity). using tox . ini : / app / tox . ini ( pid 7 ) ... [ 16 ] / app$ / app / . tox / py36 / bin / pytest ================================================= test session starts ================================================= platform linux -- Python 3.6.9, pytest-6.1.2, py-1.9.0, pluggy-0.13.1 cachedir : . tox / py36 / . pytest_cache rootdir : / app collected 1 item test_module . py . [ 100 % ] ================================================== 1 passed in 0 . 01 s ================================================== ... [ 21 ] / app$ / app / . tox / py37 / bin / pytest ================================================= test session starts ================================================= platform linux -- Python 3.7.5, pytest-6.1.2, py-1.9.0, pluggy-0.13.1 cachedir : . tox / py37 / . pytest_cache rootdir : / app collected 1 item test_module . py . [ 100 % ] ================================================== 1 passed in 0 . 01 s ================================================== ... [ 26 ] / app$ / app / . tox / py38 / bin / pytest ================================================= test session starts ================================================= platform linux -- Python 3.8.0, pytest-6.1.2, py-1.9.0, pluggy-0.13.1 cachedir : . tox / py38 / . pytest_cache rootdir : / app collected 1 item test_module . py . [ 100 % ] ================================================== 1 passed in 0 . 01 s ================================================== _______________________________________________________ summary _______________________________________________________ py36 : commands succeeded py37 : commands succeeded py38 : commands succeeded congratulations :) Running tests with tox in Docker: advanced configuration For a more complex use case, for instance, when you are working on a library that depends on some 3rd Python package, say, pandas , you can specify which versions of pandas you'd like to test your project's code with. For the example below, your tests will be run in 6 different environments. The tox.ini configuration file. [tox] envlist = py36-pandas{112,113}, py37-pandas{112,113}, py38-pandas{112,113} skipsdist = True [testenv] deps = pandas112: pandas==1.1.2 pandas113: pandas==1.1.3 pytest commands = pytest The test_pandas.py containing a simple test function to create two data frames and compare them. import pandas as pd from pandas._testing import assert_frame_equal def test_pandas (): df1 = pd . DataFrame ({ 'a' : [ 1 , 2 ], 'b' : [ 3 , 4 ]}) df2 = pd . DataFrame ({ 'a' : [ 1 , 2 ], 'b' : [ 3 , 4 ]}) assert_frame_equal ( df1 , df2 ) The pytest output will be printed for each of the Python environments in which the tests have been run (posted below with some sections removed for brevity). [ 52 ] / app$ / app / . tox / py36 - pandas112 / bin / pytest =================================================================== test session starts =================================================================== platform linux -- Python 3.6.9, pytest-6.1.2, py-1.9.0, pluggy-0.13.1 cachedir : . tox / py36 - pandas112 / . pytest_cache rootdir : / app collected 2 items test_module . py . [ 50 % ] test_pandas . py . [ 100 % ] ==================================================================== 2 passed in 0 . 63 s ==================================================================== [ 73 ] / app$ / app / . tox / py36 - pandas113 / bin / pytest =================================================================== test session starts =================================================================== platform linux -- Python 3.6.9, pytest-6.1.2, py-1.9.0, pluggy-0.13.1 cachedir : . tox / py36 - pandas113 / . pytest_cache rootdir : / app collected 2 items test_module . py . [ 50 % ] test_pandas . py . [ 100 % ] ==================================================================== 2 passed in 0 . 47 s ==================================================================== [ 115 ] / app$ / app / . tox / py37 - pandas112 / bin / pytest =================================================================== test session starts =================================================================== platform linux -- Python 3.7.5, pytest-6.1.2, py-1.9.0, pluggy-0.13.1 cachedir : . tox / py37 - pandas112 / . pytest_cache rootdir : / app collected 2 items test_module . py . [ 50 % ] test_pandas . py . [ 100 % ] ==================================================================== 2 passed in 0 . 51 s ==================================================================== [ 133 ] / app$ / app / . tox / py37 - pandas113 / bin / pytest =================================================================== test session starts =================================================================== platform linux -- Python 3.7.5, pytest-6.1.2, py-1.9.0, pluggy-0.13.1 cachedir : . tox / py37 - pandas113 / . pytest_cache rootdir : / app collected 2 items test_module . py . [ 50 % ] test_pandas . py . [ 100 % ] ==================================================================== 2 passed in 0 . 42 s ==================================================================== [ 174 ] / app$ / app / . tox / py38 - pandas112 / bin / pytest =================================================================== test session starts =================================================================== platform linux -- Python 3.8.0, pytest-6.1.2, py-1.9.0, pluggy-0.13.1 cachedir : . tox / py38 - pandas112 / . pytest_cache rootdir : / app collected 2 items test_module . py . [ 50 % ] test_pandas . py . [ 100 % ] ==================================================================== 2 passed in 0 . 48 s ==================================================================== [ 193 ] / app$ / app / . tox / py38 - pandas113 / bin / pytest =================================================================== test session starts =================================================================== platform linux -- Python 3.8.0, pytest-6.1.2, py-1.9.0, pluggy-0.13.1 cachedir : . tox / py38 - pandas113 / . pytest_cache rootdir : / app collected 2 items test_module . py . [ 50 % ] test_pandas . py . [ 100 % ] ==================================================================== 2 passed in 0 . 38 s ==================================================================== _________________________________________________________________________ summary _________________________________________________________________________ py36 - pandas112 : commands succeeded py36 - pandas113 : commands succeeded py37 - pandas112 : commands succeeded py37 - pandas113 : commands succeeded py38 - pandas112 : commands succeeded py38 - pandas113 : commands succeeded congratulations :) There are quite a few resources online that go deeper into how one can use tox in a Docker container, but this simple layout has been very useful to me in various circumstances and may help others. Happy testing!","tags":"python","url":"/run-python-tests-with-tox-in-docker.html","loc":"/run-python-tests-with-tox-in-docker.html"},{"title":"Using Docker for Python development: cheat sheet","text":"The Docker framework can be an extremely useful tool for any Python developer who wants to run their Python programs (either for development or testing purposes) in a certain isolated environment with a pre-defined set of system and Python packages installed. Docker can also help with testing your Python code against multiple versions of the Python packages in multiple operating systems. The beauty of Docker is that you don't need to understand the intricate details of how Docker technology works to take advantage of it. This blog post contains recipes any Python developer can benefit from regardless how experienced you are with Docker and containerization techniques. Each recipe or scenario is based on a problem that one may need to solve and provides a solution in form of a Dockerfile file, a docker build , and a docker run command. Basic setup The base Docker image you'll be using will likely to be different depending on a number of factors, however, to keep the build time short, I'll be using the alpine image in most cases. Dockerfile contents: FROM python:3.7-alpine CMD [ \"python3\" , \"--version\" ] The build step contains the -t flag which defines the tag name of the image that will be built. The dot ( . ) tells Docker to use the the file named Dockerfile in the current directory. Building a Docker image: $ docker build -t snake . When an image is run, the CMD command found in the Dockerfile is executed. The --rm (remove) flag will make sure to delete the container once the run command is complete. Running a Docker image: $ docker run --rm snake # Python 3.7.9 Experiment with a Python REPL of any version FROM python:3.7.8-alpine CMD [ \"python3\" ] By changing the version, you can get into an interactive Python console for the given version. This is very handy when you want to test how a certain feature works in a newer or an older Python version. When a docker run command is executed, it will run the CMD command and exit, so you won't be able to interact with the REPL. To work with the container in an interactive mode, the -it flag should be used. $ docker run --rm -it snake # Python REPL becomes available Passing a command to a Python Docker image FROM python:3.7.8-alpine When running a Docker container, it's possible to pass a command, optionally, with additional arguments. Python provides support for running a command with the -c option so that it's possible to supply the program code as a string. This can be very handy when you need to have a one-liner for an operation that will return a value you may need later as input for the subsequent operations. $ docker run --rm -it snake python3 --version # Python 3.7.8 $ docker run --rm -it snake python3 -c \"import sys; print(sys.platform)\" # linux Run a Python program from the host in a Docker container FROM python:3.7.8-alpine It is possible to mount a local directory (on your disk) as a volume to a Docker container which is done with the -v parameter. Running the command below will make the app directory files available in the Docker container. This approach can be used when you want to run a Python program in a Docker container likely having a different system environment and Python version installed. Having this Python program (stored at app/main.py ): import sys print ( sys . version_info ) you can execute it with: $ docker run -v ${ PWD } /app:/app snake python3 /app/main.py # sys.version_info(major=3, minor=7, micro=8, releaselevel='final', serial=0) It is also possible to copy files to the Docker image when the image is being built if you don't want to mount any volumes at the run time. FROM python:3.7.8-alpine WORKDIR /opt/project/app COPY ./app CMD [ \"python3\" , \"main.py\" ] You can now run the Docker container to execute the main.py file that was copied: $ docker run --rm snake # sys.version_info(major=3, minor=7, micro=8, releaselevel='final', serial=0) Alternatively, you could also make the CMD command a part of the docker run : FROM python:3.7.8-alpine WORKDIR /opt/project/app COPY ./app Then you pass the arguments to the container from the shell instead: $ docker run --rm snake python3 main.py # sys.version_info(major=3, minor=7, micro=8, releaselevel='final', serial=0) Get into a Docker container shell and run a command FROM python:3.7.8-alpine WORKDIR /opt/project/app COPY ./app . It is possible to start a Docker container and run a shell console to execute arbitrary commands. This resembles connecting to a remote machine via an SSH connection or using a local Bash console. $ docker run --rm -it snake /bin/sh # /opt/project/app # ls # main.py # /opt/project/app # python3 main.py # sys.version_info(major=3, minor=7, micro=8, releaselevel='final', serial=0) Access files created in the Docker container on the host Accessing files that are created by processes run in a Docker container is possible by mounting a volume. FROM python:3.7.8-alpine WORKDIR /opt/project/app Now you can run the container in the interactive mode and attach a shell console: $ docker run -it -v ${ PWD } /app:/opt/project/app snake /bin/sh # > touch foo.bar The foo.bar file will appear on your host disk under the app directory. This makes it possible to create arbitrary files within your Docker container saving them on your host disk making them accessible for further usage locally. Copy files produced by Docker build command to the host In certain cases, a Dockerfile may create new files which can be accessed when starting the container. FROM alpine RUN echo \"some data\" > /home/data.out The trick here is to mount your host's directory to some other directory than container's home and then copy the file(s) you need to this intermediate location. $ docker run --rm -it -v ${ PWD } /datadir:/home/datadir snake # cp /home/data.out /home/datadir/ At this point, the data.out file should appear on your host's disk under the datadir directory in your current working directory. Run Python tests stored on your host inside a Docker container To do this, one would need to have a Docker container with the pip installed and the necessary Python packages that are required by your tests. It can be wise to install a tested version of pip to avoid getting the latest one in case it's broken (this has happened before a few times). FROM python:3.7.8-alpine RUN wget -q https://bootstrap.pypa.io/get-pip.py \\ && python3 get-pip.py pip == 19 .1.1 \\ && rm get-pip.py COPY app/requirements.txt ./ RUN pip install -q --no-cache-dir \\ -r requirements.txt \\ && rm requirements.txt Given that the requirements.txt contains pytest and the app folder has modules with test functions, you should be able to run pytest against the mounted directory: $ docker run --rm -v ${ PWD } /app:/opt/project/app snake pytest /opt/project/ -v # pytest output Because your current working directory is mounted as a volume, the files generated by the commands you run will be available on the host. This means you can run the coverage command in the Docker container and the .coverage file will be available on the host. Given this file, you'll be able to generate the HTML report to view in your host's web browser. Make host environment variables available in a Docker container It is common to use environment variables for storing some settings that Python programs may depend on. Keep in mind that a more robust strategy for storing sensitive information is to use Docker secrets . FROM python:3.7.8-alpine By default, host's environment variables (both permanently stored and temporarily exported) are not available in a Docker container. Given this Python program, import os print ( f 'Token: { os . getenv ( \"SITE_TOKEN\" ) } ' ) you can run it in a Docker container: $ export SITE_TOKEN = 'mytoken' $ docker build -t snake . $ docker run -v ${ PWD } /app:/opt/project/app snake python3 /opt/project/app/main.py # Token: None The -e ( --env ) parameter will let you specify the environment variables you want to propagate into the Docker container. $ export SITE_TOKEN = 'mytoken' $ docker run -e SITE_TOKEN = ${ SITE_TOKEN } -v ${ PWD } /app:/opt/project/app snake \\ python3 /opt/project/app/main.py # Token: mytoken Attach to a running Docker container When running a Python program in a Docker container in debugging mode, it can be useful to be able to pause the program and connect to the container to be able to inspect the file system. A few IDEs such as PyCharm and VSCode provide support for remote Python debugging and will be able to start a Docker container running a Python program and then later tell you its id. This is especially useful when the Python program is expected to produce some files and you would like to inspect them to verify the program produces correct results. If you know the container id, you can attach to it with: $ docker exec -it <container_id> /bin/bash If you don't know the container id, you will need to get it first which can be done with: $ docker ps --filter status = running The CONTAINER ID field will contain the id of the Docker container you will need to attach to. If you have multiple running containers, the one you need is likely to be the first one in the list. Happy containerization!","tags":"python","url":"/docker-for-python-cheat-sheet.html","loc":"/docker-for-python-cheat-sheet.html"},{"title":"Using pyfakefs for unit testing in Python","text":"Overview of unit testing When writing unit tests for programs, it is commonly considered to be a good practice to avoid relying on any part of the system infrastructure such as: network connectivity (you can't get data from a web service) operating system functionality (you can't call grep ) additional software installations (you can't rely on having Microsoft Excel installed) Another suggestion is to avoid making modifications to the files on disk. Testing pieces of code where files may be created or modified often involves patching the functions responsible for writing on disk such as the built-in open function, various os module functions such as os.mkdir and os.makedirs , and pathlib.Path methods such as Path.touch and Path.open . If writing to file system doesn't happen very often, using a few simple patches will suffice. However, for more heavy data-driven programs or programs that are written for any kind of data processing, patching endless number of function calls throughout the code can become rather tedious very soon. Using system temp directory At some point, it may be more efficient to use a more relaxed approach which involves using the tempfile module to create and modify files within the operating system temporary directory which is guaranteed to exist and be writable (at least on POSIX). This approach has some limitations: one wouldn't be able to make changes to files at system paths if this is an essential part of the program functionality unit tests writing on disk will become slower and with many of them can slow down the development-testing iterative cycle running tests in parallel (or using multithreading) can be unreliable as multiple tests may attempt to write/read to/from the very same files at the same time running a test making file system modifications can leave the system in a favourable state for the subsequent tests to be run which can lead to flaky tests Using virtual file system Alternatively, a more robust approach is to not write on disk and instead use a virtual, in-memory file system. For Python, there is a package called pyfakefs that makes it possible. Surprisingly it's not very well known in the Python community and I thought it would be helpful to share the word as I find this package to be indispensable in unit testing of programs which work heavily with files. The package can be used both with unittest and pytest frameworks and under any operating system. Here is a trivial example of writing a unit test for a function that merges content of all files within a given directory into a new file in the same directory. from pathlib import Path from pyfakefs.pytest_plugin import Patcher as FakeFileSystem from utils import merge_files def test_merge_files (): with FakeFileSystem () as _ : dest_dir = Path ( '/opt/data' ) dest_dir . mkdir ( parents = True ) dest_dir . joinpath ( 'file1' ) . write_text ( 'line1 \\n line2 \\n ' ) dest_dir . joinpath ( 'file2' ) . write_text ( 'line3 \\n line4 \\n ' ) merge_files ( source = dest_dir , target = 'result' ) assert dest_dir . joinpath ( 'result' ) . read_text () == 'line1 \\n line2 \\n line3 \\n line4 \\n ' Please refer to the pyfakefs documentation to learn more. Virtual file system caveats A few notes that can help to avoid common pitfalls: make sure not to construct Path objects outside of the patching context (the FakeFileSystem() in the example above) because it will otherwise be pointing to the real file system since the Path class has not been patched yet when using the fake file system for integration tests, keep in mind that you won't be able to use any external tools such as file or cp commands to interact with the fake file system files to verify that you are using the virtual, fake file system in your tests, you can choose to create files in a directory where you won't have modify permissions on your real file system – this will help you identify any cases where pyfakefs support is limited watch closely the permissions the user running the tests have as pyfakefs will operate under the root if run in a Docker container do not use the operating system temporary directory as the fake file system destination directory because pyfakefs doesn't patch the tempfile module Happy faking!","tags":"python","url":"/intro-pyfakefs-python-testing.html","loc":"/intro-pyfakefs-python-testing.html"},{"title":"Brief overview of the reproducible builds concept","text":"Introduction When working with the source code in a project that has multiple build steps (compiling, linking, patching, packaging) when a final \"product\" – a Debian package, an installable application, or an executable with shared libraries – is produced, there are many reasons why it can be useful to be able to get the same binary code (bit-by-bit) from the same source code. If you are able to build your project source code and then re-build it again later (without making any changes to the source code) and the produced artifacts are identical, it is said that your builds are reproducible/deterministic . How can one set up a reproducible build? According to the https://reproducible-builds.org definition: A build is reproducible if given the same source code, build environment and build instructions, any party can recreate bit-by-bit identical copies of all specified artifacts. For a simple project with a small number of movings parts, it may be relatively easy to achieve reproducible builds whereas for a corporate software development project this can be a challenge. There are multiple reasons why the binaries produced by a build operation may differ between builds run from the same source code. There are a few resources that will help you get started: An introduction to deterministic builds with C/C++ provides a gentle introduction to the concept and its importance and benefits Reproducible builds will help you learn more about software development practices around the reproducible builds. Elements of indeterminism Two most common issues are timestamps (when the source code is built) which may be saved into the produced binaries and path information (the location of the source code files on disk) which can also be included into the output binaries. However, many other things can have impact and make two binaries different (they may have the same size, but still be different when doing bit-by-bit comparison). Some of the things you will have control from the build system tools perspective such as compilers and linkers. For instance, you can control the order in which files are being processed as file systems generally do not make any promises that when you iterate the files in a given directory, they will be retrieved in the same order at all times. Other things may be defined in your custom post-processing logic – for instance, the order in which you set certain properties on a binary (such as RPATH patching) can also result in two different binaries. Sample project I have created a GitHub repository with the source code files that have been used in the Conan article An introduction to deterministic builds with C/C++ and it is available at reproducible-builds-example . This example project demonstrates the concept of reproducible builds with a few C++ source files and CMake build steps. A great tool that will help you compare the binaries in your effort to achieve reproducible builds is Diffoscope . It is extremely powerful and has functionality for generating HTML reports showing the difference between two objects you are comparing. This makes it so much easier to see why your binaries are different. Below is a screenshot of the HTML report that shows the difference between two executables. Happy diffing!","tags":"build-systems","url":"/intro-reproducible-builds.html","loc":"/intro-reproducible-builds.html"},{"title":"Using quicktype.io service to create Python interfaces from JSON","text":"Introduction For the last few years I had to write a few simple Python wrappers around a couple of external services. There are many advantages to having a nice Pythonic interface into libraries and tools that are written in other programming languages. It often makes it easier to automate, more pleasant to interact with, and faster to program in general. Bindings and wrappers Some wrappers simply expose the original interfaces without adding anything – these are plain bindings and this is often the case for C++ libraries that have Python bindings such Qt with PyQt . Python code you'd write using plain Python bindings may not feel very Pythonic (due to camelCase ) and because you often have to write programs using other, non-Pythonic, paradigms such as obj.setColor('Red') instead of obj.color = 'Red' . It is, in fact, not uncommon to write Python wrappers around Python bindings for C++ libraries simply because the Python bindings do not make Python developers who use them much more productive. Another group of Python wrapping effort exists around wrapping web services interaction to avoid dealing with cumbersome HTTP requests construction, response processing, and service communication. Likewise, wrapping a CLI tool in Python can be very useful if this is the only way to interact with the underlying software. Working with JSON No matter how you are getting back a JSON response – from a web service or from a CLI tool – you will need to process it to either present the result to the end user or to manage it in some other way. When dealing with JSON data, the built-in json module comes in handy and extracting the information you need out of a JSON object is trivial. You could also take advantage of higher level HTTP communication library such as requests . At the beginning, the code may look something like this: import requests data = requests . get ( 'https://s2k7tnzlhrpw.statuspage.io/api/v2/status.json' ) . json () status = data [ 'status' ][ 'description' ] updated_at = data [ 'page' ][ 'updated_at' ] print ( f \"Status: { status } \\n Updated at: { updated_at } \" ) Output: Status: All Systems Operational Updated at: 2020-08-12T08:08:25.828Z Interacting with the returned JSON objects using only the json module will suffice for smaller scripts and ad-hoc web service interrogation. If you'd like to build a Python wrapper around a large REST interface with many endpoints, however, it may be useful to think about having higher level abstractions for the data entities you deal with. The code snippet above has a number of issues: it relies on having the data elements present when using accessing JSON objects (you could work around it using the .get() method – data.get('status', {}).get('description', 'N/A') but it is still very fragile) as JSON objects keys are represented as strings, it's impossible to run any static type checker (and it has additional complications – refactoring becomes really hard) it makes it hard to reason about the data entities as their data type is not obvious (and you would have to provide a type hint for each JSON object such as status: Dict[str, str] = data['status'] which will become tedious very quickly) Representation of JSON as Python classes To make it easier to interact with JSON objects, they can be used to construct instances of Python classes which are much easier to work with: they provide nice abstraction, they are easy to write unit tests for, and the code that uses them can be inspected with a static analysis tool such as mypy . import requests from typing import Optional from datetime import datetime from dateutil import parser class Page : id : Optional [ str ] name : Optional [ str ] url : Optional [ str ] time_zone : Optional [ str ] updated_at : Optional [ datetime ] def __init__ ( self , id : Optional [ str ], name : Optional [ str ], url : Optional [ str ], time_zone : Optional [ str ], updated_at : Optional [ str ]): self . id = id self . name = name self . url = url self . time_zone = time_zone self . updated_at = parser . parse ( updated_at ) class Status : indicator : Optional [ str ] description : Optional [ str ] def __init__ ( self , indicator : Optional [ str ], description : Optional [ str ]): self . indicator = indicator self . description = description class System : page : Optional [ Page ] status : Optional [ Status ] def __init__ ( self , data ): self . page = Page ( ** data [ 'page' ]) self . status = Status ( ** data [ 'status' ]) data = requests . get ( 'https://s2k7tnzlhrpw.statuspage.io/api/v2/status.json' ) . json () system = System ( data ) status = system . status . description updated_at = system . page . updated_at . strftime ( ' %d /%m/%Y' ) print ( f \"Status: { status } \\n Updated at: { updated_at } \" ) Output: Status: All Systems Operational Updated at: 2020-08-12T08:08:25.828Z Having these classes will solve the issues that the original code snippet had. You can now extend the classes with more fields and add additional logic to any class – the Page class can have a local time zone property or the Status.description can be an instance of the StatusType(Enum) class, for instance. Autogeneration of Python classes from JSON It would be very useful if one could generate Python classes declarations from an API specification file. Swagger tools make it possible to generate an API specification which one could then convert into a collection of Python classes. This approach is very useful but the generated Python classes would be simply data classes without any logic – your fields with the date would be strings, not datetime objects. I think it works best for APIs that change often, during the development when you are iterating on the API design, or when having the raw data classes is sufficient. Another approach is to auto-generate a collection of Python classes from the API specification and extend their initialization logic and to add additional fields/methods as required. This approach has worked well for me and would be particularly useful for any internal tooling when you have control over the API changes. I found the QuickType.io – the service that can convert JSON into typesafe code in many languages including Python – to be really helpful. The classes declared in the snippet above have been generated by quicktype.io from JSON and then modified so that the root class System will have other class instances as its fields. That is, you just have to provide the root JSON object and the root class System will populate all its fields with respective classes as required. For this, a handy Python feature of unpacking keyword arguments with ** is used. This way, the quicktype.io service generates all the boilerplate Python code needed and then some additional modification can be done (e.g. to overload the __repr__ magic method to dump a JSON representation of the class instance). I think you will see the value of using a Python class to represent a JSON object very quickly and with the help of quicktype.io , autogeneration of Python data classes is incredibly easy. Happy automating!","tags":"python","url":"/quicktype-json-class-generation.html","loc":"/quicktype-json-class-generation.html"},{"title":"Building Python extension modules for C++ code with pybind11","text":"Introduction If you ever needed to provide interface to the C/C++ code from your Python modules, you may have used Python extension modules . They are typically created when there is an existing C++ project and it is required to make it accessible via Python bindings. Alternatively, when performance becomes critical, a certain part of the Python project can be written in C/C++ and made accessible to the rest of the Python codebase via some kind of interface. Quite a few large C++ libraries and frameworks have associated Python bindings – they can be used for prototyping or simply to speed up the development as writing a Python program is supposed to take less time than writing an equivalent C++ program. Exposing your library interface with another popular language, such as Python, will also make your project more accessible for programmers who are not very familiar with C++. Refer to excellent RealPython: Python Bindings: Calling C or C++ From Python article to learn more. Python bindings There are quite a few options on how you can make your C++ code accessible from Python. However, I have personally worked only with SWIG and pybind11 so far. For now, let's focus on pybind11 . It's extremely easy to set up on Linux or Windows and you should be able to create a compiled Python extension module ( .so for Linux and .pyd for Windows) very quickly. The pybind11 documentation does provide excellent reference information with a ton of examples. However, those examples often demonstrate features in isolation and I thought it would be useful to share an example of a more complete \"library\" where multiple examples are combined into something that looks like a MVP. Writing C++ code Here is the C++ file, Geometry.cpp , I've written to demonstrate the pybind11 features. It showcases constructing custom Point class instances, finding the distance between them in 2D and 3D space, and overloading C++ comparison operators among a few other things. #include <pybind11/pybind11.h> #include <pybind11/operators.h> #include <string> #include <sstream> #include <iomanip> #include <cmath> namespace py = pybind11 ; using namespace std ; class Point { public : Point ( const double & x , const double & y ) : x ( x ), y ( y ) { z = numeric_limits < double >:: quiet_NaN (); py :: print ( \"Constructing a point with z set to nan\" ); } Point ( const double & x , const double & y , const double & z ) : x ( x ), y ( y ), z ( z ) { py :: print ( \"Constructing a point with z set to a user given value\" ); } double x ; double y ; double z ; string shapeType = \"Point\" ; double distanceTo ( Point point , bool in3D ) { if ( in3D ) { if ( ! isnan ( z ) && ! isnan ( point . z )) { return sqrt ( pow (( point . x - x ), 2 ) + pow (( point . y - y ), 2 ) + pow (( point . z - z ), 2 )); } else { py :: print ( \"Cannot measure distance between points in XY and XYZ space\" ); return numeric_limits < double >:: quiet_NaN (); } } else { return sqrt ( pow (( point . x - x ), 2 ) + pow (( point . y - y ), 2 )); } } bool static areEqual ( Point left , Point right ) { if ( isnan ( left . z ) && isnan ( right . z )) { return left . y == right . y && left . x == right . x ; } else if ( isnan ( left . z ) &#94; isnan ( right . z )) { return false ; } else { return left . y == right . y && left . x == right . x && left . z == right . z ; } } friend bool operator == ( const Point & left , const Point & right ) { return areEqual ( left , right ); } friend bool operator != ( const Point & left , const Point & right ) { return ! areEqual ( left , right ); } bool is3D () const { return ! isnan ( z ); } }; PYBIND11_MODULE ( Geometry , m ) { m . doc () = \"C++ toy geometry library\" ; py :: class_ < Point > ( m , \"Point\" , \"Point shape class implementation\" ) . def ( py :: init < const double & , const double &> (), py :: arg ( \"x\" ), py :: arg ( \"y\" )) . def ( py :: init < const double & , const double & , const double &> (), py :: arg ( \"x\" ), py :: arg ( \"y\" ), py :: arg ( \"z\" )) . def_readonly ( \"x\" , & Point :: x ) . def_readonly ( \"y\" , & Point :: y ) . def_readonly ( \"z\" , & Point :: z ) . def_readonly ( \"shapeType\" , & Point :: shapeType ) . def ( \"distanceTo\" , & Point :: distanceTo , py :: arg ( \"point\" ), py :: arg ( \"in3D\" ) = false , \"Distance to another point\" ) . def ( \"is3D\" , & Point :: is3D , \"Whether a point has a valid z coordinate\" ) . def ( py :: self == py :: self ) . def ( py :: self != py :: self ) . def ( \"__repr__\" , []( const Point & point ) { stringstream xAsString , yAsString , zAsString ; xAsString << std :: setprecision ( 17 ) << point . x ; yAsString << std :: setprecision ( 17 ) << point . y ; zAsString << std :: setprecision ( 17 ) << point . z ; if ( point . z != 0 ) { return \"Point (\" + xAsString . str () + \", \" + yAsString . str () + \", \" + zAsString . str () + \")\" ; } else { return \"Point (\" + xAsString . str () + \", \" + yAsString . str () + \")\" ; } }); } Building a Python extension module Once you have the Geometry.cpp file on disk and pybind11 installed, you should be able to compile the C++ code and link it to the Python headers: $ c++ -O3 -Wall -shared -std = c++11 -fPIC ` python3 -m pybind11 --includes ` Geometry.cpp -o Geometry ` python3-config --extension-suffix ` If you are not very familiar with Bash, command in the backticks in the command above are evaluated by the shell before the main command. The python3 -m pybind11 --includes part is used to get the location of Python header files and the python3-config --extension-suffix part is used to get the suffix for the shared library name – for CPython 3.6 on a 64bit Ubuntu, the Geometry.cpython-36m-x86_64-linux-gnu.so file will be created. Now, once you have the shared library file, it can be imported and used pretty much as if it was a regular Python module. Using a Python extension module Let's see our library in action by running the file python3 use_geometry.py containing the code below: import math from Geometry import Point # runtime dispatch of init constructors print ( Point . __init__ . __doc__ ) # a method signature and its docstring print ( Point . distanceTo . __doc__ ) p1 = Point ( 10 , 20 ) p2 = Point ( 20 , 30 ) p3 = Point ( 20 , 30 ) p4 = Point ( 50 , 60 , 45.67 ) p5 = Point ( 50 , 60 , 45.67 ) assert p1 . distanceTo ( p2 ) == math . sqrt ( 200 ) # check operator overloading works assert p1 != p2 assert p2 == p3 assert not p2 != p3 assert p4 == p5 assert not p4 == p3 assert math . isnan ( p1 . z ) # check distance between 3D points p1 = Point ( 50 , 60 , 45 ) p2 = Point ( 50 , 60 , 75 ) print ( p1 . distanceTo ( p2 , in3D = True )) # check __repr__ print ( p1 ) print ( p2 ) The produced output: __init__(*args, **kwargs) Overloaded function. 1. __init__(self: Geometry.Point, x: float, y: float) -> None 2. __init__(self: Geometry.Point, x: float, y: float, z: float) -> None distanceTo(self: Geometry.Point, point: Geometry.Point, in3D: bool = False) -> float Distance to another point Constructing a point with z set to nan Constructing a point with z set to nan Constructing a point with z set to nan Constructing a point with z set to a user given value Constructing a point with z set to a user given value Constructing a point with z set to a user given value Constructing a point with z set to a user given value Distance between Point (50, 60, 45) and Point (50, 60, 75) is 30.0 Representation of the p1 is \"Point (50, 60, 45)\" Representation of the p2 is \"Point (50, 60, 75)\" This is of course a very trivial example of pybind11 usage, however there have been successful attempts to use pybind11 for binding existing large C++ libraries such as CGAL and Point Cloud Library . See an example of wrapping some of the CGAL functionality with pybind11 and Python bindings for the Point Cloud Library to learn more. Happy binding!","tags":"python","url":"/pybind11-python-bindings.html","loc":"/pybind11-python-bindings.html"},{"title":"What I think a great tech support should look like","text":"Being a software engineer implies that you may be interacting with customers particularly if you are working in a small company. If you are wearing multiple hats or if you are in professional services or tech support, you will likely be contacted by your customers about issues they may experience using your product – a web site, a desktop or a mobile app, or some hardware equipment. Whatever it is, I believe it's best to have a clear plan of actions outlined which you can follow when addressing a customer's issue. No matter how fine-grained your protocol of communication with the customer already is, you can always incorporate some of the ideas I have about how I'd like to work with a customer that needs help. I do understand that it may be difficult or unrealistic to follow the steps below exactly, but this is my vision of a great support. Customer gets in touch Customer reaches out to you because some functionality on the web site of a business system your team built doesn't work as they expect. Pace yourself Do not attempt to provide any solution or ask any questions just yet. If your guess will be accurate – you ask to enable JavaScript in a web browser or to log out and log back in – the customer is less likely to share any details with you because they can now continue working. You, on the other hand, are missing a chance to document the incident to see if it would be possible to prevent it from happening again and to share it with your team members. What was the version of the web browser where the JavaScript is not enabled by default? Did they really have to log out and log back in or a simple refresh of a web page would be enough (you may tweak server-side caching settings)? You may never figure this out and even though the problem is \"solved\" and the customer is happy, in the long run this was a loss. Show empathy No matter what channel of communication is being used – a phone call or an email – the first thing you do is to show some empathy: being an engineer, you know better than anyone how frustrating it can be when a computer doesn't do what you want it to do. Collect information You can start collecting the information you will need to troubleshoot now; don't ask for information you can collect yourself, but sometimes it may still be useful – even if you can SSH into a server to get the version of a software installed, it would still be very useful to ask the customer to tell you the version they see on the web page as an older version may indicate a web page caching issue. You would ideally have a pre-defined template document where you can fill all the information you may need so that you don't have think about it during a phone call. Share information Once you have gathered all the information, make sure to share it with the customer. If on the phone, read it back to them. If it's an email conversation, either share the complete document or provide access to the internal support system (if any) where they would be able to check that the information they've shared with you is accurate. This will help to avoid any misunderstandings and provide traceability as the customer will acknowledge that the information they have provided is correct. Doing troubleshooting Depending on how urgent the request is, you may or may not have time to do some manual production inspection before telling the customer how to fix their problem. If they are in a middle of a presentation to a board of directors, it may be best to tell them how to fix the issue immediately. If it is not time critical, you may want to ask for some time to log in into the production environment to record as much as information as you possibly can. For instance, they told you that they are still able to use the system despite not being logged in. Instead of telling them to log off and log in hoping it will fix the problem (did you know that hope is not a strategy ?), you may want to log in into the server to find out what's going on with your authentication service while the user hasn't left their web browser session (provided you don't do any verbose logging of this type of events already). This issue may indicate some serious problem that is worth investigating further because it may manifest itself again at some point. Update on the progress If the problem requires more time and you will likely need to spend hours if not days working on it, it's best to let the customer know about the progress. They would be able to find it very helpful to see that you are working on their issue and ideally how much time you've spent (a support ticket can be \"in progress\" for 5 days, but the engineer may have been working on it for just 1 hour). Provide a temporary solution if applicable If it's an option, make sure to provide customer with a workaround to let them continue to do their business. If they need to process some files, offer to do it manually for them if possible. If their business operation depends on a feature from their \"basic\" plan that doesn't work and you know that an alternative feature from the \"advanced\" plan would work, upgrade their account for some time while you are troubleshooting. Tell about proposed solution Once you have identified the issue and have been able to solve it, tell the user what was happening and what you have done to solve it. Adjust the language depending on how technical things get as required but don't be afraid to offer them a chance to learn; many customers would find it helpful to understand how the product they use operate under the hood. Ask them to verify that the solution you've implemented works for them (after you have done everything you possibly could to verify this yourself first). Document your findings After you found the resolution to a problem, make sure to document not only what has to be done to fix it, but also what have you attempted to get done which didn't seem to help. For instance, you thought that the problem may be due to a broken database table index and have decided to re-build it. That didn't help and then you think that perhaps recalculating the table statistics may help. You do that now and, yes, the problem is gone. However, documenting that for this problem recalculating the table statistics is necessary may be misleading as re-building the table index may also be required. When you or a colleague of yours will be reading the incident documentation, they will know what have you attempted before finding the solution. Wherever possible, any changes to any environment should be happening via code or a terminal to make it easy to record as making changes in the GUIs are generally known to be very hard to document. A problem that can be solved purely by customers themselves (invalidating the web browser cache or to change some setting within the user interface of the business system itself) is a great candidate to be added into the user documentation. Happy supporting!","tags":"tech-support","url":"/my-version-of-a-great-tech-support.html","loc":"/my-version-of-a-great-tech-support.html"},{"title":"Find and fix Python security issues with QL","text":"This is one of the few posts I wrote in 2019 when working for Semmle (later acquired by GitHub) that was originally published on the Semmle blog that was transformed. Python library for QL has changed quite a bit since then, however, many principles are still relevant and helpful for anyone who would want to learn more about QL. See CodeQL for Python to learn more. Overview In this blog post, we'll take a look at some security concerns that are particularly relevant to Python developers. There are already queries for some of these issues, and we'll write new custom queries for the others. You can execute any query in this post against your own Python project. When you're writing code, it is very easy to accidentally introduce errors or vulnerabilities. On top of that, you need to be aware of any existing bugs in the implementation of the language you're working in, which adds an additional burden. For instance, CPython developers may need to review security vulnerabilities present in the Python version they use. Running static analysis on the source code can help you find code that would produce an incorrect result, open up hardware or software resources for malicious use, or cause a program to unexpectedly fail. Fixing those issues will make the program more secure. To learn about the Python security model, bytecode safety, and some typical security concerns, visit the Python Security resource which has an excellent set of reference resources and further readings. Unfortunately, for anyone who is maintaining a legacy Python 2 codebase, and pre-2.7 versions in particular, quite a few bugs and some security issues have been addressed only in Python 3. So upgrading the code to the latest version of Python 3 is very often the only option if you want to keep your code secure. Although Python 3 is more secure than Python 2, you still can't fully relax because it also suffers from security vulnerabilities, even the most recent versions, such as Python 3.6, 3.7, and 3.8. You can review the current security-related issues using the Python bug tracker . On this website, you will find many bugs which have a CVE number assigned such as CVE-2018-1000030 listed as CVE-2018-1000030: Python 2.7 readahead feature of file objects is not thread safe or CVE-2013-4238 listed as CVE-2013-4238: SSL module fails to handle NULL bytes inside subjectAltNames general names to mention just a few. Here we categorize Python security concerns into two groups: Issues in the Python interpreter or standard library written by Python core developers and contributors Issues in Python user code written by developers writing Python programs. Issues in CPython source code If you are a developer writing your programs in Python, you have very little control over the source code of CPython. You could, of course, make the necessary changes to the source code and compile your own Python interpreter, however, this is something that only a few developers would find practical. As an example, the urllib module didn't parse passwords containing the # character correctly . This bug was fixed in the most recent version of Python 3 and also backported to previous versions. However, there are a few bugs that were fixed only in certain versions of Python and were not backported. For example, the Hash function is not randomized properly bug was fixed only in Python 3.4.0. This means that previous versions, such as Python 3.3 and Python 2.7, are still vulnerable. This puts some developers into a difficult situation if they cannot upgrade to the latest Python interpreter to take advantage of the latest security related fixes. Semmle's continuous security analysis service, LGTM.com , includes the CPython project, analyzing both the C and Python source code. If you develop security-sensitive applications, you should review the security-related alerts that are highlighted in the latest code. For example, the following alerts were found by queries that focus on potential vulnerabilities: CPython's alert page on LGTM.com. Issues in your own Python programs In contrast, when you write your own Python programs, often it's the choices that you make as you implement features that determine the security of your program. In the rest of this post, we look at some of the issues that can make your programs less secure, and provide guidelines on how avoid these common pitfalls. We will also share built-in queries and custom queries that you can use to find security-related issues in your code. Inadequate DSA and RSA key length The paper Transitioning the Use of Cryptographic Algorithms and Key Lengths published by the NIST Computer Security Resource Center , suggests using a key of size 2048 or larger for RSA and DSA algorithms. The Python cryptography package provides tools for working with private keys and has a user key_size parameter. See the Python code snippet in the docs for details. From the docs page : key_size (int) – The length of the modulus in bits. It should be either 1024, 2048 or 3072. For keys generated in 2015 this should be at least 2048. Note that some applications (such as SSH) have not yet gained support for larger key sizes specified in FIPS 186-3 and are still restricted to only the 1024-bit keys specified in FIPS 186-2. There is a built-in query, Use of weak cryptographic key , that highlights when values smaller than 2048 are passed to the key_size parameter. For example, the query would report an alert for this Python code: from cryptography.hazmat.primitives.asymmetric import dsa from cryptography.hazmat.backends import default_backend private_key = dsa . generate_private_key ( key_size = 512 , backend = default_backend () ) The query also identifies inadequate key lengths in code that uses the Crypto and Cryptodome Python packages. You can set a different minimum key length by editing the query and changing the result of the minimumSecureKeySize predicate, which is currently set to 2048 for both the DSA and RSA algorithms: int minimumSecureKeySize(string algo) { algo = \"DSA\" and result = 2048 or algo = \"RSA\" and result = 2048 or algo = \"ECC\" and result = 224 } Using the deprecated ‘pyCrypto' package PyCrypto is a mature Python cryptography toolkit that has gained popularity over the years. However, the package has quite a few issues, some of them affecting security, and the project was last updated over five years ago. One of those issues, AES.new with invalid parameter crashes python , is actually an exploitable vulnerability, CVE-2013-7459 . The current recommendation is to use some other Python package. For instance, cryptography , is a popular choice for many Python developers: * paramiko , one of the popular native Python SSHv2 protocol libraries, has switched to cryptography from pyCrypto ; see this pull request for details. twisted , a popular event-driven networking engine, has switched to cryptography from pyCrypto as well; see this pull request for details. To check if there are any places where the pyCrypto package is imported and used, as in this Python code snippet: from Crypto.Hash import SHA256 val = SHA256 . new ( 'abc' . encode ( 'utf-8' )) . hexdigest () we could write the following custom query: /** * @name Using a deprecated pyCrypto package * @description Using an unmaintained tool kit with multiple security issues makes your code vulnerable to attack . * @kind problem * @tags security * @problem.severity error * @id py / using - insecure - pycrypto - package */ import python from ImportExpr imp , Stmt s , Expr e , string moduleName where moduleName = imp . getName () and s . getASubExpression () = e and ( e = imp or e . contains ( imp )) and ( moduleName . matches ( \"Crypto\" ) or moduleName . matches ( \"Crypto.%\" )) select imp , \"pyCrypto package has multiple security issues\" If your project is on LGTM.com, you can set up automated code review and add this query to your repository to ensure that you never accidentally introduce uses of the pyCrypto package. Binding to all IP addresses with the ‘socket' module When you're using the built-in socket module (for instance, to build a message sender service), it's possible to bind to all available IPv4 addresses by specifying 0.0.0.0 as the IP address. When you do this, you essentially allow the service to accept connections from any IPv4 address provided that it is capable of reaching it through routing. Note that an empty string '' has the same effect as 0.0.0.0 . Opening up your end point to all network interfaces is considered to be insecure. For example: import socket s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) s . bind (( '0.0.0.0' , 6080 )) s . bind (( '192.168.0.1' , 4040 )) s . bind (( '' , 8888 )) From the Python socket documentation : A pair (host, port) is used for the AF_INET address family, where host is a string representing either a hostname in Internet domain notation like ‘daring.cwi.nl' or an IPv4 address like ‘100.50.200.5', and port is an integer. The following custom query would find these insecure bindings: /** * @name Binding a socket to all network interfaces * @description Binding a socket to all interfaces would open up traffic from any IPv4 address * and is therefore associated with security risks . * @kind problem * @tags security * @problem.severity error * @id py / bind - socket - all - network - interfaces */ import python Value aSocket () { result . getClass () = Value :: named ( \"socket.socket\" ) } CallNode socketBindCall () { result = aSocket () . attr ( \"bind\" ) . ( CallableValue ) . getACall () } string allInterfaces () { result = \"0.0.0.0\" or result = \"\" } from CallNode call , string address where call = socketBindCall () and address = call . getArg ( 0 ) . getNode () . ( Tuple ) . getElt ( 0 ) . ( StrConst ) . getText () and address = allInterfaces () select call . getNode (), \"'\" + address + \"' binds a socket to all interfaces.\" Using insecure SSL versions There have been quite a few security changes in Python 3's built-in ssl module. This is particularly true for versions 3.6 and 3.7. Visit Python SSL and TLS security to learn about evolution of the ssl module. SSL versions 2 are 3 are now considered to be insecure and official Python documentation discourages their use. Since Python 3.6, many protocol versions such as ssl.PROTOCOL_SSLv23 and ssl.PROTOCOL_SSLv2 , are deprecated and OpenSSL has removed support for SSLv2 . From Python 3.6 onward, it is best to use the ssl.PROTOCOL_TLS protocol. From the docs page : ssl.PROTOCOL_TLS : Selects the highest protocol version that both the client and server support. Although you can specify the SSL version in an ssl.wrap_socket call, this was deprecated in version 3.7. Instead, the use of a more secure alternative is suggested by the Python docs : Since Python 3.2 and 2.7.9, it is recommended to use the SSLContext.wrap_socket() instead of wrap_socket() . The top-level function is limited and creates an insecure client socket without server name indication or hostname matching. There is a built-in query, Default version of SSL/TLS may be insecure , which finds uses of SSLContext.wrap_socket() . For earlier versions of Python, you want to make sure that you're not using insecure versions of SSL such as ssl.PROTOCOL_SSLv2 or ssl.PROTOCOL_SSLv3 . For this, there is another built-in query, Use of insecure SSL/TLS version , which finds insecure SSL/TLS versions both for pyOpenSSL.SSL (a Python wrapper around the OpenSSL library) and for the built-in ssl module. Not validating certificates in HTTPS connections During an HTTPS request, it is important to verify SSL certificates, which is exactly what any modern web browser does nowadays. Up to versions Python 2.7.9 (for Python 2) and Python 3.4.3 (for Python 3), CPython modules that dealt with HTTP interaction (such as httplib and urllib ) did not verify the web site certificate against a trust store. This issue was registered as CVE-2014-9365 and is an example of CWE-295: Improper Certificate Validation which can potentially lead to a man-in-the-middle (MITM) attack . A de-facto standard library used by the Python community for communicating over HTTP is requests . By default, it has SSL verification enabled , and a custom exception will be thrown if certificate verification fails. However, it is possible to disable the verification that TLS provides: import requests requests . get ( 'https://example.com' , verify = False ) To find HTTP requests that fail to verify the certificate, you can run the built-in query, Request without certificate validation . Compromising privacy in universally unique identifiers Universally unique identifiers (UUID) can be generated using the uuid module. The general recommendation is to use uuid1() or uuid4() to generate a unique identifier. However, uuid1() may compromise privacy because the UUID will include the computer's network address. uuid4() , in contrast, creates a random UUID and is simply a convenience function. From the CPython source code : def uuid4 (): \"\"\"Generate a random UUID.\"\"\" return UUID ( bytes = os . urandom ( 16 ), version = 4 ) Furthermore, there are some concerns about the \"safety\" of UUIDs. From the Python docs : Depending on support from the underlying platform, uuid1() may or may not return a \"safe\" UUID. A safe UUID is one which is generated using synchronization methods that ensure no two processes can obtain the same UUID. To find version 1 UUIDs generated by uuid.UUID(bytes=values, version=1) or uuid.uuid1() , as in the code snippet below, import os import uuid id1 = uuid . uuid1 () id2 = uuid . UUID ( bytes = os . urandom ( 16 ), version = 1 ) id3 = uuid . UUID ( None , b '1234567891234567' , None , None , None , 1 ) we can run the following custom query: /** * @name Using a uuid1 for generating UUID * @description uuid1 will use machine 's network address for generating UUID and may compromise privacy . * @kind problem * @tags security * @problem.severity error * @id py / using - uuid1 - for - UUID */ import python from CallNode call where call = Value :: named ( \"uuid.uuid1\" ) . getACall () or call = Value :: named ( \"uuid.UUID\" ) . getACall () and ( call . getArgByName ( \"version\" ) . getNode () . ( IntegerLiteral ) . getValue () = 1 or call . getArg ( 5 ) . getNode () . ( IntegerLiteral ) . getValue () = 1 ) select call , \"uuid1 will use machine's network address and may compromise privacy.\" Use of ‘assert' statements to control program flow The assert statement can be used in Python to indicate when executing the code would result in program failure or the retrieval of incorrect results. It is very common to use assert in unit and integration tests. However, assert statements are disabled when you run a Python program with optimization enabled . Running python -O program.py means that assert statements are ignored which may give a certain performance boost (either significant or negligible depending on how time-consuming the assert statements are). This means that it can be unwise to rely on assert statements to define the logic of a program execution flow, if you plan to run your Python programs with optimization enabled or the code may be run outside of your control. Moreover, use of assert statements can be associated with security risks. Consider this Python code snippet: def get_customers ( user ): \"\"\"Get list of customers.\"\"\" assert is_superuser ( user ), \"User is not a member of superuser group\" return db . lookup ( 'customers' ) When this program is run in optimized mode, the assert statement will be ignored and any user would be able to get a list of customers, regardless of whether they are a member of the superuser group or not. This code can be rewritten more securely, without assert statements, as: def get_customers ( user ): \"\"\"Get list of customers.\"\"\" if not is_superuser ( user ): raise PermissionError ( \"User is not a member of superuser group\" ) return db . lookup ( 'customers' ) Writing a custom query that catches all assert statements is trivial, however, all legitimate uses of assert would also be caught so you would need to look through each result manually. Optionally, you could search for assert statements used outside of tests. This query searches for all is_superuser function calls within the assert statements. import python from AstNode ast , Assert assert where assert . contains ( ast ) and ast . ( Call ) . getFunc () . ( Name ) . getId () = \"is_superuser\" select ast Parsing external files content into Python objects Python provides multiple ways to read external files and load their content into Python objects. There are exec and eval built-in functions along with pickle (or cPickle in Python 2). External packages such as PyYAML can also be used to parse YAML file contents. Because data from external sources may not be secure, the general security guidelines are that you should never unpickle or load by parsing any data received from an untrusted source. There is a built-in query, Deserializing untrusted input , that highlights code that may be a security concern when unpickling and other deserialization happens. The general recommendation is to avoid constructing arbitrary Python objects via pickle or via a pyYAML package if the data comes from an untrusted source (the internet in particular). PyYAML , however, has the safe_load function which limits what can be loaded to simple Python objects. In this Python code snippet, a class instance is created based on the YAML file contents (posted here as a string in yaml.load for brevity): import yaml class PasswordReader ( object ): def __init__ ( self , path ): self . path = path def read ( self ): with open ( self . path ) as fh : return fh . readlines () def __repr__ ( self ): return f \"PasswordReader: { self . path } \" obj = yaml . load ( \"\"\" !!python/object:__main__.PasswordReader path: /etc/passwd \"\"\" ) print ( obj ) Using yaml.safe_load would block construction of the class instance object unless it has been marked as safe. To be considered safe, it should inherit from yaml.YAMLObject and have a property yaml_loader set to yaml.SafeLoader . This custom query was written to find unsafe yaml.load calls in your codebase: /** * @name Using insecure yaml . load function * @description yaml . load function may be unsafe when loading data from untrusted sources * @kind problem * @tags security * @problem.severity error * @id py / using - yaml - load */ import python from CallNode call where call = Value :: named ( \"yaml.load\" ) . getACall () select call . getNode (), \"yaml.load function may be unsafe when loading data from untrusted sources. Use yaml.safe_load instead. \" This type of custom query, where you search for a specific function call, is fairly common. This approach can be used for any language feature that was considered safe a few years ago but the current recommendation is to use a newer version or an alternative, more robust one. For examples of how you can write your own queries to find the use of a certain function or import of a module, review the following built-in queries: Deprecated slice method Import of deprecated module Use of exit() or quit() In most cases, you want to make sure that the older or less secure function could not be used in the new code being written. The power of writing your own custom queries, however, lies in the ability to go beyond built-in queries and to look for the functions or class methods that you decide to blacklist. You can write a new query to trigger an alert if a blacklisted function is found. LGTM.com provides automatic code review functionality to prevent bugs from ever making it to your project. If you add custom queries to your repository, then you'll also get alerts if a pull request contains functions or class methods that you've blacklisted. References Here are some references to Python security resources you may find useful. National Vulnerability Database MITRE CWE database Python bug tracker Python vulnerability statistics OpenStack Secure Development Guidelines Security related built-in Python queries Bandit - Python security tool Python security overview Python security vulnerabilities","tags":"QL","url":"/find-fix-python-security-issues-with-ql.html","loc":"/find-fix-python-security-issues-with-ql.html"},{"title":"Finding Python code compatibility issues with QL","text":"This is one of the few posts I wrote in 2019 when working for Semmle (later acquired by GitHub) that was originally published on the Semmle blog that was transformed. Python library for QL has changed quite a bit since then, however, many principles are still relevant and helpful for anyone who would want to learn more about QL. See CodeQL for Python to learn more. Overview In this tutorial, you'll learn how to use QL to query a Python codebase and learn how to check for Python 2/3 compatibility. We'll be writing alert queries , that is, queries that highlight issues in specific locations in your code. The tutorial assumes that you're familiar with the basics of QL for Python. If not, you might want to read my previous post ( Introducing the QL libraries for Python ). Python 2 and 3 As the official end of life of Python 2 approaches, more and more Python projects are being converted from Python 2 to Python 3. The majority of infrastructure projects are now on Python 3, and many are Python 3 only. At some point, you will likely need to upgrade your project. There are myriads of useful resources that can help you upgrade your project's codebase. There are tools that can upgrade code in a semi-automatic fashion; there are linters and static code analysis tools that will help you spot code that's not compatible with Python 3. There are also quite a few documents to help you learn what's new in Python 3 and avoid the common pitfalls when you upgrade. To learn more, visit the main What's New In Python 3.0 reference page. To learn how to write code that's compatible with both Python 2 and Python 3, visit Python-Future . Upgrading a codebase to Python 3, or supporting both Python 2 and 3, can be a challenge. The Python 2 interpreter reports a SyntaxError for some of the new syntax features in Python 3. Some Python 2 features aren't available in Python 3, so when the Python 3 interpreter encounters them it raises a runtime error or gives a different result. For instance, the print statement was replaced by the print() function so running a module with a print statement under Python 3 will cause a SyntaxError . Using the print statement as if it were a function in Python 2, however, won't raise a SyntaxError , but its behavior will be different: Python 3 >>> print ( \"value1\" , \"value2\" ) value1 value2 Python 2 >>> print ( \"value1\" , \"value2\" ) ( 'value1' , 'value2' ) In contrast, the long type was removed in Python 3 leaving only one built-in integer type named int . Hence, trying to use the long keyword in a module executed by a Python 3 interpreter, will cause a NameError at runtime: Python 3 >>> isinstance ( 5 , int ) True >>> isinstance ( 5 , long ) Traceback ( most recent call last ): File \"<input>\" , line 1 , in < module > NameError : name 'long' is not defined Using QL A series of QL queries is shown below, highlighting some of the issues found when working with Python 2 and 3 compatibility. We explain how the queries work so you can learn how to use the QL libraries for Python, which will help you to write your own custom queries. A Python project can be analyzed using either a Python 2 or a Python 3 interpreter. To learn more, read How is the Python version identified? on LGTM.com. Analysis run on LGTM will spot common errors using built-in queries. To find out which version of Python was used to analyze a codebase, you can use the built-in major_version and minor_version predicates: import python select major_version (), minor_version () These predicates will come in handy later on when we will be trying to find issues in the code that are relevant only for Python 2 or for Python 3. Built-in queries Syntax error Syntax errors are found by the built-in Syntax error query. They prevent a module being evaluated and thus imported. An attempt to import a module with invalid syntax will fail; a SyntaxError will be raised. Syntax errors are caused by invalid Python syntax, for example: # variables cannot contain any symbol # other than a digit, a letter, and an underscore variable $ = \"value\" # attempt to use an invalid increment operator value = 10 value ++ # incorrect usage of lambda print ( lambda x : x += 10 ) # invalid inequality test print ( source <> target ) Note that in Python 2, it's okay to mix tabs and spaces for code indentation. However, in Python 3, a new TabError is raised when indentation contains an inconsistent use of tabs and spaces. This type of error is also caught by the syntax errors check. Encoding error Encoding errors are found by the built-in Encoding error query. They prevent a module being evaluated and thus imported. An attempt to import a module with an invalid encoding will fail; a SyntaxError will be raised. Note that in Python 2, the default encoding is ASCII. Existing custom queries In addition to the built-in queries that are part of the core LGTM suite, there are a few custom queries that the community of QL writers has contributed. I myself wrote a new custom query shortly after I joined Semmle while I was learning how the QL libraries for Python worked. This query, Use of 'return' or 'yield' outside a function , was first published in the public GitHub QL repository and later became a built-in query that is run on LGTM.com . Writing new QL queries New ‘raise from' syntax PEP 3109 – Raising Exceptions in Python 3000 and PEP 3134 – Exception Chaining and Embedded Tracebacks introduced new syntax for the raise statement: raise [expr [from expr]] . The optional from clause can be used to chain exceptions. When from is used, the second expression must be another exception class or instance. To learn more, visit The raise statement . Since version 3.3, you can use None to suppress the chained exception, for example: try : value = 1 / 0 except Exception : raise Exception () from None However, a project style guide may discourage the suppression of exception chaining using from None , for example, to maintain backwards compatibility. If this is the case, you would want to find all such occurrences. The QL libraries for Python contain classes that are useful for finding this syntax. These can be imported and used in a custom QL query. The easiest way to find this type of raise statement is to use the Raise class. Using this QL query, we can spot when the raise from None syntax is used: import python from Raise r where r . getCause () . getAFlowNode () . pointsTo ( Value :: named ( \"None\" )) select r The .getCause() method gives us the cause of the raise statement and it's possible to find out what object this cause points to using the .pointsTo() method. In this case, we test whether this is a None object. To extend our query, we could check whether a valid object is being used in the from part. The object can be either None or a valid exception class or instance. For example, this raise statement has an invalid object so, a TypeError with the message, TypeError: exception causes must derive from BaseException , is raised when it's run: try : print ( 1 / 0 ) except Exception as exc : raise RuntimeError ( \"Something happened\" ) from \"Program stopped\" This QL query will find all raise ... from ... statements where the from object is invalid. import python from Raise r , Value v where r . getCause () . getAFlowNode () . pointsTo ( v ) and v != Value :: named ( \"None\" ) and not v . getClass () . getASuperType () = Value :: named ( \"BaseException\" ) select r , v A class instance is a legal exception type if it inherits from the BaseException class. Thus, this query would be able to spot when an invalid object type is used in the raise from clause. Support for unicode in identifier names In Python 2, only ASCII characters could be used in the names of Python identifiers including, but not limited to, variables, functions, and classes. Trying to define a variable café (e-acute) in Python 2, would result in a SyntaxError: invalid syntax . In Python 3, with PEP 3131 – Supporting Non-ASCII Identifiers , this limitation was removed and now additional characters from outside the ASCII range (see the docs ) could be used in identifier names. This code is valid in Python 3: café = object () print ( café ) However, a project style guide may prohibit the use of non-ASCII characters in identifiers to maintain backwards compatibility. To find identifiers that break this rule we have to find all identifiers that contain characters other than letters, numbers, and the underscore symbol. This can be done using a regular expression. We don't have to worry about the validity of identifier names; a built-in query already finds any syntax errors, such as variable names that don't start with an underscore or a letter. Since this check is relevant only for Python 3, a condition of major_version() = 3 is included. In Python 2 this issue would be caught by the query that reports all SyntaxError cases. This QL query finds all non-ASCII Python identifiers. import python from string identifier , AstNode n where ( identifier = n . ( Name ) . getId () or identifier = n . ( Attribute ) . getName () ) and not identifier . regexpMatch ( \"[a-zA-Z_][a-zA-Z_0-9]*\" ) and major_version () = 3 select n , \"Non ASCII character in identifier's name\" In this query, the Name class represents the names of identifiers. The Attribute class represents the names of attribute expressions, for example, a class method. We need to use the AstNode class to access the location of each identifier in the code. However, the AstNode class doesn't provide the identifier's name as a string that we can test using a regular expression. To get the name as a string, we call the member predicates .getId() and .getName() . Since these are defined for a more specific type, we need to use a type cast. Dive in: This could have been done using postfix and prefix casts. Visit the Casts help page to learn more. Comparing objects of different types In Python 2, objects of different types are ordered by their type names (with the exception of numbers). This results in behavior that can puzzle developers who are unfamiliar with this implementation detail. Python 2: >>> print 50 < \"Text\" True >>> [ 10 , 20 ] > 'Text' False This comparison essentially compares the types of the objects, that is: 'int' < 'str' . This is True because the word representing type int starts with i which is smaller than s - the str type (using lexicographic order). Likewise, because 'list' > 'str' is False , comparing a list object to a string object would return False . In Python 3, if you use ordering comparison operators when the operands don't have a natural ordering that makes sense, a TypeError exception is raised. This implies that there can be Python 2 code which may compare objects of different types and this would not be an issue until you run the program with a Python 3 interpreter. For instance, this valid Python 2 code would fail in Python 3: data = [ 10 , 20 , 30 ] mapper = { \"Source\" : \"Target\" } print ( data > mapper ) print ( data < mapper ) We can use QL to write a custom query that finds comparisons of invalid data types. import python ClassValue orderedType () { exists ( string typename | result = Value :: named ( typename ) | typename = \"str\" or typename = \"float\" or typename = \"list\" ) } from CompareNode compare , ControlFlowNode left , ControlFlowNode right , Context ctx , Value lval , Value rval , Cmpop op where compare . operands ( left , op , right ) and ( op instanceof Lt or op instanceof LtE or op instanceof Gt or op instanceof GtE ) and left . pointsTo ( ctx , lval , _ ) and right . pointsTo ( ctx , rval , _ ) and lval . getClass () != rval . getClass () and lval . getClass () = orderedType () and rval . getClass () = orderedType () select compare , \"Invalid comparison of objects due to type difference\" At this point it might be useful to refactor the code above because the where clause gets too difficult to read. We can define a helper predicate, incomparableTypes , that would hold if comparison expressions are of incompatible types: import python predicate incomparableTypes ( ClassValue a , ClassValue b ) { not a = b and a = orderedType () and b = orderedType () } ClassValue orderedType () { exists ( string typename | result = Value :: named ( typename ) | typename = \"str\" or typename = \"float\" or typename = \"list\" ) } from CompareNode compare , ControlFlowNode left , ControlFlowNode right , Context ctx , Value lval , Value rval , Cmpop op where compare . operands ( left , op , right ) and ( op instanceof Lt or op instanceof LtE or op instanceof Gt or op instanceof GtE ) and left . pointsTo ( ctx , lval , _ ) and right . pointsTo ( ctx , rval , _ ) and incomparableTypes ( lval , rval ) select compare , \"Invalid comparison of objects due to type difference\" The left and right expressions of the comparison can be inspected to check what type they point to using the .pointsTo() method. We use the don't care variable _ to state that we don't care what kind of Value the left and right expressions point to, however, they must be of a certain type. The query above currently only supports comparing strings, floats, and lists. However, it is easy to extend it just by copying the relevant where section and changing the class types. For instance, to extend this query to include the comparison of integer objects, you would just need to add the following section: ... ClassValue orderedType() { exists(string typename | result = Value::named(typename) | typename = \"str\" or typename = \"float\" or typename = \"list\" or typename = \"int\" ) } ... Octal literals syntax support Octal literals in Python 3 can no longer be defined in the form of a number starting with 0 , such as 0562 , as they could be in Python 2. Python 2 has two methods for defining octal literals: >>> print ( 0562 == 0o562 ) True Python 3 only supports the second of these syntaxes and using 0562 would cause a SyntaxError . Instead, you need to use a zero followed by a lower or upper case o (that is, o and O ), for example, 0o562 or 0O562 . The upper case O looks very similar to zero ( 0 ) so using a lowercase o may be preferable. Therefore, it can be helpful to search for octal literals in Python 2 that don't use o to avoid issues after converting the codebase to Python 3. Fortunately, there's already an existing query - Confusing octal literal - which finds octal literals with a leading 0 because they can easily be misread as decimal values. This query does just what we need. It's worth bearing in mind that this query doesn't raise alerts for octal literals that are of 4, 5, or 7 digits in length. These are ignored because Python code may include Unix permission mode octals which can be safely ignored. Here we want to raise an alert for all octal literals, so we simply remove the part that filters out octals of a certain length. This QL query finds all octal literals that would raise SyntaxError in Python 3: import python predicate is_old_octal ( IntegerLiteral i ) { exists ( string text | text = i . getText () | text . charAt ( 0 ) = \"0\" and not text = \"00\" and text . charAt ( _ ) != \"0\" and exists ( text . charAt ( 1 ) . toInt ()) ) } from IntegerLiteral i where major_version () = 3 and is_old_octal ( i ) select i , \"Invalid octal literal\" In this query we take advantage of the exists quantifier to define a predicate which holds for any integer literal that starts with a zero digit. Dive in: Visit the Explicit quantifiers help page to learn more about quantifiers in QL. Delimiter in numeric literals Python 3.6 supports using _ as a delimiter in numeric literals. This functionality was introduced in PEP 515 – Underscores in Numeric Literals . This is an example of how this works in Python 3.6: >>> 5_000.46 == 5000.46 True >>> 5_000 + 1_000 == 6000 True If your project style guide prohibits using this feature, for instance, for consistency with the Python 2 code, then you could write a custom QL query that would be able to find code where _ is used in numeric literals. Running code with underscores in numeric literals using a Python 2 interpreter would raise a SyntaxError . import python predicate hasUnderscore ( Num n ) { exists ( int i | n . getText () . charAt ( i ) = \"_\" ) } string numValue ( Num n ) { result = n . ( IntegerLiteral ) . getValue () . toString () or result = n . ( FloatLiteral ) . getValue () . toString () } from Num num where hasUnderscore ( num ) select num , num . getText () as AsInCode , numValue ( num ) as AsToReader Previously, we've only included two items in the select statement, however, you can return an arbitrary number of items. The .getText() method gives the actual source code (for example, 5_000 ) whereas the numValue predicate gives the string representation of the literal with underscores removed (for example, 5000 ). Being able to return multiple items within the select statement is extremely handy during the debugging and query writing process. If your project style guide is more relaxed and permits having underscores in integers only, but prohibits using underscore in floats, you can adjust the query to work solely with floats: import python predicate hasUnderscore ( Num n ) { exists ( int i | n . getText () . charAt ( i ) = \"_\" ) } from Num num where hasUnderscore ( num ) and not num instanceof IntegerLiteral select num Long integer type is not supported by Python 3 With the implementation of PEP 237 – Unifying Long Integers and Integers , the long type was merged with the int type. This means that having integer literals with L , for example, 10560L in Python 3 would raise a SyntaxError at runtime. To spot integers that wouldn't be compatible with Python 3 in your Python 2 project, you can use this custom QL query: import python string getLongPostfix () { result = \"L\" or result = \"l\" } from IntegerLiteral num where num . getText () . charAt ( num . getText () . length () - 1 ) = getLongPostfix () select num Dive in: .charAt string method is implemented using Java String.charAt and doesn't support negative indexing. The ‘cmp' parameter for ‘sorted(list)' is no longer supported Running the valid Python 2 code in the example below using a Python 3 interpreter would result in a TypeError because cmp is no longer a supported keyword argument for the sorted function. Visit The Old Way Using the cmp Parameter to learn more. def compare_as_ints ( a , b ): return a - b sorted ([ 50 , 30 , 40 , 20 , 10 ], cmp = compare_as_ints ) To spot this issue in Python code, we would need to find all calls to the built-in sorted function and see if the cmp keyword argument is being passed. This QL query will find all calls to the sorted function where the keyword argument cmp has been used. import python from CallNode call where Value :: named ( \"sorted\" ) . getACall () = call and exists ( call . getArgByName ( \"cmp\" )) select call , \"Call to sorted built-in function with cmp keyword argument.\" The CallNode class represents all calls in the code. We use this because we aren't interested in function definitions (which are accessed through the Function class) but in function calls. Once we've got the sorted built-in function, it's just a matter of finding sorted() calls with the cmp keyword argument supplied. You could reuse this QL query to find other built-in functions where the signature varies between Python versions. Methods ‘dict.iterkeys()', ‘dict.iteritems()' and ‘dict.itervalues()' are deprecated An attempt to access any of these dictionary methods would raise an AttributeError when running the code against a Python 3 interpreter: data = { 1 : 10 , 2 : 20 } for k , v in data . iteritems (): print ( k , v ) Therefore, we might want to write a QL query to spot when those methods access an object of dict type. import python string unsupportedDictMethod () { result = \"iteritems\" or result = \"iterkeys\" or result = \"itervalues\" } from Attribute attr , Value v where attr . getValue () . getAFlowNode () . pointsTo ( v ) and v . getClass () = Value :: named ( \"dict\" ) and attr . getAttr () = unsupportedDictMethod () select attr , \"A deprecated dictionary method was used\" As before, the AstNode root class gives us access to all elements of the source code. The Attribute class gives us access to all attributes that are accessed. The attribute object is tied to the object and this tie can be identified using the .pointsTo() method. Once we've found all the dictionary attributes throughout the source code, we leave only those that are no longer supported using a convenience predicate, unsupportedDictMethod . This is the end of this tutorial. The queries posted in this post can be executed using the LGTM.com query console , however, it's also possible to run the queries locally using Eclipse. Visit Running queries in your IDE to learn more. I hope you enjoy trying out QL on your own projects! If you have any questions, don't hesitate to ask on the community forum . Happy Querying!","tags":"QL","url":"/finding-python-compatibility-issues-with-ql.html","loc":"/finding-python-compatibility-issues-with-ql.html"},{"title":"Introduction to QL with Python","text":"This is one of the few posts I wrote in 2019 when working for Semmle (later acquired by GitHub) that was originally published on the Semmle blog that was transformed. Python library for QL has changed quite a bit since then, however, many principles are still relevant and helpful for anyone who would want to learn more about QL. See CodeQL for Python to learn more. Overview In this tutorial, you'll learn how to use QL to query a Python codebase and learn how to gain knowledge about tests present in your Python project. We'll be using QL to analyze the source code of the popular cli tool click by writing metric queries to compute some useful statistics about the code. The queries posted in this tutorial can be executed using the LGTM.com query console , however, it's also possible to run the queries locally using Eclipse. Visit Running queries in your IDE to learn more. QL query components If you read the Semmle Introduction to QL , you will see how QL queries consist of 3 main clauses: from where select In the from clause, you define the objects you would like to run queries against. For example, here's a query to find all functions in a project: from Function f select f Behind these objects, there are actually database tables that are generated in QL's extraction process . The database consists of dozens of tables, each of which represents a certain object type such as modules or functions. Dive in: to see what objects you can add to a from clause, check out the most commonly used standard QL library classes . The where clause is optional, and in here you define restrictions on the values that the variables declared in the from clause can hold. For instance, you would need to use the where clause if you want to list all Python modules in the repository you analyze that have names starting with _ or find all functions with more than 10 parameters. For instance, while from Function f select f selects all functions in the project, from Function f where count(f.getAnArg()) > 10 select f selects only those functions that take more than ten arguments. The select clause lets you define what you would like to get back as the result of a QL query execution. The results are returned as a table where each row represents a single result. So for example, instead of just printing the names of all the functions that take more than 10 parameters, you could print the number of parameters as well: from Function f where count(f.getAnArg()) > 10 select f, count(f.getAnArg()) As another example, here's a query that gets a list of all Python modules available in the source code repository. We select a module object and a constant string. import python from Module m select m , \"A module\" In the next QL query, we are getting a list of Python modules that have names starting with a single underscore (excluding dunder modules such as __init__.py which start with a double underscore). --- queryConsole : https : // lgtm . com / query / 7269195673257399800 / --- import python from Module m where m . getFile () . getBaseName () . regexpMatch ( \"_[&#94;_].*\" ) select m , \"A private module\" You may have noticed that variable m (which is of the type Module ) has the getFile method associated with it. We can then call the getBaseName method, which returns a string. Strings have their own built-in methods such as regexpMatch , which can match a string using a regular expression. Dive in: For a full list of methods available on strings, see Built-ins for string . You can see what methods and properties each of the built-in types have in the QL Specification document . Having these methods on the Module object makes it possible to interrogate database tables using concepts that are more abstract and intuitive to programmers. This means you can avoid directly dealing with the raw underlying tables of the database. Finding test modules We are interested in learning more about the tests in a Python project, so let's first find out how many test modules are present in the repo. The standard Python naming convention is for test modules to begin with test_ . We can find these modules by using the matches() string method which matches strings in the same way as the LIKE operator in SQL. --- queryConsole : https : // lgtm . com / query / 8969473391628017193 / --- import python from Module m where m . getFile () . getBaseName () . matches ( \"test \\\\ _%\" ) select m , \"A test module\" Another requirement we have to meet is that all test modules in the project are located within a single directory named tests . This is because there can potentially be regular Python modules stored in some directory that just happen to have a name starting with test_ , e.g., test_db_connection.py (presumably providing means to verify that the connection to a database could be established). This module may be used as a part of the application business logic, and is not a test module, as it won't contain any tests. In this case, refining the current query doesn't change the query results but it's good practice anyway because it makes the query results more stable, if for example, a test_db_connection.py file were to be added to the project later. In this QL query, we are selecting Python modules that are located within the tests folder and have a name starting with test_ . To access the folder in which a Python module is located, we need to get access to the File object (because a Python module is just a plain file on disk), which in turn is located in some directory (which is a Folder object), which in turn has a name. --- queryConsole : https : // lgtm . com / query / 8736700387179400714 / --- import python from Module m where m . getFile () . getParent () . getBaseName () = \"tests\" and m . getFile () . getBaseName () . matches ( \"test \\\\ _%\" ) select m , \"A test module\" Dive in: the click repository does not have any nested folders within the tests folder. However, getting all test files stored in the tests recursively is possible using Transitive closures . To apply the .getParent() method recursively, we would need to write m.getFile().getParent+().getBaseName() = \"tests\" . It's possible to combine multiple condition expressions using the and operator. If, instead your project's test modules were either modules that had names starting with test_ or were stored within the tests folder, you could use the or operator: --- queryConsole : https : // lgtm . com / query / 4860024469296565410 / --- import python from Module m where m . getFile () . getParent () . getBaseName () = \"tests\" or m . getFile () . getBaseName () . matches ( \"test \\\\ _%\" ) select m , \"A test module\" Refactoring QL queries At this point, as we have identified the test modules in our project, we can start learning more about the actual tests. However, as our query gets bigger, it makes sense to refactor some of its parts into separate entities which could be reused by multiple queries. This often has the added benefit of making the query easier to read. In QL, a predicate can be used for this. In this QL query, we'll be using a predicate without result . You can think of these types of predicates as boolean functions. In the following predicate, we pass in an argument m that has the type Module . The predicate's body has an expression that is evaluated and keeps only the modules for which the condition holds. This QL predicate would keep only those Python modules that are located within the tests folder. predicate isInsideTestsFolder(Module m) { m.getFile().getParent().getBaseName() = \"tests\" } To use a predicate within a QL query, you would put a predicate call in the where clause: --- queryConsole : https : // lgtm . com / query / 2694103716087854413 / --- import python predicate isInsideTestsFolder ( Module m ) { m . getFile () . getParent () . getBaseName () = \"tests\" } from Module m where isInsideTestsFolder ( m ) select m , \"A module inside tests folder\" A predicate, just like any function, can have multiple parameters. For instance, if we want to have a more generic predicate that would keep modules located within any given folder, we would need to add a second parameter. This makes a predicate more flexible, as we can reuse this predicate in other queries passing in the name of directories with test modules when analyzing other Python repositories. --- queryConsole : https : // lgtm . com / query / 7407075760043821247 / --- import python predicate isInsideFolder ( Module m , string folderName ) { m . getFile () . getParent () . getBaseName () = folderName } from Module m where isInsideFolder ( m , \"tests\" ) select m , \"A module inside the tests folder\" Dive in: The folderName object is of string type. QL supports multiple primitive types: boolean , date , float , int , and string . To learn more about these types, visit Kinds of types . A QL query can contain many predicates. In this QL query, we have extracted the functionality of matching a test module by name into the predicate nameMatchesTestPattern . --- queryConsole : https : // lgtm . com / query / 6256521983311323313 / --- import python predicate isInsideFolder ( Module m , string folderName ) { m . getFile () . getParent () . getBaseName () = folderName } predicate nameMatchesTestPattern ( Module m ) { m . getFile () . getBaseName () . matches ( \"test \\\\ _%\" ) } from Module m where isInsideFolder ( m , \"tests\" ) and nameMatchesTestPattern ( m ) select m , \"A test module\" Count number of tests per file Now that we have found all the test modules, and have modularized our code a bit, we are ready to collect some metrics about this project's test code. It can be helpful to know how many tests are defined in each test module; having too many tests in a single module can indicate that it's testing too many pieces of the application and that there's potential for refactoring. If a module has just one or two tests, perhaps more tests could be added. The project we are exploring uses pytest for its test framework, meaning that each test that will be run is defined as a Python function named test_%name% , where %name% describes what this particular test does. This means we have to count the number of functions that have names starting with test_ defined in each test module. In order to interact with functions, you have to use the Function object. In the following QL predicate, we count the number of test functions in the module that is passed in as an input parameter. We can also write predicates that compute values derived from their parameters. Such predicates are called predicates with results . For a QL predicate to return a value, it needs to be given a return type (in this instance int ), and the special variable result needs to be bound to something of that same type, (in this case, we bind it to the result of a call to count ). int getTestsCount(Module m) { result = count(Function f | f.getEnclosingModule() = m and f.getName().matches(\"test\\\\_%\") | f) } Using the count aggregate requires having a special syntax: We define the variables we would like to use; in this case, we are using a single variable of type Function named f . We define what condition should hold using an expression; only functions that are defined in the scope of a given module would be kept ( f.getEnclosingModule() method does the work) and function names should start with test_ ( f.getName().matches() method does the work). We define what the count aggregate exactly counts; since we are interested in the number of functions that meet our condition, we simply count f . Dive in: Visit Aggregates to learn more about count and other QL aggregates such as max and sum . We can now use this predicate within the QL query: --- queryConsole : https : // lgtm . com / query / 8228699770073082994 / --- import python predicate isInsideFolder ( Module m , string folderName ) { m . getFile () . getParent () . getBaseName () = folderName } predicate nameMatchesTestPattern ( Module m ) { m . getFile () . getBaseName () . matches ( \"test \\\\ _%\" ) } int getTestsCount ( Module m ) { result = count ( Function f | f . getEnclosingModule () = m and f . getName () . matches ( \"test \\\\ _%\" ) | f ) } from Module m , Folder f where isInsideFolder ( m , \"tests\" ) and nameMatchesTestPattern ( m ) select m , m . getFile () . getBaseName (), getTestsCount ( m ) as TestCount order by TestCount To include the number of tests in the output results, we have extended the select statement with another column aliased TestCount , that represents the result of the getTestsCount(m) predicate call. It would be helpful to see the test modules with the fewest tests in them which is why we use the order by command that sorts the results in ascending order by TestCount . Find Python modules without a corresponding test module Being able to detect modules that do not have corresponding test modules can be helpful, for example when there is a policy within the development team which dictates that each Python module should have an associated test module. Since we already know how to find test modules, we need only find the modules that don't have a corresponding test module. In other words, if for the read_file.py module there is no tests/test_read_file.py module, we can conclude that this module does not have any tests (or that it has been named without following the Python convention). To find out whether a program module has a corresponding test module, we need to create a new predicate that finds only those program modules which don't have a paired test module. predicate moduleWithoutTests(Module m, Folder f) { not exists( f.getFile(\"test_\" + m.getFile().getBaseName()) ) and not exists( f.getFile(\"test\" + m.getFile().getBaseName()) ) } As you can see, a predicate can have one or more input parameters. The reason we pass a Folder is because we want to specify where the test modules are stored. The logic that is being evaluated in this predicate is rather simple; having a passed module as a parameter, say %name%.py , it checks whether there is a file named test_%name%.py . This can be done using a special exists aggregate which would hold if there is a test module in the given folder. We also have to look for modules that start with test to support private modules that, by convention, start with a single underscore ( _ ). --- queryConsole : https : // lgtm . com / query / 4650679191588923532 / --- import python predicate isInsideFolder ( Module m , string folderName ) { m . getFile () . getParent () . getBaseName () = folderName } predicate nameMatchesTestPattern ( Module m ) { m . getFile () . getBaseName () . matches ( \"test \\\\ _%\" ) } predicate moduleWithoutTests ( Module m , Folder f ) { not exists ( f . getFile ( \"test_\" + m . getFile () . getBaseName ()) ) and not exists ( f . getFile ( \"test\" + m . getFile () . getBaseName ()) ) } from Module m , Folder f where isInsideFolder ( m , \"click\" ) and moduleWithoutTests ( m , f . getFolder ( \"tests\" )) select m , m . getFile () . getBaseName () In this query, we have analyzed modules from the click folder only. It's common to restrict such queries to the main code of a project, since a Python project can have utility modules used for documentation generation or raw data storage, for which presence of tests may not be necessary. Creating a QL library As you may have noticed, we have defined quite a few useful predicates. They all have something in common—they are applicable to test modules. Our query has gotten fairly big, but the actual select statement is rather small. To keep the query tidy and to improve modularization, we can create a TestModule class that will be a subclass of the standard QL class Module . This class could be stored in a separate QL library file ( .qll ), which could then be imported in any other query that deals with test modules. Dive in: learn more about defining a class . If you want to run the queries locally in Eclipse, you can put this code into a new QL library file Testing.qll so it can be used in query files ( .ql files). import python string getTestPrefix () { result = \"test \\\\ _%\" } string getTestsFolder () { result = \"tests\" } /** Keeps modules located within the given folder . */ private predicate isInsideFolder ( Module m ) { m . getFile () . getParent () . getBaseName () = getTestsFolder () } /** Keeps modules that have names starting with the defined prefix . */ private predicate nameMatchesTestPattern ( Module m ) { m . getFile () . getBaseName () . matches ( getTestPrefix ()) } class TestModule extends Module { TestModule () { isInsideFolder ( this ) and nameMatchesTestPattern ( this ) } /** Gets count of functions within the module which name starts with the defined prefix . */ int getTestsCount () { result = count ( Function f | f . getEnclosingModule () = this and f . getName () . matches ( getTestPrefix ()) | f ) } } Alternatively, if you are using LGTM.com, then you can define the class along with the actual query: --- queryConsole : https : // lgtm . com / query / 4740785288826699608 / --- import python string getTestPrefix () { result = \"test \\\\ _%\" } string getTestsFolder () { result = \"tests\" } /** Keeps modules located within the given folder . */ private predicate isInsideFolder ( Module m ) { m . getFile () . getParent () . getBaseName () = getTestsFolder () } /** Keeps modules that have names starting with the defined prefix . */ private predicate nameMatchesTestPattern ( Module m ) { m . getFile () . getBaseName () . matches ( getTestPrefix ()) } class TestModule extends Module { TestModule () { isInsideFolder ( this ) and nameMatchesTestPattern ( this ) } /** Gets count of functions within the module which name starts with the defined prefix . */ int getTestsCount () { result = count ( Function f | f . getEnclosingModule () = this and f . getName () . matches ( getTestPrefix ()) | f ) } } from TestModule tm select tm , tm . getTestsCount () as TestCount order by TestCount desc As we have used string \"test\\\\_%\" in a few places, it can be useful to pull it into a variable. Using fields in classes is an advanced topic, so instead, for now we've simply used a predicate with result that just gives us back this string. This means we would need to define the test prefix only within this predicate and not within multiple predicates. To keep this class in a separate file, you would need to use the QL for Eclipse plugin to run queries locally on your machine. We would need then to import the library and then use the TestModule class instead of the built-in Module class we have used before. import python import Testing from TestModule tm select tm , tm . getTestsCount () as TestCount order by TestCount desc This is the end of this tutorial. I hope you enjoy trying out QL on your own projects! If you have any questions, don't hesitate to ask on the community forum . Happy Querying!","tags":"QL","url":"/introduction-to-ql-with-python.html","loc":"/introduction-to-ql-with-python.html"}]};